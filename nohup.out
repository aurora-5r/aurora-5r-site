  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2021-03-30 07:09:20,407[0m] {[34mscheduler_job.py:[0m1247} INFO[0m - Starting the scheduler[0m
[[34m2021-03-30 07:09:20,407[0m] {[34mscheduler_job.py:[0m1252} INFO[0m - Processing each file at most -1 times[0m
[[34m2021-03-30 07:09:20,410[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5607[0m
[[34m2021-03-30 07:09:20,413[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:09:20,415[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 07:09:20,426] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 07:09:20,434[0m] {[34mscheduler_job.py:[0m1901} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: build_sites.gite 2021-03-30 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:09:20,607[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:09:20,609[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:09:20,609[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:09:20,609[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:09:20,611[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 07:09:20,611[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:09:20,614[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:09:21,203[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:09:21,254[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:09:21,254[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:09:56,715[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:09:56,881[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:09:56,883[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:09:56,883[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:09:56,883[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:09:56,885[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:09:56,885[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:09:56,889[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:09:57,470[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:09:57,520[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:09:57,520[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:09:57,918[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:10:01,515[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:10:01,517[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:10:01,517[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:10:01,517[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:10:01,519[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 07:10:01,519[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:10:01,522[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:10:02,203[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:10:02,253[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:10:02,254[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:10:02,575[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:10:02,736[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:10:02,738[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:10:02,738[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:10:02,738[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:10:02,740[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 07:10:02,740[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:10:02,744[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:10:03,330[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:10:03,380[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:10:03,380[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:10:30,308[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:10:30,481[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:10:30,482[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:10:30,482[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:10:30,482[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:10:30,484[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:10:30,484[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:10:30,487[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:10:31,071[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:10:31,120[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:10:31,120[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:10:31,509[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:14:10,622[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-27 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:14:10,624[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:14:10,624[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:14:10,624[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-27 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:14:10,625[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 27, 8, 50, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 2 and queue default[0m
[[34m2021-03-30 07:14:10,626[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-27T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:14:10,629[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-27T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:14:11,206[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:14:11,252[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:14:11,252[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-27T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:14:38,124[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-27 08:50:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:14:38,153[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:14:38,156[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 2 SchedulerJob instances as failed[0m
[[34m2021-03-30 07:14:38,159[0m] {[34mscheduler_job.py:[0m1901} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: build_sites.aurora 2021-03-27 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:14:38,321[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-27 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:14:38,323[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:14:38,323[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:14:38,323[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-27 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:14:38,325[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 27, 8, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:14:38,325[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-27T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:14:38,340[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-27T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:14:38,923[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:14:38,969[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:14:38,969[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-27T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:14:39,369[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-27 08:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:14:58,191[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:14:58,193[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:14:58,193[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:14:58,193[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:14:58,195[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 6, 50, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:14:58,195[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:14:58,198[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:14:58,775[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:14:58,822[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:14:58,823[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:14:59,217[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 06:50:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:14:59,372[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 06:50:00+00:00: scheduled__2021-03-30T06:50:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 07:15:32,343[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:15:32,344[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:15:32,345[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:15:32,345[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:15:32,346[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 0, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:15:32,346[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:15:32,349[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:15:32,936[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:15:32,984[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:15:32,984[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:15:33,397[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:00:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:15:33,537[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 07:00:00+00:00: scheduled__2021-03-30T07:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 07:19:38,317[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:19:39,450[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-27 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:19:39,452[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:19:39,452[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:19:39,452[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-27 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:19:39,454[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 27, 8, 50, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:19:39,455[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-27T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:19:39,458[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-27T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:19:40,055[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:19:40,107[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:19:40,107[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-27T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:19:40,503[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-27 08:50:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:19:40,648[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-27 08:50:00+00:00: scheduled__2021-03-27T08:50:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 07:20:01,335[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:20:01,337[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:20:01,337[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:20:01,337[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:20:01,339[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 07:20:01,339[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:20:01,347[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:20:02,035[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:20:02,096[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:20:02,096[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:20:02,426[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:20:02,580[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:20:02,582[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:20:02,582[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:20:02,582[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:20:02,584[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 07:20:02,584[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:20:02,588[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:20:03,174[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:20:03,222[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:20:03,222[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:20:32,691[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:20:32,855[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:20:32,857[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:20:32,857[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:20:32,857[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:20:32,859[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:20:32,859[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:20:32,863[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:20:33,443[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:20:33,491[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:20:33,491[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:20:33,878[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:24:38,372[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:25:34,862[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:25:34,863[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:25:34,863[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:25:34,863[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:25:34,865[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 10, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:25:34,865[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:25:34,869[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:25:35,464[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:25:35,514[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:25:35,514[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:25:35,918[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:10:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:25:36,072[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 07:10:00+00:00: scheduled__2021-03-30T07:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 07:29:38,523[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:30:01,453[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:30:01,454[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:30:01,454[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:30:01,454[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:30:01,456[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 07:30:01,456[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:30:01,459[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:30:02,069[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:30:02,124[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:30:02,124[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:30:02,444[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:30:02,598[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:30:02,600[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:30:02,600[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:30:02,600[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:30:02,602[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 07:30:02,602[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:30:02,605[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:30:03,204[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:30:03,250[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:30:03,250[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:30:33,032[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:30:33,197[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:30:33,199[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:30:33,199[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:30:33,199[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:30:33,201[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:30:33,201[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:30:33,205[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:30:33,804[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:30:33,851[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:30:33,851[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:30:34,242[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:34:38,581[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:35:35,110[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:35:35,112[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:35:35,112[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:35:35,112[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:35:35,114[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 20, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:35:35,114[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:35:35,117[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:35:35,696[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:35:35,744[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:35:35,744[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:35:36,173[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:20:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:35:36,355[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 07:20:00+00:00: scheduled__2021-03-30T07:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 07:39:38,624[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:40:01,577[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:40:01,578[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:40:01,578[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:40:01,579[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:40:01,580[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 07:40:01,580[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:40:01,583[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:40:02,177[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:40:02,229[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:40:02,229[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:40:02,553[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:40:02,708[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:40:02,709[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:40:02,709[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:40:02,709[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:40:02,711[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 07:40:02,711[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:40:02,715[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:40:03,301[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:40:03,347[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:40:03,347[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:40:32,766[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:40:32,930[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:40:32,932[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:40:32,932[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:40:32,932[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:40:32,934[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:40:32,934[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:40:32,938[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:40:33,517[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:40:33,567[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:40:33,567[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:40:33,963[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:44:38,680[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:45:33,839[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:45:33,840[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:45:33,840[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:45:33,841[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:45:33,842[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 30, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:45:33,842[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:45:33,845[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:45:34,439[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:45:34,484[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:45:34,485[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:45:34,916[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:30:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:45:35,052[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 07:30:00+00:00: scheduled__2021-03-30T07:30:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 07:49:38,724[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:50:02,096[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:50:02,097[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:50:02,098[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:50:02,098[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:50:02,100[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 7, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 07:50:02,100[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:50:02,103[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:50:02,689[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:50:02,740[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:50:02,740[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T07:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:50:03,066[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 07:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:50:03,211[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:50:03,212[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:50:03,213[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:50:03,213[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:50:03,214[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 7, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 07:50:03,214[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:50:03,224[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:50:03,820[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:50:03,868[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:50:03,868[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T07:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:50:34,047[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 07:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:50:34,225[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:50:34,226[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:50:34,226[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:50:34,226[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:50:34,228[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:50:34,228[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:50:34,231[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:50:34,808[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:50:34,854[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:50:34,854[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:50:35,242[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 07:54:38,779[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 07:55:35,142[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:55:35,144[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 07:55:35,144[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 07:55:35,144[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 07:55:35,145[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 40, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 07:55:35,146[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:55:35,149[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 07:55:35,727[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 07:55:35,775[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 07:55:35,775[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 07:55:36,167[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:40:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 07:55:36,307[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 07:40:00+00:00: scheduled__2021-03-30T07:40:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 07:59:38,822[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:00:01,643[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:00:01,645[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:00:01,645[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:00:01,645[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:00:01,647[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 08:00:01,647[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:00:01,654[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:00:02,355[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:00:02,404[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:00:02,405[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:00:02,733[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 07:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:00:02,887[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:00:02,888[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:00:02,888[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:00:02,888[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:00:02,890[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 08:00:02,890[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:00:02,894[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:00:03,481[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:00:03,528[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:00:03,528[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:00:33,218[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 07:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:00:33,384[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:00:33,385[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:00:33,385[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:00:33,385[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:00:33,387[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:00:33,387[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:00:33,391[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:00:33,974[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:00:34,021[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:00:34,021[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:00:34,423[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:04:38,877[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:05:34,296[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:05:34,298[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:05:34,298[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:05:34,298[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:05:34,299[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 7, 50, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:05:34,300[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:05:34,303[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:05:34,903[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:05:34,952[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:05:34,953[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:05:35,354[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 07:50:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 08:05:35,490[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 07:50:00+00:00: scheduled__2021-03-30T07:50:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 08:09:38,919[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:10:01,405[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:10:01,406[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:10:01,407[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:10:01,407[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:10:01,408[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 8, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 08:10:01,409[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:10:01,412[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:10:02,026[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:10:02,081[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:10:02,081[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:10:02,402[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 08:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:10:02,556[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:10:02,558[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:10:02,558[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:10:02,558[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:10:02,560[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 8, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 08:10:02,560[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:10:02,563[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:10:03,148[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:10:03,196[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:10:03,196[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:10:33,053[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 08:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:10:33,219[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:10:33,220[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:10:33,220[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:10:33,220[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:10:33,222[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:10:33,222[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:10:33,227[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:10:33,805[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:10:33,852[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:10:33,852[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:10:34,246[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:14:38,974[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:15:34,102[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:15:34,103[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:15:34,103[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:15:34,103[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:15:34,105[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 0, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:15:34,105[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:15:34,110[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:15:34,701[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:15:34,748[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:15:34,748[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:15:35,139[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:00:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 08:15:35,276[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 08:00:00+00:00: scheduled__2021-03-30T08:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 08:19:39,018[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:20:01,215[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:20:01,216[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:20:01,216[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:20:01,217[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:20:01,218[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 8, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 08:20:01,219[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:20:01,222[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:20:01,887[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:20:01,938[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:20:01,939[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T08:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:20:02,312[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 08:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:20:02,466[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:20:02,468[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:20:02,468[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:20:02,468[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:20:02,470[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 8, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 08:20:02,470[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:20:02,474[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:20:03,110[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:20:03,157[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:20:03,157[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T08:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:20:32,710[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 08:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:20:32,875[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:20:32,877[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:20:32,877[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:20:32,877[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:20:32,879[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:20:32,879[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:20:32,882[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:20:33,465[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:20:33,514[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:20:33,514[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:20:33,903[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:24:39,070[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:25:33,782[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:25:33,783[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:25:33,783[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:25:33,784[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:25:33,785[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 10, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:25:33,785[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:25:33,788[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:25:34,368[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:25:34,415[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:25:34,415[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:25:34,824[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:10:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 08:25:34,960[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 08:10:00+00:00: scheduled__2021-03-30T08:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 08:29:39,113[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:30:02,036[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:30:02,038[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:30:02,038[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:30:02,038[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:30:02,041[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 8, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 08:30:02,041[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:30:02,044[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:30:02,651[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:30:02,704[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:30:02,704[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T08:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:30:03,017[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 08:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:30:03,161[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:30:03,163[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:30:03,163[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:30:03,163[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:30:03,165[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 8, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 08:30:03,166[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:30:03,169[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:30:03,758[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:30:03,806[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:30:03,806[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T08:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:30:33,104[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 08:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:30:33,269[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:30:33,271[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:30:33,271[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:30:33,271[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:30:33,273[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:30:33,274[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:30:33,277[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:30:33,860[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:30:33,909[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:30:33,909[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:30:34,298[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:34:39,172[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:35:34,233[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:35:34,234[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:35:34,234[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:35:34,235[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:35:34,236[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 20, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:35:34,236[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:35:34,240[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:35:34,841[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:35:34,887[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:35:34,887[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:35:35,282[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:20:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 08:35:35,419[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun build_sites @ 2021-03-30 08:20:00+00:00: scheduled__2021-03-30T08:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-03-30 08:39:39,312[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:40:02,356[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:40:02,358[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:40:02,358[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:40:02,358[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:40:02,360[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 8, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 08:40:02,361[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:40:02,371[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:40:03,823[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:40:03,870[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:40:03,870[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T08:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:40:04,503[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 08:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:40:04,648[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:40:04,650[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:40:04,650[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:40:04,650[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:40:04,652[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 8, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 08:40:04,652[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:40:04,656[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:40:05,260[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:40:05,309[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:40:05,309[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T08:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:40:48,956[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 08:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:40:49,466[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:40:49,468[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:40:49,468[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:40:49,468[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:40:49,470[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:40:49,470[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:40:49,474[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:40:50,727[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:40:50,803[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:40:50,803[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:43:00,388[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:43:00,406[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5607) last sent a heartbeat 131.31 seconds ago! Restarting it[0m
[[34m2021-03-30 08:43:00,410[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5607[0m
[[34m2021-03-30 08:43:00,622[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5607, status='terminated', exitcode=0, started='07:09:19') (5607) terminated with exit code 0[0m
[[34m2021-03-30 08:43:00,626[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 13297[0m
[[34m2021-03-30 08:43:00,636[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 08:43:00,663] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 08:43:01,194[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 08:30:00+00:00: scheduled__2021-03-30T08:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 08:44:39,341[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:49:39,472[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:50:01,183[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:50:01,186[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:50:01,188[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:50:01,190[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:50:01,199[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 8, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 08:50:01,200[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:50:01,204[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:50:02,442[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:50:02,506[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:50:02,506[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T08:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:50:03,142[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 08:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:50:03,301[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:50:03,303[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:50:03,303[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:50:03,303[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:50:03,304[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 8, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 08:50:03,305[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:50:03,310[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:50:04,343[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:50:04,420[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:50:04,423[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T08:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:50:49,633[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 08:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:50:50,340[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:50:50,347[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 08:50:50,347[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 08:50:50,350[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 08:50:50,352[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 08:50:50,352[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:50:50,360[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 08:50:51,958[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 08:50:52,128[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 08:50:52,129[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 08:53:04,442[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 08:53:04,460[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=13297) last sent a heartbeat 134.54 seconds ago! Restarting it[0m
[[34m2021-03-30 08:53:04,464[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 13297[0m
[[34m2021-03-30 08:53:04,636[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=13297, status='terminated', exitcode=0, started='08:42:59') (13297) terminated with exit code 0[0m
[[34m2021-03-30 08:53:04,639[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 27264[0m
[[34m2021-03-30 08:53:04,659[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 08:53:04,675] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 08:53:05,169[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 08:40:00+00:00: scheduled__2021-03-30T08:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 08:54:39,509[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 08:59:39,642[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:00:01,619[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:00:01,620[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:00:01,620[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:00:01,621[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:00:01,622[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 8, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 09:00:01,622[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:00:01,626[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:00:03,104[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:00:03,160[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:00:03,161[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:00:03,847[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 08:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:00:04,006[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:00:04,008[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:00:04,008[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:00:04,008[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:00:04,009[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 8, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 09:00:04,010[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:00:04,013[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:00:04,609[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:00:04,655[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:00:04,655[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:00:46,762[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 08:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:00:47,261[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:00:47,264[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:00:47,264[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:00:47,266[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:00:47,268[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 8, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 09:00:47,269[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:00:47,272[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:00:48,847[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:00:48,974[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:00:48,978[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:02:58,701[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 08:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:02:58,718[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=27264) last sent a heartbeat 131.81 seconds ago! Restarting it[0m
[[34m2021-03-30 09:02:58,722[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 27264[0m
[[34m2021-03-30 09:02:58,935[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=27264, status='terminated', exitcode=0, started='08:53:03') (27264) terminated with exit code 0[0m
[[34m2021-03-30 09:02:58,938[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 9161[0m
[[34m2021-03-30 09:02:58,948[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 09:02:58,964] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 09:02:59,559[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 08:50:00+00:00: scheduled__2021-03-30T08:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 09:04:39,670[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:09:39,698[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:10:01,412[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:10:01,415[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:10:01,416[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:10:01,418[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:10:01,420[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 9, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 09:10:01,421[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:10:01,426[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:10:03,386[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:10:03,497[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:10:03,497[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T09:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:10:04,442[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 09:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:10:04,630[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:10:04,632[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:10:04,632[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:10:04,632[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:10:04,634[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 9, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 09:10:04,634[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:10:04,637[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:10:05,864[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:10:05,938[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:10:05,939[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T09:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:10:49,526[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 09:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:10:49,919[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:10:49,920[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:10:49,920[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:10:49,921[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:10:49,923[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 9, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 09:10:49,923[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:10:49,926[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:10:51,131[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:10:51,211[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:10:51,211[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T09:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:12:59,994[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 09:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:13:00,013[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=9161) last sent a heartbeat 130.34 seconds ago! Restarting it[0m
[[34m2021-03-30 09:13:00,017[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 9161[0m
[[34m2021-03-30 09:13:00,230[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=9161, status='terminated', exitcode=0, started='09:02:58') (9161) terminated with exit code 0[0m
[[34m2021-03-30 09:13:00,233[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23033[0m
[[34m2021-03-30 09:13:00,259[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 09:13:00,275] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 09:13:00,793[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 09:00:00+00:00: scheduled__2021-03-30T09:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 09:14:39,739[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:19:39,781[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:20:02,220[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:20:02,222[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:20:02,222[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:20:02,223[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:20:02,225[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 9, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 09:20:02,225[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:20:02,228[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:20:03,637[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:20:03,700[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:20:03,700[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T09:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:20:04,329[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 09:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:20:04,494[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:20:04,495[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:20:04,495[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:20:04,496[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:20:04,497[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 9, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 09:20:04,497[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:20:04,501[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:20:05,143[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:20:05,189[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:20:05,189[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T09:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:20:49,002[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 09:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:20:49,471[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:20:49,483[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:20:49,483[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:20:49,483[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:20:49,486[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 9, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 09:20:49,486[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:20:49,489[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:20:51,177[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:20:51,301[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:20:51,305[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T09:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:23:02,013[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 09:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:23:02,031[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23033) last sent a heartbeat 132.87 seconds ago! Restarting it[0m
[[34m2021-03-30 09:23:02,035[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23033[0m
[[34m2021-03-30 09:23:02,248[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23033, status='terminated', exitcode=0, started='09:12:59') (23033) terminated with exit code 0[0m
[[34m2021-03-30 09:23:02,251[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5163[0m
[[34m2021-03-30 09:23:02,260[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 09:23:02,274] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 09:23:02,746[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 09:10:00+00:00: scheduled__2021-03-30T09:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 09:24:39,815[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:29:39,851[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:30:02,127[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:30:02,130[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:30:02,130[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:30:02,130[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:30:02,132[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 9, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 09:30:02,132[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:30:02,135[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:30:03,784[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:30:03,844[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:30:03,844[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T09:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:30:04,508[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 09:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:30:04,660[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:30:04,662[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:30:04,662[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:30:04,662[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:30:04,664[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 9, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 09:30:04,665[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:30:04,668[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:30:05,483[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:30:05,537[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:30:05,538[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T09:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:30:49,910[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 09:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:30:50,403[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:30:50,405[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:30:50,405[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:30:50,405[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:30:50,408[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 9, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 09:30:50,408[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:30:50,411[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:30:51,680[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:30:51,774[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:30:51,775[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T09:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:32:58,765[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 09:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:32:58,785[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5163) last sent a heartbeat 128.73 seconds ago! Restarting it[0m
[[34m2021-03-30 09:32:58,791[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5163[0m
[[34m2021-03-30 09:32:59,046[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5163, status='terminated', exitcode=0, started='09:23:01') (5163) terminated with exit code 0[0m
[[34m2021-03-30 09:32:59,051[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18996[0m
[[34m2021-03-30 09:32:59,073[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 09:32:59,102] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 09:32:59,695[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 09:20:00+00:00: scheduled__2021-03-30T09:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 09:34:39,884[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:39:39,921[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:40:01,078[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:40:01,080[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:40:01,080[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:40:01,080[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:40:01,082[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 9, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 09:40:01,082[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:40:01,085[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:40:02,377[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:40:02,434[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:40:02,434[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T09:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:40:03,104[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 09:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:40:03,263[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:40:03,264[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:40:03,264[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:40:03,264[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:40:03,266[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 9, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 09:40:03,267[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:40:03,270[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:40:03,995[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:40:04,081[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:40:04,081[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T09:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:40:44,069[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 09:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:40:44,583[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:40:44,584[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:40:44,585[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:40:44,585[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:40:44,587[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 9, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 09:40:44,587[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:40:44,591[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:40:46,196[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:40:46,327[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:40:46,328[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T09:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:42:52,881[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 09:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:42:52,899[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18996) last sent a heartbeat 128.68 seconds ago! Restarting it[0m
[[34m2021-03-30 09:42:52,903[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18996[0m
[[34m2021-03-30 09:42:53,076[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18996, status='terminated', exitcode=0, started='09:32:58') (18996) terminated with exit code 0[0m
[[34m2021-03-30 09:42:53,081[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 32437[0m
[[34m2021-03-30 09:42:53,090[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 09:42:53,104] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 09:42:53,656[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 09:30:00+00:00: scheduled__2021-03-30T09:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 09:44:40,054[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:49:40,090[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:50:01,417[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:50:01,419[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:50:01,419[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:50:01,419[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:50:01,421[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 9, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 09:50:01,421[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:50:01,424[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:50:02,769[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:50:02,829[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:50:02,829[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T09:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:50:03,454[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 09:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:50:03,613[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:50:03,615[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:50:03,615[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:50:03,616[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:50:03,617[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 9, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 09:50:03,618[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:50:03,621[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:50:04,218[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:50:04,265[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:50:04,265[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T09:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:50:47,166[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 09:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:50:47,514[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:50:47,516[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 09:50:47,516[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 09:50:47,516[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 09:50:47,518[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 9, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 09:50:47,518[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:50:47,521[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 09:50:48,574[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 09:50:48,680[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 09:50:48,680[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T09:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 09:52:58,628[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 09:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 09:52:58,653[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=32437) last sent a heartbeat 131.34 seconds ago! Restarting it[0m
[[34m2021-03-30 09:52:58,661[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 32437[0m
[[34m2021-03-30 09:52:58,918[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=32437, status='terminated', exitcode=0, started='09:42:52') (32437) terminated with exit code 0[0m
[[34m2021-03-30 09:52:58,923[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14414[0m
[[34m2021-03-30 09:52:58,938[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 09:52:58,956] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 09:52:59,545[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 09:40:00+00:00: scheduled__2021-03-30T09:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 09:54:40,118[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 09:59:40,257[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:00:02,414[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:00:02,416[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:00:02,416[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:00:02,416[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:00:02,418[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 9, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 10:00:02,419[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:00:02,422[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:00:04,077[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:00:04,177[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:00:04,177[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T09:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:00:04,975[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 09:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:00:05,150[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:00:05,153[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:00:05,153[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:00:05,153[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:00:05,155[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 9, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 10:00:05,155[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:00:05,159[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:00:05,953[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:00:06,020[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:00:06,020[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T09:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:00:51,717[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 09:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:00:52,155[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:00:52,162[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:00:52,163[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:00:52,164[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:00:52,166[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 9, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 10:00:52,167[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:00:52,172[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:00:53,873[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:00:53,992[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:00:53,993[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T09:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:03:06,440[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 09:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:03:06,457[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14414) last sent a heartbeat 134.58 seconds ago! Restarting it[0m
[[34m2021-03-30 10:03:06,462[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14414[0m
[[34m2021-03-30 10:03:06,635[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14414, status='terminated', exitcode=0, started='09:52:58') (14414) terminated with exit code 0[0m
[[34m2021-03-30 10:03:06,638[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28932[0m
[[34m2021-03-30 10:03:06,648[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 10:03:06,671] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 10:03:07,169[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 09:50:00+00:00: scheduled__2021-03-30T09:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 10:04:40,289[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:09:40,317[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:10:01,372[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:10:01,375[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:10:01,377[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:10:01,379[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:10:01,381[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 10, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 10:10:01,381[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:10:01,384[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:10:02,808[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:10:02,920[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:10:02,922[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T10:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:10:03,839[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 10:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:10:04,014[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:10:04,015[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:10:04,015[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:10:04,016[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:10:04,017[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 10, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 10:10:04,017[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:10:04,021[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:10:05,252[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:10:05,303[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:10:05,303[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T10:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:10:48,755[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 10:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:10:49,150[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:10:49,152[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:10:49,152[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:10:49,152[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:10:49,154[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 10, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 10:10:49,154[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:10:49,157[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:10:50,343[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:10:50,452[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:10:50,452[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T10:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:12:57,794[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 10:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:12:57,814[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28932) last sent a heartbeat 128.91 seconds ago! Restarting it[0m
[[34m2021-03-30 10:12:57,818[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28932[0m
[[34m2021-03-30 10:12:58,031[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28932, status='terminated', exitcode=0, started='10:03:05') (28932) terminated with exit code 0[0m
[[34m2021-03-30 10:12:58,036[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10520[0m
[[34m2021-03-30 10:12:58,047[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 10:12:58,062] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 10:12:58,593[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 10:00:00+00:00: scheduled__2021-03-30T10:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 10:14:40,344[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:19:40,492[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:20:01,101[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:20:01,103[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:20:01,103[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:20:01,103[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:20:01,105[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 10, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 10:20:01,105[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:20:01,108[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:20:02,581[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:20:02,673[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:20:02,674[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T10:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:20:03,568[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 10:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:20:03,752[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:20:03,753[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:20:03,753[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:20:03,754[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:20:03,755[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 10, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 10:20:03,755[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:20:03,758[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:20:04,828[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:20:04,972[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:20:04,973[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T10:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:20:50,599[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 10:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:20:51,082[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:20:51,084[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:20:51,084[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:20:51,084[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:20:51,086[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 10, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 10:20:51,086[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:20:51,089[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:20:52,206[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:20:52,287[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:20:52,287[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T10:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:23:01,769[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 10:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:23:01,790[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10520) last sent a heartbeat 131.04 seconds ago! Restarting it[0m
[[34m2021-03-30 10:23:01,796[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10520[0m
[[34m2021-03-30 10:23:02,010[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10520, status='terminated', exitcode=0, started='10:12:57') (10520) terminated with exit code 0[0m
[[34m2021-03-30 10:23:02,013[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24321[0m
[[34m2021-03-30 10:23:02,022[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 10:23:02,037] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 10:23:02,525[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 10:10:00+00:00: scheduled__2021-03-30T10:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 10:24:40,530[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:29:40,561[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:30:01,925[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:30:01,926[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:30:01,927[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:30:01,927[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:30:01,929[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 10, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 10:30:01,929[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:30:01,932[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:30:03,285[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:30:03,338[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:30:03,338[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T10:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:30:04,138[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 10:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:30:04,307[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:30:04,309[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:30:04,309[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:30:04,309[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:30:04,311[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 10, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 10:30:04,312[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:30:04,315[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:30:05,215[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:30:05,264[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:30:05,264[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T10:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:30:46,692[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 10:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:30:47,155[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:30:47,157[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:30:47,157[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:30:47,157[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:30:47,158[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 10, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 10:30:47,159[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:30:47,162[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:30:48,835[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:30:48,969[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:30:48,970[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T10:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:32:56,432[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 10:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:32:56,451[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24321) last sent a heartbeat 129.62 seconds ago! Restarting it[0m
[[34m2021-03-30 10:32:56,456[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24321[0m
[[34m2021-03-30 10:32:56,629[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24321, status='terminated', exitcode=0, started='10:23:01') (24321) terminated with exit code 0[0m
[[34m2021-03-30 10:32:56,633[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5935[0m
[[34m2021-03-30 10:32:56,642[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 10:32:56,656] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 10:32:57,151[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 10:20:00+00:00: scheduled__2021-03-30T10:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 10:34:40,692[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:39:40,718[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:40:01,184[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:40:01,187[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:40:01,188[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:40:01,189[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:40:01,192[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 10, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 10:40:01,193[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:40:01,198[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:40:02,938[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:40:03,020[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:40:03,021[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T10:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:40:03,930[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 10:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:40:04,091[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:40:04,094[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:40:04,095[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:40:04,097[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:40:04,100[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 10, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 10:40:04,101[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:40:04,105[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:40:05,423[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:40:05,537[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:40:05,538[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T10:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:40:51,577[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 10:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:40:52,056[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:40:52,057[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:40:52,057[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:40:52,058[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:40:52,060[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 10, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 10:40:52,060[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:40:52,063[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:40:53,296[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:40:53,376[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:40:53,376[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T10:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:43:04,013[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 10:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:43:04,033[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5935) last sent a heartbeat 132.31 seconds ago! Restarting it[0m
[[34m2021-03-30 10:43:04,038[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5935[0m
[[34m2021-03-30 10:43:04,212[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5935, status='terminated', exitcode=0, started='10:32:55') (5935) terminated with exit code 0[0m
[[34m2021-03-30 10:43:04,214[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19727[0m
[[34m2021-03-30 10:43:04,224[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 10:43:04,238] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 10:43:04,716[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 10:30:00+00:00: scheduled__2021-03-30T10:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 10:44:40,858[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:49:40,898[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:50:01,327[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:50:01,328[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:50:01,328[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:50:01,329[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:50:01,330[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 10, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 10:50:01,331[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:50:01,334[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:50:02,957[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:50:03,010[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:50:03,010[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T10:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:50:03,621[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 10:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:50:03,788[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:50:03,789[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:50:03,790[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:50:03,790[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:50:03,792[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 10, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 10:50:03,792[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:50:03,795[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:50:04,381[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:50:04,427[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:50:04,427[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T10:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:50:44,515[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 10:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:50:44,934[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:50:44,936[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 10:50:44,936[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 10:50:44,937[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 10:50:44,939[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 10, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 10:50:44,939[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:50:44,943[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 10:50:46,421[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 10:50:46,529[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 10:50:46,532[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T10:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 10:52:58,619[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 10:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 10:52:58,637[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19727) last sent a heartbeat 133.96 seconds ago! Restarting it[0m
[[34m2021-03-30 10:52:58,642[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19727[0m
[[34m2021-03-30 10:52:58,826[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19727, status='terminated', exitcode=0, started='10:43:03') (19727) terminated with exit code 0[0m
[[34m2021-03-30 10:52:58,829[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3265[0m
[[34m2021-03-30 10:52:58,838[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 10:52:58,850] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 10:52:59,337[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 10:40:00+00:00: scheduled__2021-03-30T10:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 10:54:41,055[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 10:59:41,195[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:00:01,973[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:00:01,974[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:00:01,974[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:00:01,974[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:00:01,976[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 10, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 11:00:01,976[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:00:01,980[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:00:03,998[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:00:04,147[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:00:04,153[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T10:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:00:04,942[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 10:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:00:05,108[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:00:05,110[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:00:05,110[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:00:05,111[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:00:05,113[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 10, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 11:00:05,113[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:00:05,120[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:00:06,148[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:00:06,195[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:00:06,195[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T10:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:00:50,682[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 10:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:00:51,156[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:00:51,158[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:00:51,158[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:00:51,158[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:00:51,161[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 10, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 11:00:51,161[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:00:51,164[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:00:52,382[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:00:52,468[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:00:52,468[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T10:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:03:05,761[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 10:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:03:05,783[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3265) last sent a heartbeat 134.96 seconds ago! Restarting it[0m
[[34m2021-03-30 11:03:05,791[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3265[0m
[[34m2021-03-30 11:03:06,009[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3265, status='terminated', exitcode=0, started='10:52:58') (3265) terminated with exit code 0[0m
[[34m2021-03-30 11:03:06,016[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17874[0m
[[34m2021-03-30 11:03:06,037[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 11:03:06,053] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 11:03:06,637[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 10:50:00+00:00: scheduled__2021-03-30T10:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 11:04:41,328[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:09:41,421[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:10:01,724[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:10:01,725[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:10:01,725[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:10:01,725[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:10:01,727[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 11, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 11:10:01,727[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:10:01,730[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:10:02,890[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:10:02,949[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:10:02,949[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T11:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:10:03,626[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 11:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:10:03,772[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:10:03,774[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:10:03,774[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:10:03,774[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:10:03,776[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 11, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 11:10:03,776[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:10:03,779[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:10:04,380[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:10:04,446[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:10:04,446[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T11:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:10:44,179[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 11:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:10:44,735[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:10:44,737[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:10:44,737[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:10:44,737[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:10:44,739[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 11, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 11:10:44,739[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:10:44,744[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:10:46,047[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:10:46,195[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:10:46,200[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T11:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:13:05,144[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 11:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:13:05,164[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17874) last sent a heartbeat 140.82 seconds ago! Restarting it[0m
[[34m2021-03-30 11:13:05,169[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17874[0m
[[34m2021-03-30 11:13:05,383[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17874, status='terminated', exitcode=0, started='11:03:05') (17874) terminated with exit code 0[0m
[[34m2021-03-30 11:13:05,386[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31600[0m
[[34m2021-03-30 11:13:05,395[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 11:13:05,409] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 11:13:05,880[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 11:00:00+00:00: scheduled__2021-03-30T11:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 11:14:41,555[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:19:41,689[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:20:01,262[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:20:01,265[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:20:01,266[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:20:01,267[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:20:01,270[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 11, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 11:20:01,275[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:20:01,278[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:20:03,118[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:20:03,216[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:20:03,218[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T11:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:20:03,902[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 11:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:20:04,047[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:20:04,049[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:20:04,049[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:20:04,049[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:20:04,052[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 11, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 11:20:04,052[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:20:04,055[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:20:04,640[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:20:04,693[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:20:04,693[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T11:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:20:47,857[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 11:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:20:48,318[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:20:48,320[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:20:48,320[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:20:48,320[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:20:48,322[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 11, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 11:20:48,322[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:20:48,327[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:20:49,347[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:20:49,427[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:20:49,427[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T11:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:22:56,699[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 11:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:22:56,719[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31600) last sent a heartbeat 128.72 seconds ago! Restarting it[0m
[[34m2021-03-30 11:22:56,728[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31600[0m
[[34m2021-03-30 11:22:56,983[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31600, status='terminated', exitcode=0, started='11:13:04') (31600) terminated with exit code 0[0m
[[34m2021-03-30 11:22:56,987[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12672[0m
[[34m2021-03-30 11:22:57,001[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 11:22:57,016] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 11:22:57,567[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 11:10:00+00:00: scheduled__2021-03-30T11:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 11:24:41,717[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:29:41,745[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:30:01,796[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:30:01,797[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:30:01,797[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:30:01,798[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:30:01,799[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 11, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 11:30:01,799[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:30:01,804[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:30:03,247[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:30:03,332[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:30:03,332[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T11:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:30:04,251[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 11:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:30:04,439[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:30:04,446[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:30:04,448[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:30:04,449[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:30:04,452[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 11, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 11:30:04,453[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:30:04,457[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:30:05,454[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:30:05,530[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:30:05,531[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T11:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:30:49,187[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 11:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:30:49,723[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:30:49,725[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:30:49,725[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:30:49,725[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:30:49,727[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 11, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 11:30:49,727[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:30:49,731[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:30:50,751[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:30:50,843[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:30:50,843[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T11:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:33:00,456[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 11:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:33:00,474[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12672) last sent a heartbeat 131.13 seconds ago! Restarting it[0m
[[34m2021-03-30 11:33:00,478[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12672[0m
[[34m2021-03-30 11:33:00,650[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12672, status='terminated', exitcode=0, started='11:22:56') (12672) terminated with exit code 0[0m
[[34m2021-03-30 11:33:00,653[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26345[0m
[[34m2021-03-30 11:33:00,663[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 11:33:00,677] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 11:33:01,248[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 11:20:00+00:00: scheduled__2021-03-30T11:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 11:34:41,880[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:39:42,015[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:40:01,727[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:40:01,729[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:40:01,729[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:40:01,729[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:40:01,731[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 11, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 11:40:01,731[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:40:01,735[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:40:03,005[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:40:03,058[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:40:03,058[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T11:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:40:03,710[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 11:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:40:03,861[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:40:03,863[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:40:03,863[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:40:03,863[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:40:03,866[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 11, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 11:40:03,866[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:40:03,869[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:40:04,458[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:40:04,504[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:40:04,504[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T11:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:40:46,226[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 11:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:40:46,759[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:40:46,761[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:40:46,761[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:40:46,761[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:40:46,763[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 11, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 11:40:46,763[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:40:46,766[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:40:48,413[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:40:48,514[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:40:48,514[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T11:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:42:55,998[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 11:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:42:56,015[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26345) last sent a heartbeat 129.64 seconds ago! Restarting it[0m
[[34m2021-03-30 11:42:56,021[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26345[0m
[[34m2021-03-30 11:42:56,193[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26345, status='terminated', exitcode=0, started='11:32:59') (26345) terminated with exit code 0[0m
[[34m2021-03-30 11:42:56,196[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 7589[0m
[[34m2021-03-30 11:42:56,207[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 11:42:56,220] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 11:42:56,724[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 11:30:00+00:00: scheduled__2021-03-30T11:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 11:44:42,145[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:49:42,278[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:50:02,096[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:50:02,099[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:50:02,099[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:50:02,099[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:50:02,100[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 11, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 11:50:02,101[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:50:02,105[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:50:03,491[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:50:03,545[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:50:03,545[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T11:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:50:04,416[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 11:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:50:04,576[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:50:04,579[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:50:04,580[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:50:04,582[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:50:04,585[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 11, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 11:50:04,586[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:50:04,589[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:50:05,786[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:50:05,875[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:50:05,876[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T11:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:50:50,709[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 11:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:50:51,186[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:50:51,187[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 11:50:51,187[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 11:50:51,187[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 11:50:51,190[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 11, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 11:50:51,190[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:50:51,193[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 11:50:52,463[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 11:50:52,553[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 11:50:52,553[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T11:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 11:53:03,622[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 11:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 11:53:03,643[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=7589) last sent a heartbeat 132.79 seconds ago! Restarting it[0m
[[34m2021-03-30 11:53:03,649[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 7589[0m
[[34m2021-03-30 11:53:03,864[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=7589, status='terminated', exitcode=0, started='11:42:55') (7589) terminated with exit code 0[0m
[[34m2021-03-30 11:53:03,868[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21258[0m
[[34m2021-03-30 11:53:03,890[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 11:53:03,908] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 11:53:04,504[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 11:40:00+00:00: scheduled__2021-03-30T11:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 11:54:42,305[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 11:59:42,338[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:00:01,902[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:00:01,903[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:00:01,903[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:00:01,904[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:00:01,906[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 11, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 12:00:01,906[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:00:01,909[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:00:03,121[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:00:03,174[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:00:03,174[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T11:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:00:03,788[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 11:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:00:03,957[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:00:03,959[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:00:03,959[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:00:03,960[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:00:03,962[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 11, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 12:00:03,962[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:00:03,965[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:00:04,655[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:00:04,732[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:00:04,733[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T11:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:00:45,664[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 11:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:00:46,202[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:00:46,204[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:00:46,204[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:00:46,205[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:00:46,206[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 11, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 12:00:46,206[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:00:46,210[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:00:47,820[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:00:47,935[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:00:47,938[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T11:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:02:55,580[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 11:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:02:55,597[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21258) last sent a heartbeat 129.78 seconds ago! Restarting it[0m
[[34m2021-03-30 12:02:55,602[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21258[0m
[[34m2021-03-30 12:02:55,774[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21258, status='terminated', exitcode=0, started='11:53:03') (21258) terminated with exit code 0[0m
[[34m2021-03-30 12:02:55,777[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2339[0m
[[34m2021-03-30 12:02:55,787[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 12:02:55,802] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 12:02:56,325[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 11:50:00+00:00: scheduled__2021-03-30T11:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 12:04:42,472[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:09:42,499[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:10:01,920[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:10:01,925[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:10:01,928[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:10:01,928[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:10:01,930[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 12, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 12:10:01,930[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:10:01,933[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:10:03,181[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:10:03,235[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:10:03,235[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T12:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:10:03,932[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:10:04,078[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:10:04,079[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:10:04,079[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:10:04,080[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:10:04,082[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 12, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 12:10:04,082[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:10:04,086[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:10:04,707[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:10:04,770[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:10:04,770[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T12:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:10:48,063[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:10:48,576[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:10:48,578[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:10:48,578[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:10:48,578[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:10:48,580[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 12, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 12:10:48,580[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:10:48,583[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:10:49,612[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:10:49,689[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:10:49,689[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T12:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:13:02,764[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:13:02,781[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2339) last sent a heartbeat 134.56 seconds ago! Restarting it[0m
[[34m2021-03-30 12:13:02,785[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2339[0m
[[34m2021-03-30 12:13:02,958[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2339, status='terminated', exitcode=0, started='12:02:54') (2339) terminated with exit code 0[0m
[[34m2021-03-30 12:13:02,960[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16073[0m
[[34m2021-03-30 12:13:02,969[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 12:13:02,983] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 12:13:03,507[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 12:00:00+00:00: scheduled__2021-03-30T12:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 12:14:42,526[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:19:42,562[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:20:01,529[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:20:01,530[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:20:01,531[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:20:01,531[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:20:01,532[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 12, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 12:20:01,533[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:20:01,535[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:20:02,862[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:20:02,924[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:20:02,924[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T12:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:20:03,574[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 12:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:20:03,728[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:20:03,729[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:20:03,730[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:20:03,730[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:20:03,731[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 12, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 12:20:03,731[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:20:03,735[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:20:04,376[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:20:04,422[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:20:04,422[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T12:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:20:47,584[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 12:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:20:48,110[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:20:48,112[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:20:48,112[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:20:48,113[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:20:48,114[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 12, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 12:20:48,114[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:20:48,118[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:20:49,628[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:20:49,747[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:20:49,747[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T12:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:23:05,899[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 12:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:23:05,916[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16073) last sent a heartbeat 138.18 seconds ago! Restarting it[0m
[[34m2021-03-30 12:23:05,920[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16073[0m
[[34m2021-03-30 12:23:06,133[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16073, status='terminated', exitcode=0, started='12:13:02') (16073) terminated with exit code 0[0m
[[34m2021-03-30 12:23:06,136[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 29741[0m
[[34m2021-03-30 12:23:06,150[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 12:23:06,171] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 12:23:06,688[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 12:10:00+00:00: scheduled__2021-03-30T12:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 12:24:42,705[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:29:42,734[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:30:01,747[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:30:01,753[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:30:01,754[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:30:01,754[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:30:01,760[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 12, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 12:30:01,761[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:30:01,766[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:30:03,405[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:30:03,512[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:30:03,514[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T12:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:30:04,446[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 12:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:30:04,604[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:30:04,605[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:30:04,606[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:30:04,606[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:30:04,607[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 12, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 12:30:04,607[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:30:04,610[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:30:05,224[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:30:05,275[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:30:05,275[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T12:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:30:48,376[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 12:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:30:48,883[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:30:48,884[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:30:48,884[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:30:48,884[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:30:48,886[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 12, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 12:30:48,886[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:30:48,890[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:30:50,120[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:30:50,240[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:30:50,240[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T12:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:32:58,564[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 12:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:32:58,586[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=29741) last sent a heartbeat 130.07 seconds ago! Restarting it[0m
[[34m2021-03-30 12:32:58,592[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 29741[0m
[[34m2021-03-30 12:32:58,848[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=29741, status='terminated', exitcode=0, started='12:23:05') (29741) terminated with exit code 0[0m
[[34m2021-03-30 12:32:58,852[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10930[0m
[[34m2021-03-30 12:32:58,876[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 12:32:58,894] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 12:32:59,418[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 12:20:00+00:00: scheduled__2021-03-30T12:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 12:34:42,762[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:39:42,790[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:40:01,444[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:40:01,447[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:40:01,449[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:40:01,450[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:40:01,453[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 12, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 12:40:01,454[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:40:01,458[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:40:03,266[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:40:03,365[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:40:03,365[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T12:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:40:04,275[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 12:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:40:04,467[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:40:04,469[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:40:04,469[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:40:04,469[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:40:04,471[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 12, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 12:40:04,471[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:40:04,474[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:40:05,866[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:40:05,983[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:40:05,984[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T12:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:40:51,660[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 12:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:40:52,134[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:40:52,136[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:40:52,136[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:40:52,136[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:40:52,138[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 12, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 12:40:52,138[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:40:52,141[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:40:53,137[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:40:53,217[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:40:53,217[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T12:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:43:03,526[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 12:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:43:03,543[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10930) last sent a heartbeat 131.74 seconds ago! Restarting it[0m
[[34m2021-03-30 12:43:03,551[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10930[0m
[[34m2021-03-30 12:43:03,724[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10930, status='terminated', exitcode=0, started='12:32:58') (10930) terminated with exit code 0[0m
[[34m2021-03-30 12:43:03,727[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24424[0m
[[34m2021-03-30 12:43:03,737[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 12:43:03,750] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 12:43:04,284[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 12:30:00+00:00: scheduled__2021-03-30T12:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 12:44:42,922[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:49:42,960[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:50:01,929[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:50:01,931[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:50:01,931[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:50:01,931[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:50:01,933[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 12, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 12:50:01,933[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:50:01,937[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:50:03,036[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:50:03,101[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:50:03,101[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T12:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:50:03,724[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 12:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:50:03,884[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:50:03,885[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:50:03,886[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:50:03,886[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:50:03,888[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 12, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 12:50:03,888[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:50:03,891[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:50:04,487[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:50:04,535[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:50:04,535[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T12:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 12:58:26,936[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 12:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 12:58:26,957[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24424) last sent a heartbeat 503.10 seconds ago! Restarting it[0m
[[34m2021-03-30 12:58:26,962[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24424[0m
[[34m2021-03-30 12:58:27,178[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24424, status='terminated', exitcode=0, started='12:43:02') (24424) terminated with exit code 0[0m
[[34m2021-03-30 12:58:27,181[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3931[0m
[[34m2021-03-30 12:58:27,206[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-03-30 12:58:27,225[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 12:58:27,228[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2021-03-30 12:58:27,231] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 12:58:27,235[0m] {[34mscheduler_job.py:[0m1901} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: build_sites.aurora 2021-03-30 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:58:27,834[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:58:27,835[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 12:58:27,836[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 12:58:27,836[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 12:58:27,837[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 12, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 12:58:27,837[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:58:27,845[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 12:58:29,541[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 12:58:29,637[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 12:58:29,637[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T12:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:00:37,257[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 12:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:00:37,281[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3931) last sent a heartbeat 129.85 seconds ago! Restarting it[0m
[[34m2021-03-30 13:00:37,286[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3931[0m
[[34m2021-03-30 13:00:37,499[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3931, status='terminated', exitcode=0, started='12:58:26') (3931) terminated with exit code 0[0m
[[34m2021-03-30 13:00:37,504[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8669[0m
[[34m2021-03-30 13:00:37,515[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 13:00:37,528] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 13:00:38,165[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 12:40:00+00:00: scheduled__2021-03-30T12:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 13:00:38,184[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:00:38,190[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:00:38,190[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:00:38,190[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:00:38,191[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 12, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 13:00:38,192[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:00:38,195[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:00:39,931[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:00:40,048[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:00:40,048[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T12:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:00:40,899[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 12:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:00:41,072[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:00:41,074[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:00:41,074[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:00:41,074[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:00:41,075[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 12, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 13:00:41,076[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:00:41,084[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:00:42,094[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:00:42,190[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:00:42,190[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T12:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:01:21,468[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 12:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:01:21,632[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:01:21,633[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:01:21,633[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:01:21,633[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:01:21,634[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 12, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 13:01:21,634[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:01:21,637[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:01:22,693[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:01:22,814[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:01:22,814[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T12:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:03:18,675[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 12:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:03:18,692[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8669) last sent a heartbeat 117.08 seconds ago! Restarting it[0m
[[34m2021-03-30 13:03:18,695[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8669[0m
[[34m2021-03-30 13:03:18,787[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8669, status='terminated', exitcode=0, started='13:00:36') (8669) terminated with exit code 0[0m
[[34m2021-03-30 13:03:18,790[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 13880[0m
[[34m2021-03-30 13:03:18,797[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 13:03:18,807] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 13:03:19,312[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 12:50:00+00:00: scheduled__2021-03-30T12:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 13:03:27,245[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:08:27,267[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:10:01,338[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:10:01,339[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:10:01,339[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:10:01,339[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:10:01,340[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 13, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 13:10:01,340[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:10:01,343[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:10:02,491[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:10:02,547[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:10:02,547[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T13:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:10:03,058[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 13:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:10:03,212[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:10:03,213[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:10:03,213[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:10:03,213[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:10:03,214[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 13, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 13:10:03,215[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:10:03,218[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:10:03,804[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:10:03,855[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:10:03,855[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T13:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:10:40,043[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 13:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:10:40,579[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:10:40,581[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:10:40,581[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:10:40,581[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:10:40,583[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 13, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 13:10:40,583[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:10:40,586[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:10:41,895[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:10:42,021[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:10:42,021[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T13:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:12:42,043[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 13:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:12:42,060[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=13880) last sent a heartbeat 121.86 seconds ago! Restarting it[0m
[[34m2021-03-30 13:12:42,065[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 13880[0m
[[34m2021-03-30 13:12:42,201[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=13880, status='terminated', exitcode=0, started='13:03:17') (13880) terminated with exit code 0[0m
[[34m2021-03-30 13:12:42,204[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24706[0m
[[34m2021-03-30 13:12:42,227[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 13:12:42,246] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 13:12:42,454[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 13:00:00+00:00: scheduled__2021-03-30T13:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 13:13:27,398[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:18:27,488[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:20:01,467[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:20:01,468[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:20:01,469[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:20:01,469[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:20:01,470[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 13, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 13:20:01,470[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:20:01,473[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:20:02,597[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:20:02,650[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:20:02,650[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T13:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:20:03,048[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 13:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:20:03,207[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:20:03,208[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:20:03,208[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:20:03,208[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:20:03,209[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 13, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 13:20:03,209[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:20:03,212[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:20:03,796[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:20:03,843[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:20:03,844[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T13:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:20:43,408[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 13:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:20:43,598[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:20:43,599[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:20:43,599[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:20:43,599[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:20:43,601[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 13, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 13:20:43,601[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:20:43,604[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:20:44,573[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:20:44,652[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:20:44,653[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T13:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:23:28,313[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 13:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:23:28,331[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24706) last sent a heartbeat 164.77 seconds ago! Restarting it[0m
[[34m2021-03-30 13:23:28,334[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24706[0m
[[34m2021-03-30 13:23:28,470[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24706, status='terminated', exitcode=0, started='13:12:41') (24706) terminated with exit code 0[0m
[[34m2021-03-30 13:23:28,472[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 4496[0m
[[34m2021-03-30 13:23:28,480[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 13:23:28,497] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 13:23:28,499[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:23:28,503[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2021-03-30 13:23:28,700[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 13:10:00+00:00: scheduled__2021-03-30T13:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 13:28:28,652[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:30:01,384[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:30:01,386[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:30:01,386[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:30:01,386[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:30:01,388[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 13, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 13:30:01,388[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:30:01,391[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:30:03,056[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:30:03,145[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:30:03,146[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T13:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:30:03,773[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 13:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:30:03,960[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:30:03,961[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:30:03,961[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:30:03,961[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:30:03,963[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 13, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 13:30:03,963[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:30:03,966[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:30:04,951[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:30:05,044[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:30:05,045[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T13:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:30:45,474[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 13:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:30:45,962[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:30:45,963[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:30:45,964[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:30:45,964[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:30:45,965[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 13, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 13:30:45,965[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:30:45,968[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:30:47,102[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:30:47,181[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:30:47,181[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T13:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:32:43,941[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 13:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:32:43,960[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=4496) last sent a heartbeat 118.33 seconds ago! Restarting it[0m
[[34m2021-03-30 13:32:43,965[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 4496[0m
[[34m2021-03-30 13:32:44,110[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=4496, status='terminated', exitcode=0, started='13:23:27') (4496) terminated with exit code 0[0m
[[34m2021-03-30 13:32:44,112[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15438[0m
[[34m2021-03-30 13:32:44,134[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 13:32:44,153] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 13:32:44,336[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 13:20:00+00:00: scheduled__2021-03-30T13:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 13:33:28,664[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:38:28,795[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:40:01,269[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:40:01,271[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:40:01,271[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:40:01,271[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:40:01,273[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 13, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 13:40:01,273[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:40:01,277[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:40:02,343[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:40:02,398[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:40:02,399[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T13:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:40:02,803[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 13:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:40:02,953[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:40:02,954[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:40:02,954[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:40:02,954[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:40:02,955[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 13, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 13:40:02,955[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:40:02,958[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:40:03,891[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:40:03,961[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:40:03,961[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T13:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:40:40,991[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 13:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:40:41,156[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:40:41,157[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:40:41,157[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:40:41,158[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:40:41,159[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 13, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 13:40:41,159[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:40:41,162[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:40:42,098[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:40:42,172[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:40:42,173[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T13:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:42:50,584[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 13:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:42:50,610[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15438) last sent a heartbeat 129.48 seconds ago! Restarting it[0m
[[34m2021-03-30 13:42:50,616[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15438[0m
[[34m2021-03-30 13:42:50,789[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15438, status='terminated', exitcode=0, started='13:32:43') (15438) terminated with exit code 0[0m
[[34m2021-03-30 13:42:50,792[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26955[0m
[[34m2021-03-30 13:42:50,801[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 13:42:50,815] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 13:42:51,032[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 13:30:00+00:00: scheduled__2021-03-30T13:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 13:43:28,926[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:48:28,957[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:50:01,408[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:50:01,409[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:50:01,409[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:50:01,409[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:50:01,411[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 13, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 13:50:01,411[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:50:01,414[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:50:03,016[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:50:03,099[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:50:03,099[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T13:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:50:03,489[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 13:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:50:03,634[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:50:03,635[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:50:03,636[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:50:03,636[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:50:03,637[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 13, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 13:50:03,637[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:50:03,640[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:50:04,221[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:50:04,270[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:50:04,270[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T13:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:50:44,378[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 13:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:50:44,560[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:50:44,562[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 13:50:44,562[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 13:50:44,562[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 13:50:44,564[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 13, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 13:50:44,564[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:50:44,567[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 13:50:45,708[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 13:50:45,792[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 13:50:45,792[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T13:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 13:52:46,023[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 13:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 13:52:46,038[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26955) last sent a heartbeat 121.52 seconds ago! Restarting it[0m
[[34m2021-03-30 13:52:46,042[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26955[0m
[[34m2021-03-30 13:52:46,214[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26955, status='terminated', exitcode=0, started='13:42:49') (26955) terminated with exit code 0[0m
[[34m2021-03-30 13:52:46,216[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6058[0m
[[34m2021-03-30 13:52:46,226[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 13:52:46,238] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 13:52:46,412[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 13:40:00+00:00: scheduled__2021-03-30T13:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 13:53:29,088[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 13:58:29,115[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:00:02,037[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:00:02,038[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:00:02,038[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:00:02,038[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:00:02,039[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 13, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 14:00:02,039[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:00:02,042[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:00:03,362[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:00:03,417[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:00:03,417[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T13:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:00:03,863[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 13:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:00:04,008[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:00:04,009[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:00:04,010[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:00:04,010[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:00:04,011[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 13, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 14:00:04,011[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:00:04,014[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:00:04,600[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:00:04,648[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:00:04,648[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T13:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:00:40,385[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 13:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:00:40,550[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:00:40,551[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:00:40,551[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:00:40,551[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:00:40,552[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 13, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 14:00:40,552[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:00:40,555[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:00:41,678[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:00:41,785[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:00:41,785[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T13:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:02:39,224[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 13:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:02:39,239[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6058) last sent a heartbeat 118.71 seconds ago! Restarting it[0m
[[34m2021-03-30 14:02:39,243[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6058[0m
[[34m2021-03-30 14:02:39,375[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6058, status='terminated', exitcode=0, started='13:52:45') (6058) terminated with exit code 0[0m
[[34m2021-03-30 14:02:39,377[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17432[0m
[[34m2021-03-30 14:02:39,387[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 14:02:39,399] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 14:02:39,549[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 13:50:00+00:00: scheduled__2021-03-30T13:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 14:03:29,245[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:08:29,376[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:10:01,964[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:10:01,965[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:10:01,965[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:10:01,965[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:10:01,966[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 14, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 14:10:01,966[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:10:01,969[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:10:03,056[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:10:03,108[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:10:03,108[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T14:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:10:03,565[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 14:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:10:03,710[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:10:03,711[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:10:03,711[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:10:03,711[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:10:03,712[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 14, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 14:10:03,712[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:10:03,715[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:10:04,457[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:10:04,525[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:10:04,525[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T14:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:10:43,882[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 14:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:10:44,046[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:10:44,047[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:10:44,047[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:10:44,047[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:10:44,048[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 14, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 14:10:44,048[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:10:44,053[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:10:44,685[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:10:44,735[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:10:44,735[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T14:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:12:50,429[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 14:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:12:50,445[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17432) last sent a heartbeat 126.42 seconds ago! Restarting it[0m
[[34m2021-03-30 14:12:50,448[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17432[0m
[[34m2021-03-30 14:12:50,580[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17432, status='terminated', exitcode=0, started='14:02:38') (17432) terminated with exit code 0[0m
[[34m2021-03-30 14:12:50,582[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28890[0m
[[34m2021-03-30 14:12:50,591[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 14:12:50,603] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 14:12:50,753[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 14:00:00+00:00: scheduled__2021-03-30T14:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 14:13:29,522[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:18:29,550[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:20:01,464[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:20:01,465[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:20:01,465[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:20:01,465[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:20:01,466[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 14, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 14:20:01,466[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:20:01,469[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:20:02,680[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:20:02,733[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:20:02,733[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T14:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:20:03,192[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 14:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:20:03,337[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:20:03,338[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:20:03,338[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:20:03,338[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:20:03,339[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 14, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 14:20:03,339[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:20:03,343[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:20:03,928[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:20:03,979[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:20:03,979[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T14:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:20:42,837[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 14:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:20:43,037[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:20:43,038[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:20:43,039[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:20:43,039[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:20:43,041[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 14, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 14:20:43,041[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:20:43,045[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:20:44,460[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:20:44,539[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:20:44,539[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T14:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:22:39,727[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 14:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:22:39,745[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28890) last sent a heartbeat 116.74 seconds ago! Restarting it[0m
[[34m2021-03-30 14:22:39,750[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28890[0m
[[34m2021-03-30 14:22:39,924[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28890, status='terminated', exitcode=0, started='14:12:49') (28890) terminated with exit code 0[0m
[[34m2021-03-30 14:22:39,927[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 7860[0m
[[34m2021-03-30 14:22:39,950[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 14:22:39,968] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 14:22:40,145[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 14:10:00+00:00: scheduled__2021-03-30T14:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 14:23:29,591[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:28:29,722[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:30:01,047[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:30:01,048[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:30:01,048[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:30:01,048[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:30:01,049[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 14, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 14:30:01,049[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:30:01,052[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:30:02,183[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:30:02,240[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:30:02,241[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T14:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:30:02,707[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 14:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:30:02,853[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:30:02,854[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:30:02,854[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:30:02,854[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:30:02,855[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 14, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 14:30:02,855[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:30:02,858[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:30:03,574[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:30:03,620[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:30:03,621[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T14:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:30:42,379[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 14:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:30:42,543[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:30:42,544[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:30:42,544[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:30:42,544[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:30:42,546[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 14, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 14:30:42,546[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:30:42,549[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:30:43,736[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:30:43,813[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:30:43,813[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T14:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:32:41,598[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 14:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:32:41,614[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=7860) last sent a heartbeat 119.09 seconds ago! Restarting it[0m
[[34m2021-03-30 14:32:41,617[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 7860[0m
[[34m2021-03-30 14:32:41,749[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=7860, status='terminated', exitcode=0, started='14:22:39') (7860) terminated with exit code 0[0m
[[34m2021-03-30 14:32:41,752[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19235[0m
[[34m2021-03-30 14:32:41,763[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 14:32:41,778] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 14:32:41,954[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 14:20:00+00:00: scheduled__2021-03-30T14:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 14:33:29,852[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:38:29,951[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:40:01,109[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:40:01,111[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:40:01,111[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:40:01,111[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:40:01,113[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 14, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 14:40:01,113[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:40:01,124[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:40:02,728[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:40:02,823[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:40:02,824[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T14:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:40:03,579[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 14:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:40:03,771[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:40:03,777[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:40:03,777[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:40:03,777[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:40:03,778[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 14, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 14:40:03,778[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:40:03,781[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:40:04,827[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:40:04,901[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:40:04,901[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T14:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:40:46,720[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 14:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:40:46,897[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:40:46,898[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:40:46,898[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:40:46,898[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:40:46,899[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 14, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 14:40:46,899[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:40:46,902[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:40:48,162[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:40:48,248[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:40:48,248[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T14:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:42:44,336[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 14:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:42:44,352[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19235) last sent a heartbeat 117.49 seconds ago! Restarting it[0m
[[34m2021-03-30 14:42:44,355[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19235[0m
[[34m2021-03-30 14:42:44,528[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19235, status='terminated', exitcode=0, started='14:32:40') (19235) terminated with exit code 0[0m
[[34m2021-03-30 14:42:44,530[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30654[0m
[[34m2021-03-30 14:42:44,538[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 14:42:44,551] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 14:42:44,724[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 14:30:00+00:00: scheduled__2021-03-30T14:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 14:43:30,083[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:48:30,111[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:50:01,760[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:50:01,761[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:50:01,761[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:50:01,761[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:50:01,762[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 14, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 14:50:01,762[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:50:01,765[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:50:02,971[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:50:03,022[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:50:03,022[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T14:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:50:03,482[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 14:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:50:03,631[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:50:03,632[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:50:03,632[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:50:03,632[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:50:03,633[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 14, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 14:50:03,633[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:50:03,636[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:50:04,223[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:50:04,269[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:50:04,269[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T14:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:50:40,883[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 14:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:50:41,047[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:50:41,048[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 14:50:41,049[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 14:50:41,049[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 14:50:41,050[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 14, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 14:50:41,050[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:50:41,053[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 14:50:41,769[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 14:50:41,848[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 14:50:41,848[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T14:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 14:52:41,954[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 14:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 14:52:41,970[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30654) last sent a heartbeat 120.95 seconds ago! Restarting it[0m
[[34m2021-03-30 14:52:41,973[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30654[0m
[[34m2021-03-30 14:52:42,105[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30654, status='terminated', exitcode=0, started='14:42:43') (30654) terminated with exit code 0[0m
[[34m2021-03-30 14:52:42,107[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 9733[0m
[[34m2021-03-30 14:52:42,116[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 14:52:42,128] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 14:52:42,279[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 14:40:00+00:00: scheduled__2021-03-30T14:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 14:53:30,241[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 14:58:30,371[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:00:01,867[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:00:01,868[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:00:01,868[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:00:01,868[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:00:01,869[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 14, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 15:00:01,869[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:00:01,872[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:00:02,954[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:00:03,009[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:00:03,009[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T14:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:00:03,462[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 14:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:00:03,622[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:00:03,623[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:00:03,624[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:00:03,624[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:00:03,625[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 14, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 15:00:03,625[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:00:03,628[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:00:04,225[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:00:04,273[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:00:04,273[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T14:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:00:44,517[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 14:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:00:44,702[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:00:44,703[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:00:44,704[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:00:44,704[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:00:44,705[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 14, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 15:00:44,705[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:00:44,709[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:00:45,789[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:00:45,870[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:00:45,870[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T14:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:02:46,126[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 14:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:02:46,142[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=9733) last sent a heartbeat 121.48 seconds ago! Restarting it[0m
[[34m2021-03-30 15:02:46,145[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 9733[0m
[[34m2021-03-30 15:02:46,318[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=9733, status='terminated', exitcode=0, started='14:52:41') (9733) terminated with exit code 0[0m
[[34m2021-03-30 15:02:46,320[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21138[0m
[[34m2021-03-30 15:02:46,329[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 15:02:46,341] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 15:02:46,516[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 14:50:00+00:00: scheduled__2021-03-30T14:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 15:03:30,509[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:08:30,538[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:10:01,075[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:10:01,076[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:10:01,076[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:10:01,076[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:10:01,077[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 15, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 15:10:01,077[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:10:01,080[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:10:02,409[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:10:02,463[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:10:02,463[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T15:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:10:03,004[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 15:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:10:03,159[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:10:03,160[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:10:03,160[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:10:03,160[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:10:03,161[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 15, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 15:10:03,161[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:10:03,164[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:10:03,752[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:10:03,800[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:10:03,800[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T15:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:10:40,816[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 15:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:10:41,008[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:10:41,010[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:10:41,010[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:10:41,010[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:10:41,012[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 15, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 15:10:41,012[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:10:41,015[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:10:42,416[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:10:42,528[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:10:42,528[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T15:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:12:39,047[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 15:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:12:39,063[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21138) last sent a heartbeat 118.09 seconds ago! Restarting it[0m
[[34m2021-03-30 15:12:39,066[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21138[0m
[[34m2021-03-30 15:12:39,198[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21138, status='terminated', exitcode=0, started='15:02:45') (21138) terminated with exit code 0[0m
[[34m2021-03-30 15:12:39,200[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 32402[0m
[[34m2021-03-30 15:12:39,211[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 15:12:39,233] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 15:12:39,409[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 15:00:00+00:00: scheduled__2021-03-30T15:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 15:13:30,628[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:18:30,655[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:20:01,138[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:20:01,139[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:20:01,139[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:20:01,140[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:20:01,140[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 15, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 15:20:01,141[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:20:01,144[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:20:02,421[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:20:02,479[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:20:02,479[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T15:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:20:02,943[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 15:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:20:03,087[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:20:03,088[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:20:03,088[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:20:03,089[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:20:03,090[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 15, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 15:20:03,090[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:20:03,093[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:20:03,736[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:20:03,804[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:20:03,804[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T15:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:20:43,819[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 15:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:20:43,984[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:20:43,985[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:20:43,986[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:20:43,986[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:20:43,987[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 15, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 15:20:43,987[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:20:43,990[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:20:45,011[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:20:45,115[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:20:45,115[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T15:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:22:45,697[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 15:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:22:45,713[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=32402) last sent a heartbeat 121.75 seconds ago! Restarting it[0m
[[34m2021-03-30 15:22:45,716[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 32402[0m
[[34m2021-03-30 15:22:45,849[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=32402, status='terminated', exitcode=0, started='15:12:38') (32402) terminated with exit code 0[0m
[[34m2021-03-30 15:22:45,851[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 11725[0m
[[34m2021-03-30 15:22:45,860[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 15:22:45,871] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 15:22:46,021[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 15:10:00+00:00: scheduled__2021-03-30T15:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 15:23:30,785[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:28:30,924[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:30:01,985[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:30:01,988[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:30:01,988[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:30:01,988[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:30:01,990[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 15, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 15:30:01,990[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:30:01,993[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:30:03,540[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:30:03,623[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:30:03,624[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T15:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:30:04,278[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 15:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:30:04,451[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:30:04,453[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:30:04,453[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:30:04,453[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:30:04,455[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 15, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 15:30:04,455[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:30:04,464[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:30:05,506[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:30:05,618[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:30:05,618[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T15:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:30:45,849[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 15:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:30:46,014[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:30:46,015[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:30:46,016[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:30:46,016[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:30:46,017[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 15, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 15:30:46,017[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:30:46,020[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:30:47,356[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:30:47,437[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:30:47,437[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T15:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:32:45,489[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 15:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:32:45,505[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=11725) last sent a heartbeat 119.51 seconds ago! Restarting it[0m
[[34m2021-03-30 15:32:45,508[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 11725[0m
[[34m2021-03-30 15:32:45,680[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=11725, status='terminated', exitcode=0, started='15:22:45') (11725) terminated with exit code 0[0m
[[34m2021-03-30 15:32:45,683[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23024[0m
[[34m2021-03-30 15:32:45,691[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 15:32:45,703] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 15:32:45,877[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 15:20:00+00:00: scheduled__2021-03-30T15:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 15:33:31,005[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:38:31,037[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:40:01,757[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:40:01,759[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:40:01,759[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:40:01,759[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:40:01,761[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 15, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 15:40:01,761[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:40:01,766[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:40:03,223[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:40:03,279[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:40:03,280[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T15:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:40:03,753[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 15:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:40:03,904[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:40:03,905[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:40:03,905[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:40:03,905[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:40:03,906[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 15, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 15:40:03,906[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:40:03,910[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:40:04,508[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:40:04,557[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:40:04,558[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T15:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:40:41,498[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 15:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:40:41,664[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:40:41,665[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:40:41,665[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:40:41,665[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:40:41,666[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 15, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 15:40:41,666[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:40:41,669[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:40:42,271[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:40:42,322[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:40:42,322[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T15:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:42:39,791[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 15:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:42:39,807[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23024) last sent a heartbeat 118.17 seconds ago! Restarting it[0m
[[34m2021-03-30 15:42:39,812[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23024[0m
[[34m2021-03-30 15:42:39,945[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23024, status='terminated', exitcode=0, started='15:32:44') (23024) terminated with exit code 0[0m
[[34m2021-03-30 15:42:39,948[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 1880[0m
[[34m2021-03-30 15:42:39,974[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 15:42:39,986] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 15:42:40,156[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 15:30:00+00:00: scheduled__2021-03-30T15:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 15:43:31,173[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:48:31,313[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:50:01,216[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:50:01,217[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:50:01,217[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:50:01,218[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:50:01,219[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 15, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 15:50:01,219[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:50:01,223[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:50:02,778[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:50:02,829[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:50:02,829[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T15:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:50:03,333[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 15:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:50:03,496[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:50:03,498[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:50:03,498[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:50:03,498[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:50:03,499[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 15, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 15:50:03,499[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:50:03,503[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:50:04,273[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:50:04,356[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:50:04,357[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T15:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:50:42,736[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 15:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:50:42,900[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:50:42,901[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 15:50:42,901[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 15:50:42,901[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 15:50:42,902[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 15, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 15:50:42,902[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:50:42,905[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 15:50:44,010[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 15:50:44,089[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 15:50:44,089[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T15:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 15:52:43,612[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 15:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 15:52:43,628[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=1880) last sent a heartbeat 120.75 seconds ago! Restarting it[0m
[[34m2021-03-30 15:52:43,631[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 1880[0m
[[34m2021-03-30 15:52:43,803[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=1880, status='terminated', exitcode=0, started='15:42:39') (1880) terminated with exit code 0[0m
[[34m2021-03-30 15:52:43,806[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 13486[0m
[[34m2021-03-30 15:52:43,815[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 15:52:43,828] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 15:52:44,002[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 15:40:00+00:00: scheduled__2021-03-30T15:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 15:53:31,451[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 15:58:31,596[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:00:01,804[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:00:01,805[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:00:01,805[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:00:01,805[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:00:01,806[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 15, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 16:00:01,806[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:00:01,809[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:00:03,051[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:00:03,104[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:00:03,104[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T15:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:00:03,553[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 15:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:00:03,716[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:00:03,717[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:00:03,717[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:00:03,717[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:00:03,718[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 15, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 16:00:03,718[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:00:03,721[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:00:04,386[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:00:04,454[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:00:04,454[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T15:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:00:46,086[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 15:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:00:46,291[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:00:46,292[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:00:46,292[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:00:46,292[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:00:46,296[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 15, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 16:00:46,296[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:00:46,300[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:00:47,784[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:00:47,889[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:00:47,890[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T15:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:02:48,290[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 15:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:02:48,305[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=13486) last sent a heartbeat 122.06 seconds ago! Restarting it[0m
[[34m2021-03-30 16:02:48,309[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 13486[0m
[[34m2021-03-30 16:02:48,482[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=13486, status='terminated', exitcode=0, started='15:52:43') (13486) terminated with exit code 0[0m
[[34m2021-03-30 16:02:48,485[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24902[0m
[[34m2021-03-30 16:02:48,514[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 16:02:48,543] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 16:02:48,721[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 15:50:00+00:00: scheduled__2021-03-30T15:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 16:03:31,734[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:08:31,764[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:10:01,557[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:10:01,558[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:10:01,558[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:10:01,558[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:10:01,560[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 16, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 16:10:01,560[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:10:01,563[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:10:02,750[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:10:02,805[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:10:02,805[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T16:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:10:03,250[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 16:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:10:03,418[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:10:03,419[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:10:03,419[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:10:03,419[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:10:03,421[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 16, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 16:10:03,421[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:10:03,424[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:10:04,218[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:10:04,264[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:10:04,264[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T16:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:10:43,882[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 16:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:10:44,355[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:10:44,356[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:10:44,356[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:10:44,356[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:10:44,357[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 16, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 16:10:44,358[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:10:44,360[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:10:45,206[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:10:45,283[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:10:45,283[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T16:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:12:51,255[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 16:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:12:51,270[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24902) last sent a heartbeat 127.25 seconds ago! Restarting it[0m
[[34m2021-03-30 16:12:51,274[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24902[0m
[[34m2021-03-30 16:12:51,406[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24902, status='terminated', exitcode=0, started='16:02:47') (24902) terminated with exit code 0[0m
[[34m2021-03-30 16:12:51,408[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 4070[0m
[[34m2021-03-30 16:12:51,417[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 16:12:51,429] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 16:12:51,586[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 16:00:00+00:00: scheduled__2021-03-30T16:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 16:13:31,902[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:18:31,931[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:20:01,324[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:20:01,325[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:20:01,325[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:20:01,325[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:20:01,326[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 16, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 16:20:01,326[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:20:01,330[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:20:02,420[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:20:02,472[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:20:02,472[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T16:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:20:02,979[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 16:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:20:03,132[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:20:03,133[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:20:03,134[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:20:03,134[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:20:03,135[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 16, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 16:20:03,135[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:20:03,138[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:20:03,720[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:20:03,767[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:20:03,767[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T16:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:20:45,156[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 16:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:20:45,320[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:20:45,321[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:20:45,321[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:20:45,322[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:20:45,323[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 16, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 16:20:45,323[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:20:45,326[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:20:46,590[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:20:46,713[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:20:46,713[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T16:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:22:43,509[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 16:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:22:43,525[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=4070) last sent a heartbeat 118.23 seconds ago! Restarting it[0m
[[34m2021-03-30 16:22:43,528[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 4070[0m
[[34m2021-03-30 16:22:43,620[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=4070, status='terminated', exitcode=0, started='16:12:50') (4070) terminated with exit code 0[0m
[[34m2021-03-30 16:22:43,622[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15299[0m
[[34m2021-03-30 16:22:43,631[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 16:22:43,641] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 16:22:43,792[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 16:10:00+00:00: scheduled__2021-03-30T16:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 16:23:32,046[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:28:32,190[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:30:01,987[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:30:01,988[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:30:01,988[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:30:01,988[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:30:01,989[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 16, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 16:30:01,989[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:30:01,992[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:30:02,967[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:30:03,018[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:30:03,018[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T16:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:30:03,484[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 16:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:30:03,641[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:30:03,642[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:30:03,643[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:30:03,643[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:30:03,644[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 16, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 16:30:03,644[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:30:03,647[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:30:04,344[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:30:04,391[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:30:04,391[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T16:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:30:55,267[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 16:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:30:55,285[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15299) last sent a heartbeat 51.67 seconds ago! Restarting it[0m
[[34m2021-03-30 16:30:55,290[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15299[0m
[[34m2021-03-30 16:30:55,424[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15299, status='terminated', exitcode=0, started='16:22:42') (15299) terminated with exit code 0[0m
[[34m2021-03-30 16:30:55,427[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23005[0m
[[34m2021-03-30 16:30:55,450[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 16:30:55,468] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 16:30:55,654[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:30:55,655[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:30:55,656[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:30:55,656[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:30:55,658[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 16, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 16:30:55,658[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:30:55,661[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:30:56,758[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:30:56,841[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:30:56,841[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T16:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:32:58,394[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 16:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:32:58,409[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23005) last sent a heartbeat 122.79 seconds ago! Restarting it[0m
[[34m2021-03-30 16:32:58,412[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23005[0m
[[34m2021-03-30 16:32:58,545[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23005, status='terminated', exitcode=0, started='16:30:54') (23005) terminated with exit code 0[0m
[[34m2021-03-30 16:32:58,547[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26842[0m
[[34m2021-03-30 16:32:58,554[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 16:32:58,564] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 16:32:58,715[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 16:20:00+00:00: scheduled__2021-03-30T16:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 16:33:32,322[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:38:32,460[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:40:01,548[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:40:01,550[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:40:01,550[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:40:01,550[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:40:01,552[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 16, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 16:40:01,552[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:40:01,555[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:40:03,195[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:40:03,277[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:40:03,277[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T16:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:40:03,972[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 16:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:40:04,159[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:40:04,160[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:40:04,160[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:40:04,160[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:40:04,162[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 16, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 16:40:04,162[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:40:04,165[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:40:05,190[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:40:05,261[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:40:05,261[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T16:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:40:47,804[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 16:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:40:47,992[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:40:47,993[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:40:47,993[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:40:47,993[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:40:47,995[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 16, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 16:40:47,995[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:40:48,001[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:40:49,028[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:40:49,107[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:40:49,107[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T16:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:42:56,261[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 16:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:42:56,277[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26842) last sent a heartbeat 128.32 seconds ago! Restarting it[0m
[[34m2021-03-30 16:42:56,280[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26842[0m
[[34m2021-03-30 16:42:56,372[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26842, status='terminated', exitcode=0, started='16:32:57') (26842) terminated with exit code 0[0m
[[34m2021-03-30 16:42:56,375[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5876[0m
[[34m2021-03-30 16:42:56,383[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 16:42:56,394] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 16:42:56,544[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 16:30:00+00:00: scheduled__2021-03-30T16:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 16:43:32,498[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:48:32,525[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:50:01,050[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:50:01,051[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:50:01,051[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:50:01,051[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:50:01,052[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 16, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 16:50:01,052[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:50:01,055[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:50:02,162[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:50:02,218[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:50:02,218[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T16:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:50:02,665[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 16:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:50:02,819[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:50:02,820[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:50:02,820[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:50:02,820[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:50:02,821[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 16, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 16:50:02,821[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:50:02,824[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:50:03,408[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:50:03,454[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:50:03,454[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T16:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:50:41,939[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 16:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:50:42,104[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:50:42,105[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 16:50:42,105[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 16:50:42,105[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 16:50:42,106[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 16, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 16:50:42,106[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:50:42,109[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 16:50:43,478[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 16:50:43,591[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 16:50:43,591[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T16:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 16:52:41,240[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 16:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 16:52:41,257[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5876) last sent a heartbeat 119.18 seconds ago! Restarting it[0m
[[34m2021-03-30 16:52:41,262[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5876[0m
[[34m2021-03-30 16:52:41,396[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5876, status='terminated', exitcode=0, started='16:42:55') (5876) terminated with exit code 0[0m
[[34m2021-03-30 16:52:41,399[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16967[0m
[[34m2021-03-30 16:52:41,413[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 16:52:41,433] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 16:52:41,618[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 16:40:00+00:00: scheduled__2021-03-30T16:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 16:53:32,661[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 16:58:32,791[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:00:02,078[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:00:02,080[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:00:02,080[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:00:02,080[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:00:02,081[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 16, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 17:00:02,082[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:00:02,085[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:00:03,185[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:00:03,237[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:00:03,237[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T16:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:00:03,686[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 16:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:00:03,831[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:00:03,832[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:00:03,832[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:00:03,832[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:00:03,833[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 16, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 17:00:03,833[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:00:03,836[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:00:04,547[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:00:04,592[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:00:04,592[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T16:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:00:43,723[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 16:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:00:43,887[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:00:43,888[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:00:43,888[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:00:43,889[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:00:43,890[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 16, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 17:00:43,890[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:00:43,893[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:00:45,024[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:00:45,101[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:00:45,101[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T16:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:02:49,307[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 16:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:02:49,323[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16967) last sent a heartbeat 125.46 seconds ago! Restarting it[0m
[[34m2021-03-30 17:02:49,326[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16967[0m
[[34m2021-03-30 17:02:49,538[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16967, status='terminated', exitcode=0, started='16:52:40') (16967) terminated with exit code 0[0m
[[34m2021-03-30 17:02:49,541[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28511[0m
[[34m2021-03-30 17:02:49,564[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 17:02:49,583] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 17:02:49,768[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 16:50:00+00:00: scheduled__2021-03-30T16:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 17:03:32,930[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:08:33,062[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:10:01,444[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:10:01,445[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:10:01,445[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:10:01,445[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:10:01,446[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 17, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 17:10:01,446[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:10:01,449[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:10:02,653[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:10:02,711[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:10:02,711[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T17:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:10:03,253[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 17:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:10:03,413[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:10:03,414[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:10:03,414[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:10:03,414[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:10:03,416[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 17, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 17:10:03,416[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:10:03,421[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:10:04,032[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:10:04,079[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:10:04,079[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T17:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:10:44,108[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 17:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:10:44,294[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:10:44,295[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:10:44,295[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:10:44,295[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:10:44,297[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 17, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 17:10:44,297[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:10:44,300[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:10:45,527[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:10:45,602[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:10:45,602[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T17:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:12:40,945[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 17:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:12:40,963[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28511) last sent a heartbeat 116.70 seconds ago! Restarting it[0m
[[34m2021-03-30 17:12:40,968[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28511[0m
[[34m2021-03-30 17:12:41,105[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28511, status='terminated', exitcode=0, started='17:02:48') (28511) terminated with exit code 0[0m
[[34m2021-03-30 17:12:41,108[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 7447[0m
[[34m2021-03-30 17:12:41,138[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 17:12:41,150] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 17:12:41,329[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 17:00:00+00:00: scheduled__2021-03-30T17:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 17:13:33,193[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:18:33,331[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:20:01,148[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:20:01,149[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:20:01,149[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:20:01,149[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:20:01,150[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 17, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 17:20:01,150[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:20:01,153[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:20:02,274[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:20:02,331[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:20:02,331[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T17:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:20:02,794[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 17:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:20:02,949[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:20:02,950[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:20:02,950[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:20:02,950[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:20:02,951[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 17, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 17:20:02,951[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:20:02,954[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:20:03,557[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:20:03,611[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:20:03,611[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T17:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:20:42,488[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 17:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:20:42,653[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:20:42,654[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:20:42,654[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:20:42,654[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:20:42,655[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 17, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 17:20:42,655[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:20:42,658[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:20:43,397[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:20:43,470[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:20:43,470[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T17:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:22:45,040[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 17:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:22:45,056[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=7447) last sent a heartbeat 122.43 seconds ago! Restarting it[0m
[[34m2021-03-30 17:22:45,059[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 7447[0m
[[34m2021-03-30 17:22:45,191[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=7447, status='terminated', exitcode=0, started='17:12:40') (7447) terminated with exit code 0[0m
[[34m2021-03-30 17:22:45,193[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18821[0m
[[34m2021-03-30 17:22:45,202[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 17:22:45,214] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 17:22:45,364[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 17:10:00+00:00: scheduled__2021-03-30T17:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 17:23:33,463[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:28:33,506[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:30:01,254[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:30:01,255[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:30:01,255[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:30:01,255[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:30:01,256[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 17, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 17:30:01,256[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:30:01,259[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:30:02,426[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:30:02,481[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:30:02,481[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T17:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:30:03,017[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 17:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:30:03,185[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:30:03,188[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:30:03,188[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:30:03,188[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:30:03,190[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 17, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 17:30:03,190[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:30:03,194[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:30:04,281[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:30:04,355[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:30:04,355[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T17:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:30:43,677[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 17:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:30:43,843[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:30:43,844[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:30:43,844[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:30:43,844[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:30:43,845[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 17, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 17:30:43,845[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:30:43,850[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:30:45,039[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:30:45,120[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:30:45,120[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T17:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:32:41,119[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 17:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:32:41,135[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18821) last sent a heartbeat 117.32 seconds ago! Restarting it[0m
[[34m2021-03-30 17:32:41,138[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18821[0m
[[34m2021-03-30 17:32:41,230[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18821, status='terminated', exitcode=0, started='17:22:44') (18821) terminated with exit code 0[0m
[[34m2021-03-30 17:32:41,233[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30177[0m
[[34m2021-03-30 17:32:41,259[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 17:32:41,271] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 17:32:41,441[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 17:20:00+00:00: scheduled__2021-03-30T17:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 17:33:33,638[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:38:33,676[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:40:02,126[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:40:02,127[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:40:02,127[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:40:02,127[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:40:02,128[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 17, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 17:40:02,128[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:40:02,131[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:40:03,117[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:40:03,168[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:40:03,168[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T17:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:40:03,660[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 17:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:40:03,817[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:40:03,818[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:40:03,818[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:40:03,818[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:40:03,819[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 17, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 17:40:03,820[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:40:03,823[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:40:04,405[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:40:04,450[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:40:04,451[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T17:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:40:42,205[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 17:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:40:42,369[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:40:42,370[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:40:42,371[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:40:42,371[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:40:42,372[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 17, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 17:40:42,372[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:40:42,375[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:40:43,248[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:40:43,356[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:40:43,356[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T17:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:42:43,589[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 17:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:42:43,605[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30177) last sent a heartbeat 121.26 seconds ago! Restarting it[0m
[[34m2021-03-30 17:42:43,608[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30177[0m
[[34m2021-03-30 17:42:43,740[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30177, status='terminated', exitcode=0, started='17:32:40') (30177) terminated with exit code 0[0m
[[34m2021-03-30 17:42:43,743[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 9270[0m
[[34m2021-03-30 17:42:43,752[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 17:42:43,764] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 17:42:43,914[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 17:30:00+00:00: scheduled__2021-03-30T17:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 17:43:33,807[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:48:33,937[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:50:02,096[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:50:02,097[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:50:02,099[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:50:02,099[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:50:02,101[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 17, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 17:50:02,101[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:50:02,105[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:50:03,654[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:50:03,718[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:50:03,718[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T17:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:50:04,164[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 17:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:50:04,327[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:50:04,328[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:50:04,328[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:50:04,328[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:50:04,329[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 17, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 17:50:04,330[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:50:04,333[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:50:04,914[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:50:04,961[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:50:04,961[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T17:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:50:45,995[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 17:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:50:46,160[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:50:46,161[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 17:50:46,161[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 17:50:46,161[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 17:50:46,162[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 17, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 17:50:46,162[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:50:46,165[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 17:50:47,164[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 17:50:47,245[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 17:50:47,245[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T17:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 17:52:45,920[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 17:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 17:52:45,935[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=9270) last sent a heartbeat 119.80 seconds ago! Restarting it[0m
[[34m2021-03-30 17:52:45,939[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 9270[0m
[[34m2021-03-30 17:52:46,111[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=9270, status='terminated', exitcode=0, started='17:42:42') (9270) terminated with exit code 0[0m
[[34m2021-03-30 17:52:46,113[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 20695[0m
[[34m2021-03-30 17:52:46,121[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 17:52:46,133] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 17:52:46,306[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 17:40:00+00:00: scheduled__2021-03-30T17:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 17:53:34,077[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 17:58:34,105[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:00:01,807[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:00:01,808[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:00:01,808[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:00:01,808[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:00:01,809[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 17, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 18:00:01,809[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:00:01,812[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:00:03,010[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:00:03,063[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:00:03,063[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T17:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:00:03,510[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 17:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:00:03,655[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:00:03,656[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:00:03,656[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:00:03,656[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:00:03,657[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 17, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 18:00:03,658[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:00:03,661[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:00:04,247[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:00:04,292[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:00:04,292[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T17:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:00:39,490[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 17:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:00:39,668[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:00:39,669[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:00:39,670[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:00:39,670[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:00:39,671[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 17, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 18:00:39,671[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:00:39,674[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:00:40,584[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:00:40,658[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:00:40,658[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T17:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:02:37,260[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 17:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:02:37,275[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=20695) last sent a heartbeat 117.64 seconds ago! Restarting it[0m
[[34m2021-03-30 18:02:37,279[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 20695[0m
[[34m2021-03-30 18:02:37,411[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=20695, status='terminated', exitcode=0, started='17:52:45') (20695) terminated with exit code 0[0m
[[34m2021-03-30 18:02:37,413[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31889[0m
[[34m2021-03-30 18:02:37,422[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 18:02:37,434] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 18:02:37,586[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 17:50:00+00:00: scheduled__2021-03-30T17:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 18:03:34,235[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:08:34,262[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:10:01,969[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:10:01,971[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:10:01,971[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:10:01,971[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:10:01,973[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 18, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 18:10:01,973[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:10:01,976[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:10:03,079[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:10:03,130[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:10:03,130[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T18:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:10:03,577[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:10:03,729[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:10:03,730[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:10:03,730[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:10:03,731[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:10:03,732[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 18, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 18:10:03,732[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:10:03,735[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:10:04,596[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:10:04,643[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:10:04,643[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T18:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:10:44,286[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:10:44,451[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:10:44,452[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:10:44,452[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:10:44,452[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:10:44,453[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 18, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 18:10:44,453[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:10:44,456[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:10:45,531[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:10:45,609[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:10:45,610[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T18:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:12:47,895[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:12:47,911[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31889) last sent a heartbeat 123.48 seconds ago! Restarting it[0m
[[34m2021-03-30 18:12:47,914[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31889[0m
[[34m2021-03-30 18:12:48,046[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31889, status='terminated', exitcode=0, started='18:02:36') (31889) terminated with exit code 0[0m
[[34m2021-03-30 18:12:48,048[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 11134[0m
[[34m2021-03-30 18:12:48,059[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 18:12:48,071] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 18:12:48,221[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 18:00:00+00:00: scheduled__2021-03-30T18:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 18:13:34,391[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:18:34,407[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:20:01,198[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:20:01,200[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:20:01,200[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:20:01,200[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:20:01,201[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 18, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 18:20:01,201[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:20:01,204[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:20:02,466[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:20:02,520[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:20:02,520[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T18:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:20:03,022[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 18:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:20:03,197[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:20:03,198[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:20:03,198[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:20:03,198[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:20:03,199[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 18, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 18:20:03,199[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:20:03,202[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:20:03,791[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:20:03,838[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:20:03,838[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T18:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:20:41,056[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 18:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:20:41,250[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:20:41,251[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:20:41,251[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:20:41,251[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:20:41,253[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 18, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 18:20:41,253[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:20:41,256[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:20:42,616[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:20:42,728[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:20:42,728[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T18:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:22:41,070[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 18:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:22:41,088[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=11134) last sent a heartbeat 119.88 seconds ago! Restarting it[0m
[[34m2021-03-30 18:22:41,093[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 11134[0m
[[34m2021-03-30 18:22:41,266[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=11134, status='terminated', exitcode=0, started='18:12:47') (11134) terminated with exit code 0[0m
[[34m2021-03-30 18:22:41,270[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 22374[0m
[[34m2021-03-30 18:22:41,293[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 18:22:41,308] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 18:22:41,486[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 18:10:00+00:00: scheduled__2021-03-30T18:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 18:23:34,539[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:28:34,566[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:30:02,108[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:30:02,109[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:30:02,109[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:30:02,109[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:30:02,110[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 18, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 18:30:02,110[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:30:02,114[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:30:03,255[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:30:03,314[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:30:03,314[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T18:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:30:03,769[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 18:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:30:03,914[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:30:03,915[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:30:03,915[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:30:03,916[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:30:03,917[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 18, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 18:30:03,917[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:30:03,920[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:30:04,505[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:30:04,551[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:30:04,552[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T18:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:30:42,610[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 18:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:30:42,774[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:30:42,775[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:30:42,775[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:30:42,775[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:30:42,776[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 18, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 18:30:42,776[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:30:42,779[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:30:43,749[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:30:43,826[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:30:43,826[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T18:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:32:51,696[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 18:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:32:51,712[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=22374) last sent a heartbeat 128.96 seconds ago! Restarting it[0m
[[34m2021-03-30 18:32:51,715[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 22374[0m
[[34m2021-03-30 18:32:51,807[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=22374, status='terminated', exitcode=0, started='18:22:40') (22374) terminated with exit code 0[0m
[[34m2021-03-30 18:32:51,809[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 1397[0m
[[34m2021-03-30 18:32:51,818[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 18:32:51,830] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 18:32:51,980[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 18:20:00+00:00: scheduled__2021-03-30T18:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 18:33:34,701[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:38:34,744[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:40:01,658[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:40:01,660[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:40:01,660[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:40:01,660[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:40:01,662[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 18, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 18:40:01,662[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:40:01,666[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:40:02,781[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:40:02,836[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:40:02,837[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T18:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:40:03,322[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 18:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:40:03,477[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:40:03,478[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:40:03,478[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:40:03,478[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:40:03,479[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 18, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 18:40:03,480[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:40:03,487[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:40:04,388[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:40:04,469[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:40:04,469[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T18:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:40:43,884[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 18:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:40:44,050[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:40:44,051[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:40:44,051[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:40:44,051[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:40:44,052[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 18, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 18:40:44,052[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:40:44,055[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:40:45,342[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:40:45,460[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:40:45,460[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T18:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:41:26,559[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 18:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:43:34,895[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:46:26,640[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:46:26,641[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:46:26,641[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:46:26,642[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:46:26,643[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 18, 30, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-30 18:46:26,643[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:46:26,646[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:46:27,227[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:46:27,274[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:46:27,274[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T18:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:48:19,727[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 18:30:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-30 18:48:19,745[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=1397) last sent a heartbeat 113.13 seconds ago! Restarting it[0m
[[34m2021-03-30 18:48:19,749[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 1397[0m
[[34m2021-03-30 18:48:19,883[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=1397, status='terminated', exitcode=0, started='18:32:51') (1397) terminated with exit code 0[0m
[[34m2021-03-30 18:48:19,885[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17580[0m
[[34m2021-03-30 18:48:19,892[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 18:48:19,903] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 18:48:20,054[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 18:30:00+00:00: scheduled__2021-03-30T18:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 18:48:35,025[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:50:02,125[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:50:02,126[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:50:02,126[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:50:02,126[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:50:02,127[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 18, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 18:50:02,127[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:50:02,131[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:50:02,769[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:50:02,820[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:50:02,820[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T18:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:50:03,220[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 18:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:50:03,426[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:50:03,427[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:50:03,427[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:50:03,428[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:50:03,429[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 18, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 18:50:03,429[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:50:03,434[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:50:04,105[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:50:04,151[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:50:04,152[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T18:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:50:41,099[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 18:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:50:41,286[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:50:41,287[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 18:50:41,287[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 18:50:41,287[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 18:50:41,288[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 18, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 18:50:41,288[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:50:41,291[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 18:50:42,428[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 18:50:42,505[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 18:50:42,506[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T18:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 18:52:26,703[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 18:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 18:52:26,721[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17580) last sent a heartbeat 105.47 seconds ago! Restarting it[0m
[[34m2021-03-30 18:52:26,726[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17580[0m
[[34m2021-03-30 18:52:26,859[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17580, status='terminated', exitcode=0, started='18:48:19') (17580) terminated with exit code 0[0m
[[34m2021-03-30 18:52:26,861[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24031[0m
[[34m2021-03-30 18:52:26,871[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 18:52:26,883] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 18:52:27,033[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 18:40:00+00:00: scheduled__2021-03-30T18:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 18:53:35,155[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 18:58:35,188[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:00:01,983[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:00:01,985[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:00:01,985[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:00:01,985[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:00:01,987[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 18, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 19:00:01,987[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:00:01,993[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:00:03,616[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:00:03,695[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:00:03,695[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T18:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:00:04,334[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 18:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:00:04,509[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:00:04,510[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:00:04,510[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:00:04,511[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:00:04,512[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 18, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 19:00:04,512[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:00:04,517[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:00:05,570[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:00:05,664[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:00:05,664[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T18:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:00:48,688[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 18:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:00:48,854[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:00:48,855[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:00:48,855[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:00:48,855[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:00:48,856[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 18, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 19:00:48,856[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:00:48,859[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:00:50,074[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:00:50,161[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:00:50,161[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T18:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:02:50,211[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 18:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:02:50,227[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24031) last sent a heartbeat 121.40 seconds ago! Restarting it[0m
[[34m2021-03-30 19:02:50,230[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24031[0m
[[34m2021-03-30 19:02:50,402[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24031, status='terminated', exitcode=0, started='18:52:26') (24031) terminated with exit code 0[0m
[[34m2021-03-30 19:02:50,405[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3416[0m
[[34m2021-03-30 19:02:50,416[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 19:02:50,429] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 19:02:50,605[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 18:50:00+00:00: scheduled__2021-03-30T18:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 19:03:35,223[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:08:35,357[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:10:01,243[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:10:01,244[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:10:01,244[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:10:01,244[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:10:01,245[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 19, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 19:10:01,245[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:10:01,248[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:10:02,457[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:10:02,513[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:10:02,513[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T19:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:10:02,993[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 19:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:10:03,139[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:10:03,140[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:10:03,140[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:10:03,140[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:10:03,141[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 19, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 19:10:03,141[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:10:03,144[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:10:03,734[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:10:03,790[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:10:03,790[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T19:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:10:39,902[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 19:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:10:40,067[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:10:40,068[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:10:40,068[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:10:40,069[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:10:40,070[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 19, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 19:10:40,070[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:10:40,073[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:10:40,816[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:10:40,893[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:10:40,893[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T19:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:12:38,821[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 19:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:12:38,837[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3416) last sent a heartbeat 118.79 seconds ago! Restarting it[0m
[[34m2021-03-30 19:12:38,840[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3416[0m
[[34m2021-03-30 19:12:38,972[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3416, status='terminated', exitcode=0, started='19:02:49') (3416) terminated with exit code 0[0m
[[34m2021-03-30 19:12:38,975[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14714[0m
[[34m2021-03-30 19:12:38,985[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 19:12:38,997] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 19:12:39,148[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 19:00:00+00:00: scheduled__2021-03-30T19:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 19:13:35,443[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:18:35,470[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:20:02,142[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:20:02,143[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:20:02,143[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:20:02,144[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:20:02,145[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 19, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 19:20:02,145[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:20:02,148[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:20:03,381[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:20:03,432[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:20:03,432[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T19:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:20:03,888[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 19:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:20:04,043[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:20:04,044[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:20:04,044[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:20:04,044[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:20:04,045[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 19, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 19:20:04,046[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:20:04,048[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:20:04,696[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:20:04,769[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:20:04,770[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T19:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:20:45,221[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 19:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:20:45,389[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:20:45,390[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:20:45,390[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:20:45,390[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:20:45,391[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 19, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 19:20:45,391[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:20:45,394[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:20:46,530[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:20:46,612[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:20:46,612[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T19:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:22:45,907[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 19:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:22:45,923[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14714) last sent a heartbeat 120.56 seconds ago! Restarting it[0m
[[34m2021-03-30 19:22:45,926[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14714[0m
[[34m2021-03-30 19:22:46,018[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14714, status='terminated', exitcode=0, started='19:12:38') (14714) terminated with exit code 0[0m
[[34m2021-03-30 19:22:46,020[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26117[0m
[[34m2021-03-30 19:22:46,028[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 19:22:46,038] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 19:22:46,211[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 19:10:00+00:00: scheduled__2021-03-30T19:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 19:23:35,609[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:28:35,639[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:30:02,038[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:30:02,039[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:30:02,039[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:30:02,039[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:30:02,040[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 19, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 19:30:02,040[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:30:02,044[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:30:03,199[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:30:03,287[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:30:03,287[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T19:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:30:03,721[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 19:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:30:03,866[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:30:03,867[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:30:03,867[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:30:03,867[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:30:03,868[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 19, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 19:30:03,868[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:30:03,871[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:30:04,552[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:30:04,651[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:30:04,651[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T19:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:30:40,865[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 19:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:30:41,439[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:30:41,440[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:30:41,441[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:30:41,441[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:30:41,442[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 19, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 19:30:41,443[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:30:41,446[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:30:42,745[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:30:42,857[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:30:42,857[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T19:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:32:38,939[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 19:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:32:38,955[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26117) last sent a heartbeat 117.93 seconds ago! Restarting it[0m
[[34m2021-03-30 19:32:38,958[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26117[0m
[[34m2021-03-30 19:32:39,050[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26117, status='terminated', exitcode=0, started='19:22:45') (26117) terminated with exit code 0[0m
[[34m2021-03-30 19:32:39,053[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5149[0m
[[34m2021-03-30 19:32:39,060[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 19:32:39,070] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 19:32:39,226[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 19:20:00+00:00: scheduled__2021-03-30T19:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 19:33:35,769[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:38:35,796[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:40:01,398[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:40:01,399[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:40:01,399[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:40:01,400[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:40:01,401[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 19, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 19:40:01,401[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:40:01,404[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:40:02,584[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:40:02,637[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:40:02,637[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T19:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:40:03,031[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 19:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:40:03,177[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:40:03,178[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:40:03,178[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:40:03,178[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:40:03,179[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 19, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 19:40:03,179[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:40:03,182[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:40:03,763[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:40:03,809[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:40:03,809[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T19:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:40:42,152[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 19:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:40:42,317[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:40:42,318[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:40:42,318[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:40:42,318[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:40:42,319[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 19, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 19:40:42,319[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:40:42,322[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:40:43,425[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:40:43,533[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:40:43,533[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T19:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:42:45,813[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 19:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:42:45,829[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5149) last sent a heartbeat 123.54 seconds ago! Restarting it[0m
[[34m2021-03-30 19:42:45,832[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5149[0m
[[34m2021-03-30 19:42:45,924[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5149, status='terminated', exitcode=0, started='19:32:38') (5149) terminated with exit code 0[0m
[[34m2021-03-30 19:42:45,927[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16577[0m
[[34m2021-03-30 19:42:45,934[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 19:42:45,944] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 19:42:46,117[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 19:30:00+00:00: scheduled__2021-03-30T19:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 19:43:35,926[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:48:36,059[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:50:01,754[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:50:01,755[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:50:01,755[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:50:01,755[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:50:01,756[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 19, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 19:50:01,756[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:50:01,759[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:50:02,952[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:50:03,032[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:50:03,032[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T19:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:50:03,569[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 19:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:50:03,735[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:50:03,737[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:50:03,737[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:50:03,737[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:50:03,738[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 19, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 19:50:03,739[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:50:03,742[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:50:04,716[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:50:04,803[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:50:04,804[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T19:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:50:43,084[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 19:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:50:43,255[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:50:43,256[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 19:50:43,256[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 19:50:43,256[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 19:50:43,257[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 19, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 19:50:43,257[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:50:43,260[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 19:50:44,093[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 19:50:44,167[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 19:50:44,167[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T19:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 19:52:37,187[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 19:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 19:52:37,205[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16577) last sent a heartbeat 113.97 seconds ago! Restarting it[0m
[[34m2021-03-30 19:52:37,210[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16577[0m
[[34m2021-03-30 19:52:37,345[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16577, status='terminated', exitcode=0, started='19:42:45') (16577) terminated with exit code 0[0m
[[34m2021-03-30 19:52:37,348[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 27857[0m
[[34m2021-03-30 19:52:37,370[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 19:52:37,389] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 19:52:37,563[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 19:40:00+00:00: scheduled__2021-03-30T19:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 19:53:36,089[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 19:58:36,219[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:00:01,441[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:00:01,442[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:00:01,442[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:00:01,442[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:00:01,443[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 19, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 20:00:01,443[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:00:01,446[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:00:02,575[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:00:02,633[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:00:02,633[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T19:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:00:03,012[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 19:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:00:03,167[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:00:03,168[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:00:03,168[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:00:03,168[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:00:03,169[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 19, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 20:00:03,169[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:00:03,172[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:00:03,771[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:00:03,831[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:00:03,831[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T19:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:00:40,899[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 19:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:00:41,065[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:00:41,066[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:00:41,066[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:00:41,067[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:00:41,068[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 19, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 20:00:41,068[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:00:41,071[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:00:42,011[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:00:42,099[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:00:42,099[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T19:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:02:38,123[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 19:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:02:38,139[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=27857) last sent a heartbeat 117.10 seconds ago! Restarting it[0m
[[34m2021-03-30 20:02:38,142[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 27857[0m
[[34m2021-03-30 20:02:38,234[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=27857, status='terminated', exitcode=0, started='19:52:36') (27857) terminated with exit code 0[0m
[[34m2021-03-30 20:02:38,237[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6947[0m
[[34m2021-03-30 20:02:38,245[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 20:02:38,255] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 20:02:38,429[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 19:50:00+00:00: scheduled__2021-03-30T19:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 20:03:36,246[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:08:36,333[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:10:01,436[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:10:01,437[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:10:01,437[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:10:01,437[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:10:01,439[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 20, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 20:10:01,440[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:10:01,443[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:10:03,048[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:10:03,128[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:10:03,128[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T20:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:10:03,656[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 20:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:10:03,828[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:10:03,829[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:10:03,829[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:10:03,829[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:10:03,831[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 20, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 20:10:03,831[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:10:03,835[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:10:04,846[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:10:04,939[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:10:04,939[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T20:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:10:44,188[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 20:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:10:44,354[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:10:44,355[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:10:44,355[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:10:44,355[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:10:44,356[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 20, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 20:10:44,356[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:10:44,359[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:10:45,352[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:10:45,431[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:10:45,432[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T20:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:12:38,295[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 20:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:12:38,313[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6947) last sent a heartbeat 113.98 seconds ago! Restarting it[0m
[[34m2021-03-30 20:12:38,318[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6947[0m
[[34m2021-03-30 20:12:38,422[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6947, status='terminated', exitcode=0, started='20:02:37') (6947) terminated with exit code 0[0m
[[34m2021-03-30 20:12:38,424[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18298[0m
[[34m2021-03-30 20:12:38,432[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 20:12:38,442] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 20:12:38,594[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 20:00:00+00:00: scheduled__2021-03-30T20:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 20:13:36,369[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:18:36,513[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:20:01,567[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:20:01,568[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:20:01,569[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:20:01,569[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:20:01,570[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 20, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 20:20:01,570[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:20:01,573[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:20:02,728[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:20:02,785[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:20:02,785[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T20:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:20:03,151[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 20:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:20:03,304[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:20:03,305[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:20:03,306[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:20:03,306[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:20:03,307[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 20, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 20:20:03,307[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:20:03,310[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:20:03,904[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:20:03,954[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:20:03,954[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T20:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:20:39,161[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 20:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:20:39,326[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:20:39,327[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:20:39,327[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:20:39,327[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:20:39,328[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 20, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 20:20:39,328[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:20:39,331[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:20:40,328[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:20:40,407[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:20:40,407[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T20:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:22:35,232[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 20:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:22:35,248[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18298) last sent a heartbeat 115.95 seconds ago! Restarting it[0m
[[34m2021-03-30 20:22:35,251[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18298[0m
[[34m2021-03-30 20:22:35,343[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18298, status='terminated', exitcode=0, started='20:12:37') (18298) terminated with exit code 0[0m
[[34m2021-03-30 20:22:35,346[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 29675[0m
[[34m2021-03-30 20:22:35,353[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 20:22:35,363] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 20:22:35,535[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 20:10:00+00:00: scheduled__2021-03-30T20:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 20:23:36,650[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:28:36,781[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:30:01,783[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:30:01,785[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:30:01,785[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:30:01,786[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:30:01,788[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 20, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 20:30:01,788[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:30:01,792[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:30:02,922[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:30:02,980[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:30:02,980[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T20:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:30:03,347[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 20:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:30:03,493[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:30:03,494[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:30:03,494[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:30:03,494[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:30:03,495[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 20, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 20:30:03,496[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:30:03,499[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:30:04,091[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:30:04,147[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:30:04,147[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T20:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:30:42,618[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 20:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:30:42,785[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:30:42,786[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:30:42,786[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:30:42,786[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:30:42,787[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 20, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 20:30:42,787[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:30:42,790[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:30:43,876[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:30:43,956[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:30:43,956[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T20:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:32:43,805[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 20:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:32:43,821[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=29675) last sent a heartbeat 121.06 seconds ago! Restarting it[0m
[[34m2021-03-30 20:32:43,824[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 29675[0m
[[34m2021-03-30 20:32:43,916[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=29675, status='terminated', exitcode=0, started='20:22:34') (29675) terminated with exit code 0[0m
[[34m2021-03-30 20:32:43,919[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8889[0m
[[34m2021-03-30 20:32:43,926[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 20:32:43,937] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 20:32:44,109[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 20:20:00+00:00: scheduled__2021-03-30T20:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 20:33:36,911[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:38:37,049[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:40:01,119[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:40:01,120[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:40:01,120[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:40:01,120[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:40:01,121[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 20, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 20:40:01,121[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:40:01,124[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:40:02,259[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:40:02,314[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:40:02,314[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T20:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:40:02,694[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 20:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:40:02,854[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:40:02,855[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:40:02,856[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:40:02,856[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:40:02,857[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 20, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 20:40:02,858[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:40:02,860[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:40:03,535[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:40:03,585[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:40:03,585[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T20:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:40:37,721[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 20:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:40:37,903[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:40:37,904[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:40:37,904[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:40:37,905[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:40:37,906[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 20, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 20:40:37,906[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:40:37,910[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:40:38,712[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:40:38,810[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:40:38,810[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T20:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:42:35,268[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 20:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:42:35,284[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8889) last sent a heartbeat 117.42 seconds ago! Restarting it[0m
[[34m2021-03-30 20:42:35,287[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8889[0m
[[34m2021-03-30 20:42:35,379[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8889, status='terminated', exitcode=0, started='20:32:43') (8889) terminated with exit code 0[0m
[[34m2021-03-30 20:42:35,382[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 20073[0m
[[34m2021-03-30 20:42:35,389[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 20:42:35,400] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 20:42:35,552[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 20:30:00+00:00: scheduled__2021-03-30T20:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 20:43:37,078[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:48:37,209[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:50:01,941[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:50:01,942[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:50:01,942[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:50:01,943[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:50:01,944[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 20, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 20:50:01,944[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:50:01,952[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:50:03,128[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:50:03,185[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:50:03,185[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T20:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:50:03,549[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 20:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:50:03,694[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:50:03,695[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:50:03,695[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:50:03,696[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:50:03,697[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 20, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 20:50:03,697[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:50:03,700[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:50:04,296[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:50:04,346[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:50:04,346[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T20:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:50:43,485[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 20:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:50:43,650[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:50:43,651[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 20:50:43,651[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 20:50:43,651[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 20:50:43,652[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 20, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 20:50:43,653[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:50:43,655[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 20:50:44,607[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 20:50:44,687[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 20:50:44,687[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T20:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 20:52:41,173[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 20:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 20:52:41,189[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=20073) last sent a heartbeat 117.56 seconds ago! Restarting it[0m
[[34m2021-03-30 20:52:41,192[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 20073[0m
[[34m2021-03-30 20:52:41,284[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=20073, status='terminated', exitcode=0, started='20:42:34') (20073) terminated with exit code 0[0m
[[34m2021-03-30 20:52:41,287[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31512[0m
[[34m2021-03-30 20:52:41,294[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 20:52:41,305] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 20:52:41,479[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 20:40:00+00:00: scheduled__2021-03-30T20:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 20:53:37,232[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 20:58:37,363[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:00:01,258[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:00:01,259[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:00:01,260[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:00:01,260[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:00:01,261[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 20, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 21:00:01,261[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:00:01,264[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:00:02,436[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:00:02,493[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:00:02,493[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T20:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:00:02,874[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 20:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:00:03,030[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:00:03,031[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:00:03,031[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:00:03,031[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:00:03,032[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 20, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 21:00:03,032[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:00:03,035[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:00:03,624[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:00:03,672[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:00:03,672[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T20:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:00:40,517[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 20:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:00:40,714[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:00:40,716[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:00:40,716[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:00:40,716[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:00:40,717[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 20, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 21:00:40,718[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:00:40,721[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:00:41,826[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:00:41,930[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:00:41,930[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T20:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:02:42,543[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 20:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:02:42,559[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31512) last sent a heartbeat 121.88 seconds ago! Restarting it[0m
[[34m2021-03-30 21:02:42,562[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31512[0m
[[34m2021-03-30 21:02:42,694[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31512, status='terminated', exitcode=0, started='20:52:40') (31512) terminated with exit code 0[0m
[[34m2021-03-30 21:02:42,699[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10684[0m
[[34m2021-03-30 21:02:42,726[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 21:02:42,743] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 21:02:42,916[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 20:50:00+00:00: scheduled__2021-03-30T20:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 21:03:37,494[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:08:37,522[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:10:01,984[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:10:01,985[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:10:01,985[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:10:01,986[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:10:01,987[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 21, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 21:10:01,987[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:10:01,990[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:10:02,939[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:10:02,994[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:10:02,994[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T21:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:10:03,467[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 21:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:10:03,627[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:10:03,628[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:10:03,628[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:10:03,628[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:10:03,629[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 21, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 21:10:03,630[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:10:03,633[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:10:04,232[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:10:04,281[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:10:04,281[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T21:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:10:42,570[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 21:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:10:42,736[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:10:42,737[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:10:42,737[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:10:42,737[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:10:42,738[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 21, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 21:10:42,738[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:10:42,741[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:10:43,612[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:10:43,693[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:10:43,693[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T21:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:12:38,196[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 21:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:12:38,212[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10684) last sent a heartbeat 115.50 seconds ago! Restarting it[0m
[[34m2021-03-30 21:12:38,215[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10684[0m
[[34m2021-03-30 21:12:38,307[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10684, status='terminated', exitcode=0, started='21:02:41') (10684) terminated with exit code 0[0m
[[34m2021-03-30 21:12:38,309[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21995[0m
[[34m2021-03-30 21:12:38,317[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 21:12:38,328] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 21:12:38,502[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 21:00:00+00:00: scheduled__2021-03-30T21:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 21:13:37,549[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:18:37,680[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:20:02,088[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:20:02,090[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:20:02,090[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:20:02,090[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:20:02,092[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 21, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 21:20:02,092[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:20:02,095[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:20:03,792[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:20:03,876[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:20:03,876[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T21:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:20:04,478[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 21:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:20:04,654[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:20:04,655[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:20:04,656[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:20:04,656[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:20:04,657[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 21, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 21:20:04,657[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:20:04,661[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:20:05,641[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:20:05,721[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:20:05,721[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T21:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:20:44,380[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 21:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:20:44,856[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:20:44,857[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:20:44,857[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:20:44,857[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:20:44,858[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 21, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 21:20:44,858[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:20:44,861[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:20:45,736[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:20:45,818[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:20:45,819[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T21:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:22:36,618[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 21:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:22:36,636[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21995) last sent a heartbeat 112.11 seconds ago! Restarting it[0m
[[34m2021-03-30 21:22:36,641[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21995[0m
[[34m2021-03-30 21:22:36,786[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21995, status='terminated', exitcode=0, started='21:12:37') (21995) terminated with exit code 0[0m
[[34m2021-03-30 21:22:36,789[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 848[0m
[[34m2021-03-30 21:22:36,806[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 21:22:36,832] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 21:22:37,012[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 21:10:00+00:00: scheduled__2021-03-30T21:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 21:23:37,826[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:28:37,958[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:30:01,308[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:30:01,310[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:30:01,310[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:30:01,310[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:30:01,311[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 21, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 21:30:01,312[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:30:01,315[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:30:02,404[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:30:02,460[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:30:02,461[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T21:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:30:02,851[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 21:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:30:03,004[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:30:03,005[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:30:03,005[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:30:03,005[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:30:03,006[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 21, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 21:30:03,006[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:30:03,009[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:30:03,595[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:30:03,641[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:30:03,641[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T21:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:30:40,217[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 21:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:30:40,381[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:30:40,382[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:30:40,382[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:30:40,383[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:30:40,384[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 21, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 21:30:40,384[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:30:40,387[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:30:40,978[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:30:41,025[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:30:41,025[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T21:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:32:35,605[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 21:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:32:35,621[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=848) last sent a heartbeat 115.26 seconds ago! Restarting it[0m
[[34m2021-03-30 21:32:35,624[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 848[0m
[[34m2021-03-30 21:32:35,716[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=848, status='terminated', exitcode=0, started='21:22:35') (848) terminated with exit code 0[0m
[[34m2021-03-30 21:32:35,719[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12499[0m
[[34m2021-03-30 21:32:35,727[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 21:32:35,737] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 21:32:35,911[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 21:20:00+00:00: scheduled__2021-03-30T21:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 21:33:38,088[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:38:38,219[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:40:01,740[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:40:01,742[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:40:01,742[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:40:01,742[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:40:01,744[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 21, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 21:40:01,744[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:40:01,751[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:40:02,979[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:40:03,033[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:40:03,033[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T21:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:40:03,434[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 21:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:40:03,580[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:40:03,581[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:40:03,581[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:40:03,581[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:40:03,582[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 21, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 21:40:03,582[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:40:03,585[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:40:04,179[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:40:04,232[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:40:04,232[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T21:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:40:42,395[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 21:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:40:42,560[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:40:42,561[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:40:42,561[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:40:42,561[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:40:42,562[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 21, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 21:40:42,562[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:40:42,565[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:40:43,571[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:40:43,652[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:40:43,652[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T21:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:42:35,107[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 21:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:42:35,125[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12499) last sent a heartbeat 112.59 seconds ago! Restarting it[0m
[[34m2021-03-30 21:42:35,130[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12499[0m
[[34m2021-03-30 21:42:35,269[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12499, status='terminated', exitcode=0, started='21:32:34') (12499) terminated with exit code 0[0m
[[34m2021-03-30 21:42:35,272[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23863[0m
[[34m2021-03-30 21:42:35,296[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 21:42:35,314] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 21:42:35,487[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 21:30:00+00:00: scheduled__2021-03-30T21:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 21:43:38,246[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:48:38,389[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:50:01,520[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:50:01,521[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:50:01,521[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:50:01,521[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:50:01,522[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 21, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 21:50:01,522[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:50:01,525[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:50:02,533[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:50:02,587[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:50:02,587[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T21:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:50:02,947[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 21:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:50:03,093[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:50:03,093[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:50:03,094[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:50:03,094[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:50:03,095[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 21, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 21:50:03,095[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:50:03,098[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:50:03,954[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:50:04,001[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:50:04,001[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T21:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:50:40,099[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 21:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:50:40,264[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:50:40,265[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 21:50:40,265[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 21:50:40,265[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 21:50:40,266[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 21, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 21:50:40,266[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:50:40,269[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 21:50:41,696[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 21:50:41,818[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 21:50:41,819[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T21:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 21:52:33,796[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 21:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 21:52:33,812[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23863) last sent a heartbeat 113.57 seconds ago! Restarting it[0m
[[34m2021-03-30 21:52:33,815[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23863[0m
[[34m2021-03-30 21:52:33,907[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23863, status='terminated', exitcode=0, started='21:42:34') (23863) terminated with exit code 0[0m
[[34m2021-03-30 21:52:33,909[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2864[0m
[[34m2021-03-30 21:52:33,917[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 21:52:33,927] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 21:52:34,098[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 21:40:00+00:00: scheduled__2021-03-30T21:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 21:53:38,523[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 21:58:38,660[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:00:01,505[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:00:01,506[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:00:01,506[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:00:01,506[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:00:01,510[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 21, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 22:00:01,510[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:00:01,514[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:00:02,668[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:00:02,724[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:00:02,724[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T21:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:00:03,098[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 21:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:00:03,244[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:00:03,245[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:00:03,245[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:00:03,246[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:00:03,247[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 21, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 22:00:03,247[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:00:03,250[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:00:03,843[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:00:03,891[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:00:03,891[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T21:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:00:42,854[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 21:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:00:43,020[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:00:43,021[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:00:43,021[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:00:43,021[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:00:43,022[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 21, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 22:00:43,023[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:00:43,026[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:00:43,975[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:00:44,072[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:00:44,072[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T21:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:02:37,112[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 21:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:02:37,128[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2864) last sent a heartbeat 114.13 seconds ago! Restarting it[0m
[[34m2021-03-30 22:02:37,131[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2864[0m
[[34m2021-03-30 22:02:37,223[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2864, status='terminated', exitcode=0, started='21:52:33') (2864) terminated with exit code 0[0m
[[34m2021-03-30 22:02:37,226[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14369[0m
[[34m2021-03-30 22:02:37,233[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 22:02:37,243] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 22:02:37,447[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 21:50:00+00:00: scheduled__2021-03-30T21:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 22:03:38,707[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:08:38,741[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:10:01,078[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:10:01,079[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:10:01,079[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:10:01,079[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:10:01,080[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 22, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 22:10:01,080[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:10:01,084[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:10:02,315[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:10:02,370[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:10:02,370[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T22:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:10:02,718[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 22:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:10:02,871[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:10:02,872[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:10:02,873[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:10:02,873[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:10:02,874[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 22, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 22:10:02,874[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:10:02,877[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:10:03,631[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:10:03,705[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:10:03,705[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T22:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:10:42,050[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 22:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:10:42,246[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:10:42,248[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:10:42,248[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:10:42,248[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:10:42,250[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 22, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 22:10:42,250[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:10:42,253[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:10:43,629[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:10:43,752[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:10:43,753[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T22:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:12:35,894[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 22:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:12:35,910[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14369) last sent a heartbeat 113.70 seconds ago! Restarting it[0m
[[34m2021-03-30 22:12:35,913[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14369[0m
[[34m2021-03-30 22:12:36,005[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14369, status='terminated', exitcode=0, started='22:02:36') (14369) terminated with exit code 0[0m
[[34m2021-03-30 22:12:36,007[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25704[0m
[[34m2021-03-30 22:12:36,015[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 22:12:36,026] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 22:12:36,177[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 22:00:00+00:00: scheduled__2021-03-30T22:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 22:13:38,769[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:18:38,822[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:20:01,095[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:20:01,096[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:20:01,096[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:20:01,096[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:20:01,097[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 22, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 22:20:01,098[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:20:01,101[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:20:02,193[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:20:02,246[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:20:02,246[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T22:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:20:02,623[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 22:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:20:02,789[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:20:02,791[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:20:02,791[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:20:02,791[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:20:02,793[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 22, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 22:20:02,793[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:20:02,796[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:20:03,769[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:20:03,840[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:20:03,840[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T22:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:20:41,895[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 22:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:20:42,072[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:20:42,073[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:20:42,073[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:20:42,073[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:20:42,074[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 22, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 22:20:42,074[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:20:42,077[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:20:42,964[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:20:43,038[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:20:43,038[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T22:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:22:39,177[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 22:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:22:39,195[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25704) last sent a heartbeat 117.15 seconds ago! Restarting it[0m
[[34m2021-03-30 22:22:39,199[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25704[0m
[[34m2021-03-30 22:22:39,332[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25704, status='terminated', exitcode=0, started='22:12:35') (25704) terminated with exit code 0[0m
[[34m2021-03-30 22:22:39,335[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 4846[0m
[[34m2021-03-30 22:22:39,343[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 22:22:39,354] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 22:22:39,562[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 22:10:00+00:00: scheduled__2021-03-30T22:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 22:23:38,852[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:28:38,908[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:30:01,756[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:30:01,760[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:30:01,760[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:30:01,760[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:30:01,762[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 22, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 22:30:01,762[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:30:01,769[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:30:03,319[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:30:03,402[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:30:03,402[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T22:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:30:04,011[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 22:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:30:04,182[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:30:04,183[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:30:04,183[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:30:04,183[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:30:04,185[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 22, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 22:30:04,185[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:30:04,190[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:30:05,243[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:30:05,313[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:30:05,313[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T22:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:30:43,812[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 22:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:30:44,000[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:30:44,001[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:30:44,001[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:30:44,001[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:30:44,002[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 22, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 22:30:44,003[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:30:44,006[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:30:45,111[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:30:45,192[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:30:45,192[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T22:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:32:35,663[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 22:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:32:35,681[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=4846) last sent a heartbeat 111.73 seconds ago! Restarting it[0m
[[34m2021-03-30 22:32:35,686[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 4846[0m
[[34m2021-03-30 22:32:35,820[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=4846, status='terminated', exitcode=0, started='22:22:38') (4846) terminated with exit code 0[0m
[[34m2021-03-30 22:32:35,823[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16177[0m
[[34m2021-03-30 22:32:35,854[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 22:32:35,874] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 22:32:36,060[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 22:20:00+00:00: scheduled__2021-03-30T22:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 22:33:38,940[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:38:39,072[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:40:02,130[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:40:02,131[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:40:02,131[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:40:02,131[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:40:02,132[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 22, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 22:40:02,132[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:40:02,135[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:40:03,300[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:40:03,355[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:40:03,355[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T22:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:40:03,697[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 22:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:40:03,842[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:40:03,843[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:40:03,843[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:40:03,843[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:40:03,845[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 22, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 22:40:03,845[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:40:03,848[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:40:04,430[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:40:04,477[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:40:04,477[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T22:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:40:39,185[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 22:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:40:39,349[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:40:39,350[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:40:39,351[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:40:39,351[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:40:39,352[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 22, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 22:40:39,352[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:40:39,355[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:40:40,205[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:40:40,280[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:40:40,281[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T22:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:42:32,218[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 22:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:42:32,234[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16177) last sent a heartbeat 112.91 seconds ago! Restarting it[0m
[[34m2021-03-30 22:42:32,237[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16177[0m
[[34m2021-03-30 22:42:32,329[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16177, status='terminated', exitcode=0, started='22:32:35') (16177) terminated with exit code 0[0m
[[34m2021-03-30 22:42:32,331[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 27459[0m
[[34m2021-03-30 22:42:32,339[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 22:42:32,349] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 22:42:32,520[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 22:30:00+00:00: scheduled__2021-03-30T22:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 22:43:39,085[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:48:39,126[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:50:01,077[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:50:01,078[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:50:01,078[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:50:01,078[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:50:01,079[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 22, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 22:50:01,079[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:50:01,082[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:50:02,503[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:50:02,554[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:50:02,554[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T22:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:50:02,899[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 22:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:50:03,057[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:50:03,058[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:50:03,059[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:50:03,059[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:50:03,060[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 22, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 22:50:03,060[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:50:03,063[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:50:03,773[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:50:03,822[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:50:03,822[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T22:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:50:42,636[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 22:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:50:42,801[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:50:42,802[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 22:50:42,802[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 22:50:42,802[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 22:50:42,803[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 22, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 22:50:42,803[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:50:42,806[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 22:50:44,010[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 22:50:44,089[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 22:50:44,089[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T22:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 22:52:48,862[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 22:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 22:52:48,877[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=27459) last sent a heartbeat 126.10 seconds ago! Restarting it[0m
[[34m2021-03-30 22:52:48,881[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 27459[0m
[[34m2021-03-30 22:52:48,973[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=27459, status='terminated', exitcode=0, started='22:42:31') (27459) terminated with exit code 0[0m
[[34m2021-03-30 22:52:48,975[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6826[0m
[[34m2021-03-30 22:52:48,983[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 22:52:48,993] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 22:52:49,165[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 22:40:00+00:00: scheduled__2021-03-30T22:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 22:53:39,258[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 22:58:39,399[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:00:01,099[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:00:01,100[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:00:01,100[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:00:01,101[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:00:01,102[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 22, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 23:00:01,102[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:00:01,105[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:00:02,259[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:00:02,317[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:00:02,317[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T22:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:00:02,727[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 22:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:00:02,874[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:00:02,875[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:00:02,875[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:00:02,875[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:00:02,876[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 22, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 23:00:02,876[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:00:02,879[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:00:03,460[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:00:03,511[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:00:03,511[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T22:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:00:38,119[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 22:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:00:38,283[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:00:38,284[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:00:38,284[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:00:38,284[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:00:38,285[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 22, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 23:00:38,285[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:00:38,289[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:00:39,484[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:00:39,594[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:00:39,594[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T22:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:02:34,945[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 22:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:02:34,960[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6826) last sent a heartbeat 116.70 seconds ago! Restarting it[0m
[[34m2021-03-30 23:02:34,964[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6826[0m
[[34m2021-03-30 23:02:35,056[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6826, status='terminated', exitcode=0, started='22:52:48') (6826) terminated with exit code 0[0m
[[34m2021-03-30 23:02:35,058[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17939[0m
[[34m2021-03-30 23:02:35,066[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 23:02:35,076] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 23:02:35,229[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 22:50:00+00:00: scheduled__2021-03-30T22:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 23:03:39,532[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:08:39,663[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:10:01,402[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:10:01,403[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:10:01,403[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:10:01,404[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:10:01,405[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 23, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 23:10:01,405[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:10:01,408[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:10:02,481[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:10:02,531[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:10:02,531[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T23:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:10:02,879[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 23:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:10:03,023[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:10:03,024[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:10:03,024[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:10:03,024[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:10:03,025[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 23, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 23:10:03,025[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:10:03,028[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:10:03,797[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:10:03,867[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:10:03,868[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T23:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:10:42,009[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 23:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:10:42,174[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:10:42,175[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:10:42,175[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:10:42,175[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:10:42,176[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 23, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 23:10:42,176[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:10:42,179[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:10:43,084[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:10:43,159[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:10:43,159[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T23:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:12:36,833[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 23:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:12:36,848[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17939) last sent a heartbeat 114.70 seconds ago! Restarting it[0m
[[34m2021-03-30 23:12:36,852[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17939[0m
[[34m2021-03-30 23:12:36,944[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17939, status='terminated', exitcode=0, started='23:02:34') (17939) terminated with exit code 0[0m
[[34m2021-03-30 23:12:36,946[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 29290[0m
[[34m2021-03-30 23:12:36,954[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 23:12:36,964] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 23:12:37,137[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 23:00:00+00:00: scheduled__2021-03-30T23:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 23:13:39,697[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:18:39,828[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:20:01,462[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:20:01,463[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:20:01,464[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:20:01,464[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:20:01,465[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 23, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 23:20:01,465[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:20:01,469[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:20:02,746[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:20:02,798[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:20:02,798[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T23:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:20:03,153[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 23:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:20:03,307[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:20:03,308[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:20:03,308[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:20:03,308[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:20:03,309[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 23, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 23:20:03,309[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:20:03,312[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:20:04,325[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:20:04,399[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:20:04,399[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T23:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:20:41,909[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 23:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:20:42,106[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:20:42,108[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:20:42,108[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:20:42,110[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:20:42,112[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 23, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 23:20:42,112[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:20:42,115[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:20:43,387[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:20:43,464[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:20:43,464[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T23:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:22:37,855[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 23:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:22:37,873[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=29290) last sent a heartbeat 115.81 seconds ago! Restarting it[0m
[[34m2021-03-30 23:22:37,878[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 29290[0m
[[34m2021-03-30 23:22:38,020[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=29290, status='terminated', exitcode=0, started='23:12:36') (29290) terminated with exit code 0[0m
[[34m2021-03-30 23:22:38,023[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8531[0m
[[34m2021-03-30 23:22:38,053[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 23:22:38,066] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 23:22:38,243[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 23:10:00+00:00: scheduled__2021-03-30T23:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 23:23:39,872[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:28:39,906[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:30:01,484[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:30:01,485[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:30:01,485[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:30:01,485[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:30:01,486[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 23, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 23:30:01,486[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:30:01,489[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:30:02,659[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:30:02,713[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:30:02,713[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T23:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:30:03,072[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 23:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:30:03,239[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:30:03,240[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:30:03,240[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:30:03,240[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:30:03,242[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 23, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 23:30:03,242[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:30:03,245[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:30:03,922[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:30:03,968[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:30:03,968[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T23:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:30:40,987[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 23:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:30:41,158[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:30:41,159[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:30:41,159[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:30:41,159[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:30:41,160[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 23, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 23:30:41,160[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:30:41,164[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:30:42,104[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:30:42,180[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:30:42,180[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T23:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:32:35,658[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 23:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:32:35,674[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8531) last sent a heartbeat 114.54 seconds ago! Restarting it[0m
[[34m2021-03-30 23:32:35,679[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8531[0m
[[34m2021-03-30 23:32:35,781[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8531, status='terminated', exitcode=0, started='23:22:37') (8531) terminated with exit code 0[0m
[[34m2021-03-30 23:32:35,783[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19799[0m
[[34m2021-03-30 23:32:35,791[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 23:32:35,801] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 23:32:35,973[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 23:20:00+00:00: scheduled__2021-03-30T23:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 23:33:39,935[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:38:40,069[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:40:02,111[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:40:02,113[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:40:02,113[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:40:02,113[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:40:02,114[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 23, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 23:40:02,115[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:40:02,119[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:40:03,593[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:40:03,673[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:40:03,674[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T23:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:40:04,182[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 23:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:40:04,342[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:40:04,344[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:40:04,344[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:40:04,344[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:40:04,346[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 23, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 23:40:04,346[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:40:04,349[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:40:05,319[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:40:05,404[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:40:05,404[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T23:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:40:44,433[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 23:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:40:44,620[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:40:44,621[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:40:44,621[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:40:44,621[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:40:44,623[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 23, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 23:40:44,623[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:40:44,626[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:40:45,687[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:40:45,766[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:40:45,766[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T23:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:42:36,391[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 23:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:42:36,409[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19799) last sent a heartbeat 111.82 seconds ago! Restarting it[0m
[[34m2021-03-30 23:42:36,414[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19799[0m
[[34m2021-03-30 23:42:36,549[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19799, status='terminated', exitcode=0, started='23:32:34') (19799) terminated with exit code 0[0m
[[34m2021-03-30 23:42:36,552[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31247[0m
[[34m2021-03-30 23:42:36,565[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 23:42:36,580] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 23:42:36,765[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 23:30:00+00:00: scheduled__2021-03-30T23:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 23:43:40,200[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:48:40,239[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:50:01,390[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:50:01,391[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:50:01,391[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:50:01,391[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:50:01,392[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 23, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-30 23:50:01,392[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:50:01,396[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:50:02,461[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:50:02,532[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:50:02,533[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T23:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:50:03,000[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 23:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:50:03,155[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:50:03,156[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:50:03,156[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:50:03,156[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:50:03,157[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 23, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-30 23:50:03,157[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:50:03,160[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:50:03,742[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:50:03,793[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:50:03,793[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T23:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:50:38,416[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 23:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:50:38,581[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:50:38,582[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-30 23:50:38,582[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-30 23:50:38,582[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-30 23:50:38,583[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 23, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-30 23:50:38,583[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:50:38,586[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-30 23:50:39,467[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-30 23:50:39,547[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-30 23:50:39,547[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T23:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-30 23:52:32,208[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 23:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-30 23:52:32,223[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31247) last sent a heartbeat 113.67 seconds ago! Restarting it[0m
[[34m2021-03-30 23:52:32,227[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31247[0m
[[34m2021-03-30 23:52:32,319[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31247, status='terminated', exitcode=0, started='23:42:35') (31247) terminated with exit code 0[0m
[[34m2021-03-30 23:52:32,321[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10310[0m
[[34m2021-03-30 23:52:32,329[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-30 23:52:32,340] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-30 23:52:32,512[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 23:40:00+00:00: scheduled__2021-03-30T23:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-30 23:53:40,266[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-30 23:58:40,292[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:00:02,155[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:00:02,156[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:00:02,156[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:00:02,157[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-30 23:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:00:02,158[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 30, 23, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 00:00:02,158[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:00:02,162[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-30T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:00:03,227[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:00:03,278[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:00:03,278[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-30T23:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:00:03,637[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-30 23:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:00:03,796[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-30 23:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:00:03,797[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:00:03,797[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:00:03,797[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-30 23:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:00:03,798[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 30, 23, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 00:00:03,799[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:00:03,802[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-30T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:00:04,616[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:00:04,691[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:00:04,691[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-30T23:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:00:42,999[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-30 23:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:00:43,164[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-30 23:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:00:43,165[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:00:43,165[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:00:43,165[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-30 23:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:00:43,166[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 30, 23, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 00:00:43,166[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:00:43,170[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-30T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:00:43,996[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:00:44,074[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:00:44,074[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-30T23:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:02:35,511[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-30 23:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:02:35,529[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10310) last sent a heartbeat 112.39 seconds ago! Restarting it[0m
[[34m2021-03-31 00:02:35,533[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10310[0m
[[34m2021-03-31 00:02:35,672[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10310, status='terminated', exitcode=0, started='23:52:31') (10310) terminated with exit code 0[0m
[[34m2021-03-31 00:02:35,675[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21723[0m
[[34m2021-03-31 00:02:35,688[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 00:02:35,702] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 00:02:35,902[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-30 23:50:00+00:00: scheduled__2021-03-30T23:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 00:03:40,423[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:08:40,450[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:10:01,632[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:10:01,633[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:10:01,633[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:10:01,633[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:10:01,634[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 00:10:01,634[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:10:01,637[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:10:02,849[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:10:02,899[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:10:02,900[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T00:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:10:03,262[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:10:03,407[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:10:03,408[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:10:03,408[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:10:03,408[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:10:03,409[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 00:10:03,409[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:10:03,412[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:10:03,994[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:10:04,040[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:10:04,040[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T00:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:10:40,656[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:10:40,848[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:10:40,850[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:10:40,850[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:10:40,851[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:10:40,852[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 00:10:40,853[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:10:40,856[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:10:42,188[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:10:42,299[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:10:42,299[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T00:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:12:34,768[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:12:34,784[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21723) last sent a heartbeat 113.97 seconds ago! Restarting it[0m
[[34m2021-03-31 00:12:34,787[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21723[0m
[[34m2021-03-31 00:12:34,879[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21723, status='terminated', exitcode=0, started='00:02:34') (21723) terminated with exit code 0[0m
[[34m2021-03-31 00:12:34,882[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 574[0m
[[34m2021-03-31 00:12:34,889[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 00:12:34,899] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 00:12:35,050[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 00:00:00+00:00: scheduled__2021-03-31T00:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 00:13:40,595[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:18:40,624[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:20:01,135[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:20:01,137[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:20:01,137[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:20:01,137[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:20:01,139[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 0, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 00:20:01,139[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:20:01,143[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:20:02,353[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:20:02,408[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:20:02,408[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T00:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:20:02,877[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 00:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:20:03,022[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 00:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:20:03,023[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:20:03,023[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:20:03,023[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 00:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:20:03,024[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 0, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 00:20:03,024[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:20:03,027[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:20:03,616[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:20:03,664[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:20:03,664[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T00:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:20:40,830[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 00:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:20:40,995[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 00:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:20:40,996[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:20:40,996[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:20:40,996[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 00:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:20:40,997[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 0, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 00:20:40,997[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:20:41,000[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:20:41,867[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:20:41,970[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:20:41,970[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T00:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:22:33,103[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 00:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:22:33,119[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=574) last sent a heartbeat 112.15 seconds ago! Restarting it[0m
[[34m2021-03-31 00:22:33,122[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 574[0m
[[34m2021-03-31 00:22:33,214[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=574, status='terminated', exitcode=0, started='00:12:34') (574) terminated with exit code 0[0m
[[34m2021-03-31 00:22:33,216[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12162[0m
[[34m2021-03-31 00:22:33,224[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 00:22:33,234] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 00:22:33,422[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 00:10:00+00:00: scheduled__2021-03-31T00:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 00:23:40,758[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:28:40,793[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:30:01,774[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:30:01,775[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:30:01,775[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:30:01,775[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:30:01,776[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 0, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 00:30:01,776[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:30:01,779[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:30:03,042[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:30:03,138[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:30:03,138[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T00:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:30:03,774[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 00:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:30:03,954[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 00:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:30:03,956[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:30:03,956[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:30:03,956[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 00:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:30:03,958[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 0, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 00:30:03,958[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:30:03,961[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:30:04,937[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:30:05,015[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:30:05,015[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T00:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:30:43,963[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 00:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:30:44,440[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 00:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:30:44,441[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:30:44,442[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:30:44,442[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 00:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:30:44,443[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 0, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 00:30:44,443[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:30:44,447[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:30:45,431[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:30:45,505[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:30:45,505[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T00:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:32:48,013[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 00:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:32:48,029[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12162) last sent a heartbeat 123.92 seconds ago! Restarting it[0m
[[34m2021-03-31 00:32:48,032[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12162[0m
[[34m2021-03-31 00:32:48,124[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12162, status='terminated', exitcode=0, started='00:22:32') (12162) terminated with exit code 0[0m
[[34m2021-03-31 00:32:48,127[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23745[0m
[[34m2021-03-31 00:32:48,135[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 00:32:48,145] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 00:32:48,569[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 00:20:00+00:00: scheduled__2021-03-31T00:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 00:33:40,935[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:38:41,066[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:40:01,386[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:40:01,387[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:40:01,387[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:40:01,387[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:40:01,388[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 0, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 00:40:01,388[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:40:01,392[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:40:02,456[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:40:02,532[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:40:02,533[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T00:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:40:03,130[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 00:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:40:03,285[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 00:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:40:03,286[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:40:03,287[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:40:03,287[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 00:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:40:03,288[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 0, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 00:40:03,288[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:40:03,291[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:40:03,877[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:40:03,923[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:40:03,923[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T00:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:40:41,386[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 00:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:40:41,552[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 00:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:40:41,553[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:40:41,553[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:40:41,553[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 00:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:40:41,554[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 0, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 00:40:41,554[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:40:41,557[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:40:42,728[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:40:42,803[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:40:42,803[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T00:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:42:35,011[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 00:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:42:35,026[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23745) last sent a heartbeat 113.50 seconds ago! Restarting it[0m
[[34m2021-03-31 00:42:35,030[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23745[0m
[[34m2021-03-31 00:42:35,122[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23745, status='terminated', exitcode=0, started='00:32:47') (23745) terminated with exit code 0[0m
[[34m2021-03-31 00:42:35,124[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2574[0m
[[34m2021-03-31 00:42:35,132[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 00:42:35,144] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 00:42:35,321[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 00:30:00+00:00: scheduled__2021-03-31T00:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 00:43:41,197[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:48:41,227[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:50:01,600[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:50:01,601[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:50:01,601[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:50:01,601[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:50:01,603[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 0, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 00:50:01,603[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:50:01,612[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:50:03,209[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:50:03,298[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:50:03,299[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T00:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:50:04,052[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 00:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:50:04,215[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 00:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:50:04,217[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:50:04,217[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:50:04,217[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 00:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:50:04,221[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 0, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 00:50:04,222[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:50:04,225[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:50:05,269[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:50:05,317[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:50:05,317[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T00:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:50:44,655[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 00:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:50:45,119[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 00:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:50:45,120[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 00:50:45,120[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 00:50:45,120[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 00:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 00:50:45,121[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 0, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 00:50:45,121[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:50:45,124[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 00:50:45,994[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 00:50:46,075[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 00:50:46,075[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T00:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 00:52:34,085[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 00:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 00:52:34,103[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2574) last sent a heartbeat 109.30 seconds ago! Restarting it[0m
[[34m2021-03-31 00:52:34,106[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2574[0m
[[34m2021-03-31 00:52:34,246[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2574, status='terminated', exitcode=0, started='00:42:34') (2574) terminated with exit code 0[0m
[[34m2021-03-31 00:52:34,249[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14192[0m
[[34m2021-03-31 00:52:34,282[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 00:52:34,297] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 00:52:34,510[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 00:40:00+00:00: scheduled__2021-03-31T00:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 00:53:41,255[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 00:58:41,294[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:00:02,002[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:00:02,003[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:00:02,004[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:00:02,004[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 00:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:00:02,005[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 0, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 01:00:02,005[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:00:02,008[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:00:02,933[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:00:02,993[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:00:02,993[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T00:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:00:03,406[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 00:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:00:03,552[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 00:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:00:03,553[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:00:03,553[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:00:03,553[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 00:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:00:03,555[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 0, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 01:00:03,555[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:00:03,558[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:00:04,157[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:00:04,209[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:00:04,209[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T00:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:00:38,601[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 00:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:00:38,795[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 00:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:00:38,796[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:00:38,797[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:00:38,797[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 00:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:00:38,798[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 0, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 01:00:38,798[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:00:38,802[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:00:39,805[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:00:39,890[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:00:39,890[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T00:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:02:36,002[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 00:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:02:36,020[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14192) last sent a heartbeat 117.26 seconds ago! Restarting it[0m
[[34m2021-03-31 01:02:36,025[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14192[0m
[[34m2021-03-31 01:02:36,245[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14192, status='terminated', exitcode=0, started='00:52:33') (14192) terminated with exit code 0[0m
[[34m2021-03-31 01:02:36,248[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25507[0m
[[34m2021-03-31 01:02:36,256[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 01:02:36,267] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 01:02:36,466[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 00:50:00+00:00: scheduled__2021-03-31T00:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 01:03:41,300[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:08:41,329[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:10:01,584[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:10:01,585[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:10:01,585[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:10:01,585[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:10:01,587[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 1, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 01:10:01,587[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:10:01,590[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:10:02,729[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:10:02,786[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:10:02,786[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T01:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:10:03,184[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 01:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:10:03,339[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 01:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:10:03,340[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:10:03,340[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:10:03,340[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 01:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:10:03,341[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 1, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 01:10:03,341[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:10:03,344[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:10:04,078[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:10:04,128[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:10:04,128[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T01:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:10:43,803[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 01:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:10:43,968[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 01:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:10:43,969[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:10:43,969[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:10:43,969[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 01:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:10:43,970[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 1, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 01:10:43,970[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:10:43,973[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:10:44,836[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:10:44,938[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:10:44,938[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T01:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:12:41,084[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 01:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:12:41,100[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25507) last sent a heartbeat 117.16 seconds ago! Restarting it[0m
[[34m2021-03-31 01:12:41,104[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25507[0m
[[34m2021-03-31 01:12:41,196[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25507, status='terminated', exitcode=0, started='01:02:35') (25507) terminated with exit code 0[0m
[[34m2021-03-31 01:12:41,199[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 4751[0m
[[34m2021-03-31 01:12:41,206[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 01:12:41,216] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 01:12:41,389[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 01:00:00+00:00: scheduled__2021-03-31T01:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 01:13:41,357[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:18:41,484[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:20:01,980[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:20:01,981[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:20:01,981[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:20:01,981[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:20:01,982[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 1, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 01:20:01,982[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:20:01,985[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:20:03,044[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:20:03,103[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:20:03,103[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T01:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:20:03,476[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 01:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:20:03,622[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 01:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:20:03,623[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:20:03,623[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:20:03,623[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 01:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:20:03,624[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 1, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 01:20:03,624[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:20:03,627[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:20:04,233[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:20:04,286[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:20:04,286[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T01:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:20:39,319[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 01:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:20:39,513[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 01:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:20:39,515[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:20:39,515[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:20:39,515[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 01:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:20:39,517[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 1, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 01:20:39,517[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:20:39,520[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:20:40,793[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:20:40,909[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:20:40,909[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T01:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:22:37,511[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 01:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:22:37,528[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=4751) last sent a heartbeat 118.05 seconds ago! Restarting it[0m
[[34m2021-03-31 01:22:37,532[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 4751[0m
[[34m2021-03-31 01:22:37,664[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=4751, status='terminated', exitcode=0, started='01:12:40') (4751) terminated with exit code 0[0m
[[34m2021-03-31 01:22:37,668[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15974[0m
[[34m2021-03-31 01:22:37,692[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 01:22:37,709] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 01:22:37,872[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 01:10:00+00:00: scheduled__2021-03-31T01:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 01:23:41,611[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:28:41,627[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:30:01,709[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:30:01,710[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:30:01,710[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:30:01,710[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:30:01,711[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 1, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 01:30:01,711[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:30:01,714[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:30:02,836[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:30:02,891[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:30:02,891[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T01:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:30:03,242[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 01:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:30:03,387[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 01:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:30:03,388[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:30:03,388[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:30:03,388[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 01:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:30:03,390[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 1, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 01:30:03,390[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:30:03,393[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:30:03,977[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:30:04,034[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:30:04,034[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T01:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:30:41,051[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 01:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:30:41,216[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 01:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:30:41,217[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:30:41,217[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:30:41,217[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 01:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:30:41,218[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 1, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 01:30:41,218[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:30:41,222[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:30:42,133[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:30:42,241[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:30:42,241[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T01:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:32:36,927[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 01:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:32:36,943[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15974) last sent a heartbeat 115.75 seconds ago! Restarting it[0m
[[34m2021-03-31 01:32:36,946[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15974[0m
[[34m2021-03-31 01:32:37,039[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15974, status='terminated', exitcode=0, started='01:22:36') (15974) terminated with exit code 0[0m
[[34m2021-03-31 01:32:37,041[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 27417[0m
[[34m2021-03-31 01:32:37,050[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 01:32:37,060] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 01:32:37,232[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 01:20:00+00:00: scheduled__2021-03-31T01:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 01:33:41,758[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:38:41,783[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:40:01,794[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:40:01,795[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:40:01,795[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:40:01,795[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:40:01,796[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 1, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 01:40:01,796[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:40:01,798[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:40:02,821[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:40:02,873[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:40:02,873[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T01:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:40:03,221[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 01:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:40:03,375[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 01:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:40:03,376[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:40:03,377[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:40:03,377[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 01:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:40:03,378[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 1, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 01:40:03,378[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:40:03,381[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:40:04,125[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:40:04,235[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:40:04,235[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T01:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:40:42,597[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 01:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:40:42,786[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 01:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:40:42,788[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:40:42,789[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:40:42,789[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 01:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:40:42,791[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 1, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 01:40:42,791[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:40:42,794[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:40:43,984[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:40:44,057[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:40:44,057[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T01:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:42:38,523[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 01:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:42:38,541[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=27417) last sent a heartbeat 115.79 seconds ago! Restarting it[0m
[[34m2021-03-31 01:42:38,546[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 27417[0m
[[34m2021-03-31 01:42:38,689[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=27417, status='terminated', exitcode=0, started='01:32:36') (27417) terminated with exit code 0[0m
[[34m2021-03-31 01:42:38,693[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6567[0m
[[34m2021-03-31 01:42:38,718[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 01:42:38,737] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 01:42:38,936[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 01:30:00+00:00: scheduled__2021-03-31T01:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 01:43:41,814[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:48:41,961[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:50:01,946[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:50:01,947[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:50:01,947[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:50:01,948[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:50:01,949[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 1, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 01:50:01,949[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:50:01,952[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:50:03,131[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:50:03,181[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:50:03,181[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T01:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:50:03,537[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 01:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:50:03,691[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 01:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:50:03,692[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:50:03,692[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:50:03,692[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 01:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:50:03,693[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 1, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 01:50:03,693[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:50:03,696[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:50:04,282[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:50:04,330[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:50:04,331[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T01:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:50:40,716[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 01:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:50:40,882[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 01:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:50:40,883[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 01:50:40,883[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 01:50:40,883[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 01:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 01:50:40,884[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 1, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 01:50:40,884[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:50:40,887[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 01:50:41,936[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 01:50:42,010[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 01:50:42,010[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T01:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 01:52:37,314[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 01:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 01:52:37,331[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6567) last sent a heartbeat 116.47 seconds ago! Restarting it[0m
[[34m2021-03-31 01:52:37,334[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6567[0m
[[34m2021-03-31 01:52:37,426[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6567, status='terminated', exitcode=0, started='01:42:37') (6567) terminated with exit code 0[0m
[[34m2021-03-31 01:52:37,429[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17885[0m
[[34m2021-03-31 01:52:37,438[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 01:52:37,448] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 01:52:37,885[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 01:40:00+00:00: scheduled__2021-03-31T01:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 01:53:42,094[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 01:58:42,121[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:00:01,781[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:00:01,783[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:00:01,783[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:00:01,783[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 01:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:00:01,785[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 1, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 02:00:01,786[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:00:01,789[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:00:03,252[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:00:03,307[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:00:03,307[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T01:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:00:03,675[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 01:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:00:03,830[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 01:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:00:03,831[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:00:03,831[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:00:03,831[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 01:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:00:03,832[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 1, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 02:00:03,832[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:00:03,835[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:00:04,669[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:00:04,752[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:00:04,752[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T01:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:00:43,781[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 01:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:00:43,946[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 01:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:00:43,947[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:00:43,948[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:00:43,948[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 01:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:00:43,949[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 1, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 02:00:43,949[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:00:43,952[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:00:44,792[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:00:44,873[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:00:44,873[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T01:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:02:35,963[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 01:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:02:35,982[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17885) last sent a heartbeat 112.06 seconds ago! Restarting it[0m
[[34m2021-03-31 02:02:35,986[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17885[0m
[[34m2021-03-31 02:02:36,121[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17885, status='terminated', exitcode=0, started='01:52:36') (17885) terminated with exit code 0[0m
[[34m2021-03-31 02:02:36,125[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 29287[0m
[[34m2021-03-31 02:02:36,142[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 02:02:36,160] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 02:02:36,339[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 01:50:00+00:00: scheduled__2021-03-31T01:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 02:03:42,189[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:08:42,231[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:10:01,392[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:10:01,393[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:10:01,393[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:10:01,393[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:10:01,394[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 2, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 02:10:01,394[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:10:01,397[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:10:02,510[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:10:02,566[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:10:02,566[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T02:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:10:02,940[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 02:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:10:03,095[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 02:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:10:03,096[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:10:03,096[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:10:03,096[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 02:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:10:03,097[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 2, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 02:10:03,097[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:10:03,100[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:10:03,688[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:10:03,737[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:10:03,737[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T02:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:10:38,565[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 02:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:10:38,730[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 02:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:10:38,731[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:10:38,731[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:10:38,731[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 02:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:10:38,732[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 2, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 02:10:38,732[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:10:38,735[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:10:39,720[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:10:39,796[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:10:39,796[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T02:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:12:32,305[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 02:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:12:32,320[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=29287) last sent a heartbeat 113.61 seconds ago! Restarting it[0m
[[34m2021-03-31 02:12:32,324[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 29287[0m
[[34m2021-03-31 02:12:32,416[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=29287, status='terminated', exitcode=0, started='02:02:35') (29287) terminated with exit code 0[0m
[[34m2021-03-31 02:12:32,418[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8359[0m
[[34m2021-03-31 02:12:32,426[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 02:12:32,436] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 02:12:32,608[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 02:00:00+00:00: scheduled__2021-03-31T02:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 02:13:42,364[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:18:42,391[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:20:02,143[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:20:02,144[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:20:02,144[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:20:02,144[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:20:02,145[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 2, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 02:20:02,145[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:20:02,148[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:20:03,160[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:20:03,210[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:20:03,211[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T02:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:20:03,561[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 02:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:20:03,719[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 02:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:20:03,720[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:20:03,720[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:20:03,720[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 02:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:20:03,721[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 2, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 02:20:03,721[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:20:03,724[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:20:04,311[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:20:04,357[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:20:04,357[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T02:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:20:42,003[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 02:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:20:42,169[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 02:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:20:42,170[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:20:42,170[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:20:42,170[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 02:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:20:42,171[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 2, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 02:20:42,171[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:20:42,174[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:20:43,043[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:20:43,157[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:20:43,157[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T02:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:22:50,897[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 02:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:22:50,915[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8359) last sent a heartbeat 128.77 seconds ago! Restarting it[0m
[[34m2021-03-31 02:22:50,920[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8359[0m
[[34m2021-03-31 02:22:51,059[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8359, status='terminated', exitcode=0, started='02:12:31') (8359) terminated with exit code 0[0m
[[34m2021-03-31 02:22:51,062[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19940[0m
[[34m2021-03-31 02:22:51,085[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 02:22:51,102] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 02:22:51,280[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 02:10:00+00:00: scheduled__2021-03-31T02:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 02:23:42,510[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:28:42,641[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:30:01,215[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:30:01,217[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:30:01,217[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:30:01,217[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:30:01,218[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 2, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 02:30:01,219[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:30:01,222[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:30:02,461[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:30:02,514[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:30:02,514[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T02:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:30:02,880[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 02:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:30:03,035[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 02:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:30:03,036[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:30:03,036[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:30:03,036[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 02:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:30:03,037[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 2, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 02:30:03,037[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:30:03,040[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:30:03,626[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:30:03,673[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:30:03,673[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T02:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:30:38,893[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 02:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:30:39,086[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 02:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:30:39,087[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:30:39,087[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:30:39,087[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 02:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:30:39,089[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 2, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 02:30:39,089[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:30:39,093[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:30:40,343[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:30:40,453[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:30:40,453[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T02:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:32:34,647[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 02:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:32:34,662[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19940) last sent a heartbeat 115.61 seconds ago! Restarting it[0m
[[34m2021-03-31 02:32:34,665[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19940[0m
[[34m2021-03-31 02:32:34,759[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19940, status='terminated', exitcode=0, started='02:22:50') (19940) terminated with exit code 0[0m
[[34m2021-03-31 02:32:34,762[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31054[0m
[[34m2021-03-31 02:32:34,788[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 02:32:34,801] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 02:32:34,997[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 02:20:00+00:00: scheduled__2021-03-31T02:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 02:33:42,663[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:38:42,693[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:40:01,896[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:40:01,898[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:40:01,898[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:40:01,898[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:40:01,900[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 2, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 02:40:01,900[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:40:01,903[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:40:03,219[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:40:03,269[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:40:03,270[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T02:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:40:03,629[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 02:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:40:03,775[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 02:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:40:03,776[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:40:03,776[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:40:03,776[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 02:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:40:03,777[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 2, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 02:40:03,777[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:40:03,780[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:40:04,363[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:40:04,410[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:40:04,410[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T02:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:40:41,631[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 02:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:40:41,796[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 02:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:40:41,797[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:40:41,798[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:40:41,798[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 02:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:40:41,799[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 2, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 02:40:41,799[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:40:41,802[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:40:42,722[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:40:42,831[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:40:42,831[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T02:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:42:39,960[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 02:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:42:39,976[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31054) last sent a heartbeat 118.20 seconds ago! Restarting it[0m
[[34m2021-03-31 02:42:39,979[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31054[0m
[[34m2021-03-31 02:42:40,072[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31054, status='terminated', exitcode=0, started='02:32:33') (31054) terminated with exit code 0[0m
[[34m2021-03-31 02:42:40,074[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10304[0m
[[34m2021-03-31 02:42:40,082[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 02:42:40,091] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 02:42:40,261[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 02:30:00+00:00: scheduled__2021-03-31T02:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 02:43:42,814[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:48:42,842[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:50:01,360[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:50:01,361[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:50:01,361[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:50:01,361[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:50:01,363[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 2, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 02:50:01,363[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:50:01,366[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:50:02,889[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:50:02,971[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:50:02,971[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T02:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:50:03,486[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 02:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:50:03,647[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 02:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:50:03,648[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:50:03,649[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:50:03,649[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 02:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:50:03,650[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 2, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 02:50:03,651[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:50:03,654[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:50:04,649[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:50:04,730[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:50:04,731[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T02:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:50:42,732[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 02:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:50:43,238[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 02:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:50:43,239[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 02:50:43,239[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 02:50:43,240[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 02:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 02:50:43,241[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 2, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 02:50:43,241[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:50:43,244[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 02:50:44,096[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 02:50:44,203[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 02:50:44,204[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T02:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 02:52:37,976[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 02:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 02:52:37,994[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10304) last sent a heartbeat 115.10 seconds ago! Restarting it[0m
[[34m2021-03-31 02:52:37,999[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10304[0m
[[34m2021-03-31 02:52:38,142[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10304, status='terminated', exitcode=0, started='02:42:39') (10304) terminated with exit code 0[0m
[[34m2021-03-31 02:52:38,145[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21596[0m
[[34m2021-03-31 02:52:38,170[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 02:52:38,187] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 02:52:38,366[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 02:40:00+00:00: scheduled__2021-03-31T02:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 02:53:42,972[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 02:58:43,008[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:00:01,318[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:00:01,319[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:00:01,319[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:00:01,319[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 02:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:00:01,320[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 2, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 03:00:01,320[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:00:01,323[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:00:02,419[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:00:02,476[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:00:02,476[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T02:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:00:02,897[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 02:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:00:03,052[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 02:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:00:03,053[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:00:03,053[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:00:03,054[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 02:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:00:03,055[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 2, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 03:00:03,055[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:00:03,058[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:00:03,653[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:00:03,721[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:00:03,721[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T02:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:00:39,608[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 02:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:00:39,800[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 02:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:00:39,801[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:00:39,801[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:00:39,801[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 02:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:00:39,802[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 2, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 03:00:39,802[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:00:39,805[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:00:40,658[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:00:40,735[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:00:40,735[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T02:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:02:38,176[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 02:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:02:38,192[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21596) last sent a heartbeat 118.42 seconds ago! Restarting it[0m
[[34m2021-03-31 03:02:38,195[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21596[0m
[[34m2021-03-31 03:02:38,287[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21596, status='terminated', exitcode=0, started='02:52:37') (21596) terminated with exit code 0[0m
[[34m2021-03-31 03:02:38,290[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 610[0m
[[34m2021-03-31 03:02:38,314[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 03:02:38,328] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 03:02:38,519[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 02:50:00+00:00: scheduled__2021-03-31T02:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 03:03:43,141[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:08:43,167[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:10:01,920[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:10:01,922[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:10:01,922[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:10:01,922[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:10:01,924[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 3, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 03:10:01,924[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:10:01,927[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:10:03,207[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:10:03,295[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:10:03,295[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T03:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:10:03,949[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 03:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:10:04,103[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 03:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:10:04,104[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:10:04,104[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:10:04,104[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 03:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:10:04,105[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 3, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 03:10:04,105[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:10:04,108[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:10:04,707[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:10:04,756[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:10:04,756[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T03:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:10:45,568[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 03:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:10:45,988[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 03:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:10:45,989[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:10:45,989[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:10:45,989[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 03:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:10:45,990[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 3, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 03:10:45,990[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:10:45,994[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:10:46,934[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:10:47,007[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:10:47,007[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T03:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:12:43,274[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 03:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:12:43,289[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=610) last sent a heartbeat 117.58 seconds ago! Restarting it[0m
[[34m2021-03-31 03:12:43,293[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 610[0m
[[34m2021-03-31 03:12:43,385[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=610, status='terminated', exitcode=0, started='03:02:37') (610) terminated with exit code 0[0m
[[34m2021-03-31 03:12:43,388[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12309[0m
[[34m2021-03-31 03:12:43,396[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 03:12:43,406] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 03:12:43,585[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 03:00:00+00:00: scheduled__2021-03-31T03:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 03:13:43,193[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:18:43,220[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:20:01,488[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:20:01,489[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:20:01,489[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:20:01,489[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:20:01,490[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 3, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 03:20:01,491[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:20:01,494[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:20:02,975[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:20:03,028[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:20:03,028[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T03:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:20:03,418[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 03:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:20:03,581[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 03:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:20:03,582[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:20:03,582[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:20:03,582[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 03:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:20:03,583[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 3, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 03:20:03,583[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:20:03,586[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:20:04,176[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:20:04,228[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:20:04,228[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T03:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:20:41,448[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 03:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:20:42,018[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 03:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:20:42,019[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:20:42,019[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:20:42,019[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 03:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:20:42,021[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 3, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 03:20:42,021[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:20:42,024[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:20:43,574[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:20:43,701[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:20:43,701[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T03:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:22:39,437[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 03:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:22:39,453[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12309) last sent a heartbeat 117.84 seconds ago! Restarting it[0m
[[34m2021-03-31 03:22:39,456[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12309[0m
[[34m2021-03-31 03:22:39,548[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12309, status='terminated', exitcode=0, started='03:12:42') (12309) terminated with exit code 0[0m
[[34m2021-03-31 03:22:39,550[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23676[0m
[[34m2021-03-31 03:22:39,558[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 03:22:39,568] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 03:22:39,723[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 03:10:00+00:00: scheduled__2021-03-31T03:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 03:23:43,302[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:28:43,331[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:30:01,846[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:30:01,847[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:30:01,848[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:30:01,848[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:30:01,849[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 3, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 03:30:01,849[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:30:01,852[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:30:03,017[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:30:03,068[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:30:03,068[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T03:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:30:03,459[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 03:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:30:03,605[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 03:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:30:03,606[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:30:03,606[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:30:03,606[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 03:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:30:03,607[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 3, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 03:30:03,607[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:30:03,611[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:30:04,205[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:30:04,252[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:30:04,252[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T03:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:30:43,337[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 03:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:30:43,509[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 03:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:30:43,510[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:30:43,510[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:30:43,510[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 03:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:30:43,511[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 3, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 03:30:43,511[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:30:43,516[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:30:44,679[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:30:44,756[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:30:44,756[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T03:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:32:39,716[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 03:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:32:39,732[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23676) last sent a heartbeat 116.25 seconds ago! Restarting it[0m
[[34m2021-03-31 03:32:39,735[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23676[0m
[[34m2021-03-31 03:32:39,827[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23676, status='terminated', exitcode=0, started='03:22:38') (23676) terminated with exit code 0[0m
[[34m2021-03-31 03:32:39,830[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2813[0m
[[34m2021-03-31 03:32:39,838[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 03:32:39,848] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 03:32:40,027[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 03:20:00+00:00: scheduled__2021-03-31T03:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 03:33:43,462[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:38:43,540[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:40:01,211[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:40:01,213[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:40:01,213[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:40:01,213[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:40:01,215[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 3, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 03:40:01,215[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:40:01,222[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:40:02,479[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:40:02,531[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:40:02,532[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T03:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:40:02,950[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 03:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:40:03,096[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 03:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:40:03,097[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:40:03,097[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:40:03,097[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 03:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:40:03,098[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 3, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 03:40:03,098[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:40:03,101[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:40:03,733[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:40:03,804[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:40:03,804[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T03:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:40:39,601[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 03:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:40:39,793[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 03:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:40:39,795[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:40:39,795[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:40:39,795[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 03:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:40:39,796[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 3, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 03:40:39,797[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:40:39,800[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:40:41,095[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:40:41,200[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:40:41,200[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T03:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:42:34,875[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 03:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:42:34,891[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2813) last sent a heartbeat 115.13 seconds ago! Restarting it[0m
[[34m2021-03-31 03:42:34,894[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2813[0m
[[34m2021-03-31 03:42:35,027[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2813, status='terminated', exitcode=0, started='03:32:39') (2813) terminated with exit code 0[0m
[[34m2021-03-31 03:42:35,030[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14255[0m
[[34m2021-03-31 03:42:35,057[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 03:42:35,074] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 03:42:35,253[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 03:30:00+00:00: scheduled__2021-03-31T03:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 03:43:43,679[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:48:43,713[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:50:01,450[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:50:01,451[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:50:01,451[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:50:01,451[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:50:01,452[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 3, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 03:50:01,452[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:50:01,455[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:50:02,502[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:50:02,556[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:50:02,556[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T03:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:50:02,975[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 03:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:50:03,133[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 03:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:50:03,135[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:50:03,135[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:50:03,135[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 03:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:50:03,137[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 3, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 03:50:03,137[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:50:03,140[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:50:04,055[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:50:04,102[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:50:04,102[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T03:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:50:42,642[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 03:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:50:43,113[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 03:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:50:43,114[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 03:50:43,114[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 03:50:43,114[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 03:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 03:50:43,115[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 3, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 03:50:43,115[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:50:43,118[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 03:50:44,035[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 03:50:44,112[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 03:50:44,113[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T03:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 03:52:39,225[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 03:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 03:52:39,242[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14255) last sent a heartbeat 116.45 seconds ago! Restarting it[0m
[[34m2021-03-31 03:52:39,247[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14255[0m
[[34m2021-03-31 03:52:39,387[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14255, status='terminated', exitcode=0, started='03:42:34') (14255) terminated with exit code 0[0m
[[34m2021-03-31 03:52:39,391[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25642[0m
[[34m2021-03-31 03:52:39,405[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 03:52:39,420] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 03:52:39,647[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 03:40:00+00:00: scheduled__2021-03-31T03:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 03:53:43,844[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 03:58:43,872[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:00:01,431[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:00:01,433[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:00:01,433[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:00:01,433[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 03:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:00:01,441[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 3, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 04:00:01,441[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:00:01,445[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:00:03,061[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:00:03,140[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:00:03,140[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T03:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:00:03,649[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 03:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:00:03,812[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 03:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:00:03,814[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:00:03,814[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:00:03,814[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 03:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:00:03,816[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 3, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 04:00:03,816[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:00:03,820[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:00:04,811[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:00:04,888[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:00:04,888[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T03:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:00:43,596[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 03:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:00:43,761[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 03:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:00:43,762[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:00:43,762[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:00:43,762[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 03:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:00:43,763[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 3, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 04:00:43,763[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:00:43,766[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:00:44,868[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:00:44,946[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:00:44,946[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T03:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:03:13,251[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 03:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:03:13,269[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25642) last sent a heartbeat 149.53 seconds ago! Restarting it[0m
[[34m2021-03-31 04:03:13,273[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25642[0m
[[34m2021-03-31 04:03:13,447[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25642, status='terminated', exitcode=0, started='03:52:38') (25642) terminated with exit code 0[0m
[[34m2021-03-31 04:03:13,451[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5233[0m
[[34m2021-03-31 04:03:13,464[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 04:03:13,476] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 04:03:13,639[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 03:50:00+00:00: scheduled__2021-03-31T03:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 04:03:43,899[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:08:44,039[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:10:01,933[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:10:01,934[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:10:01,934[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:10:01,935[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:10:01,936[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 4, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 04:10:01,936[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:10:01,939[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:10:02,880[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:10:02,934[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:10:02,934[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T04:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:10:03,371[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 04:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:10:03,554[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 04:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:10:03,555[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:10:03,555[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:10:03,555[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 04:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:10:03,556[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 4, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 04:10:03,556[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:10:03,559[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:10:04,146[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:10:04,195[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:10:04,196[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T04:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:10:40,266[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 04:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:10:40,431[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 04:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:10:40,432[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:10:40,432[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:10:40,432[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 04:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:10:40,433[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 4, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 04:10:40,433[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:10:40,437[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:10:41,594[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:10:41,672[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:10:41,673[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T04:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:12:39,699[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 04:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:12:39,715[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5233) last sent a heartbeat 119.31 seconds ago! Restarting it[0m
[[34m2021-03-31 04:12:39,718[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5233[0m
[[34m2021-03-31 04:12:39,810[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5233, status='terminated', exitcode=0, started='04:03:12') (5233) terminated with exit code 0[0m
[[34m2021-03-31 04:12:39,813[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16174[0m
[[34m2021-03-31 04:12:39,821[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 04:12:39,831] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 04:12:39,984[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 04:00:00+00:00: scheduled__2021-03-31T04:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 04:13:44,171[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:18:44,303[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:20:02,063[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:20:02,064[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:20:02,064[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:20:02,064[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:20:02,065[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 4, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 04:20:02,065[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:20:02,068[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:20:02,988[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:20:03,042[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:20:03,042[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T04:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:20:03,391[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 04:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:20:03,537[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 04:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:20:03,538[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:20:03,538[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:20:03,538[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 04:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:20:03,539[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 4, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 04:20:03,539[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:20:03,542[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:20:04,127[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:20:04,173[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:20:04,173[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T04:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:20:44,519[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 04:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:20:44,684[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 04:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:20:44,685[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:20:44,685[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:20:44,685[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 04:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:20:44,686[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 4, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 04:20:44,686[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:20:44,692[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:20:45,590[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:20:45,666[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:20:45,666[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T04:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:22:38,716[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 04:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:22:38,732[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16174) last sent a heartbeat 114.07 seconds ago! Restarting it[0m
[[34m2021-03-31 04:22:38,735[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16174[0m
[[34m2021-03-31 04:22:38,827[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16174, status='terminated', exitcode=0, started='04:12:39') (16174) terminated with exit code 0[0m
[[34m2021-03-31 04:22:38,830[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 27447[0m
[[34m2021-03-31 04:22:38,837[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 04:22:38,847] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 04:22:39,001[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 04:10:00+00:00: scheduled__2021-03-31T04:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 04:23:44,433[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:28:44,462[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:30:01,100[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:30:01,102[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:30:01,102[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:30:01,102[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:30:01,103[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 4, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 04:30:01,103[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:30:01,107[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:30:02,228[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:30:02,279[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:30:02,279[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T04:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:30:02,640[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 04:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:30:02,785[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 04:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:30:02,786[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:30:02,786[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:30:02,786[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 04:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:30:02,787[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 4, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 04:30:02,787[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:30:02,790[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:30:03,497[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:30:03,542[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:30:03,543[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T04:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:30:37,752[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 04:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:30:37,916[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 04:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:30:37,917[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:30:37,918[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:30:37,918[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 04:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:30:37,919[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 4, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 04:30:37,919[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:30:37,922[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:30:38,824[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:30:38,966[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:30:38,966[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T04:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:32:34,810[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 04:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:32:34,826[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=27447) last sent a heartbeat 116.93 seconds ago! Restarting it[0m
[[34m2021-03-31 04:32:34,830[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 27447[0m
[[34m2021-03-31 04:32:34,922[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=27447, status='terminated', exitcode=0, started='04:22:38') (27447) terminated with exit code 0[0m
[[34m2021-03-31 04:32:34,924[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6499[0m
[[34m2021-03-31 04:32:34,932[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 04:32:34,942] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 04:32:35,092[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 04:20:00+00:00: scheduled__2021-03-31T04:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 04:33:44,602[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:38:44,631[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:40:01,112[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:40:01,113[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:40:01,113[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:40:01,113[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:40:01,114[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 4, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 04:40:01,114[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:40:01,117[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:40:02,132[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:40:02,232[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:40:02,232[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T04:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:40:02,689[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 04:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:40:02,834[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 04:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:40:02,835[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:40:02,836[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:40:02,836[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 04:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:40:02,837[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 4, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 04:40:02,837[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:40:02,839[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:40:03,573[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:40:03,618[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:40:03,619[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T04:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:40:43,423[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 04:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:40:43,588[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 04:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:40:43,589[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:40:43,589[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:40:43,589[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 04:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:40:43,590[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 4, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 04:40:43,591[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:40:43,594[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:40:44,637[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:40:44,714[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:40:44,714[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T04:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:42:37,072[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 04:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:42:37,088[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6499) last sent a heartbeat 113.52 seconds ago! Restarting it[0m
[[34m2021-03-31 04:42:37,093[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6499[0m
[[34m2021-03-31 04:42:37,193[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6499, status='terminated', exitcode=0, started='04:32:34') (6499) terminated with exit code 0[0m
[[34m2021-03-31 04:42:37,195[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17893[0m
[[34m2021-03-31 04:42:37,203[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 04:42:37,213] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 04:42:37,384[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 04:30:00+00:00: scheduled__2021-03-31T04:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 04:43:44,762[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:48:44,790[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:50:01,738[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:50:01,739[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:50:01,740[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:50:01,740[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:50:01,741[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 4, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 04:50:01,741[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:50:01,744[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:50:03,002[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:50:03,054[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:50:03,054[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T04:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:50:03,438[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 04:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:50:03,610[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 04:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:50:03,611[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:50:03,611[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:50:03,611[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 04:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:50:03,612[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 4, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 04:50:03,612[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:50:03,615[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:50:04,597[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:50:04,670[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:50:04,670[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T04:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:50:43,753[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 04:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:50:44,308[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 04:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:50:44,310[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 04:50:44,310[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 04:50:44,310[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 04:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 04:50:44,312[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 4, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 04:50:44,312[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:50:44,315[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 04:50:45,412[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 04:50:45,490[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 04:50:45,491[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T04:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 04:52:37,808[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 04:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 04:52:37,826[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17893) last sent a heartbeat 113.92 seconds ago! Restarting it[0m
[[34m2021-03-31 04:52:37,830[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17893[0m
[[34m2021-03-31 04:52:37,965[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17893, status='terminated', exitcode=0, started='04:42:36') (17893) terminated with exit code 0[0m
[[34m2021-03-31 04:52:37,969[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 29208[0m
[[34m2021-03-31 04:52:37,979[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 04:52:38,005] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 04:52:38,215[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 04:40:00+00:00: scheduled__2021-03-31T04:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 04:53:44,827[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 04:58:44,864[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:00:01,398[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:00:01,399[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:00:01,399[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:00:01,399[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 04:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:00:01,400[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 4, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 05:00:01,400[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:00:01,403[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:00:02,322[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:00:02,373[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:00:02,373[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T04:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:00:02,739[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 04:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:00:02,885[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 04:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:00:02,886[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:00:02,886[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:00:02,886[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 04:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:00:02,887[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 4, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 05:00:02,887[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:00:02,890[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:00:03,476[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:00:03,522[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:00:03,522[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T04:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:00:40,833[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 04:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:00:40,999[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 04:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:00:41,000[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:00:41,000[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:00:41,000[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 04:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:00:41,001[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 4, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 05:00:41,001[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:00:41,004[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:00:42,219[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:00:42,313[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:00:42,313[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T04:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:02:36,801[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 04:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:02:36,817[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=29208) last sent a heartbeat 115.84 seconds ago! Restarting it[0m
[[34m2021-03-31 05:02:36,820[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 29208[0m
[[34m2021-03-31 05:02:36,993[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=29208, status='terminated', exitcode=0, started='04:52:37') (29208) terminated with exit code 0[0m
[[34m2021-03-31 05:02:36,995[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8370[0m
[[34m2021-03-31 05:02:37,004[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 05:02:37,017] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 05:02:37,188[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 04:50:00+00:00: scheduled__2021-03-31T04:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 05:03:44,892[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:08:44,919[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:10:01,249[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:10:01,251[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:10:01,251[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:10:01,251[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:10:01,252[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 05:10:01,253[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:10:01,256[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:10:03,009[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:10:03,094[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:10:03,094[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T05:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:10:03,729[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:10:03,904[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 05:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:10:03,905[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:10:03,906[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:10:03,906[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 05:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:10:03,907[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 05:10:03,908[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:10:03,911[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:10:04,913[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:10:04,999[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:10:04,999[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T05:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:10:45,845[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:10:46,013[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 05:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:10:46,014[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:10:46,014[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:10:46,014[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 05:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:10:46,015[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 05:10:46,015[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:10:46,018[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:10:47,294[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:10:47,383[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:10:47,383[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T05:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:12:39,385[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:12:39,401[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8370) last sent a heartbeat 113.41 seconds ago! Restarting it[0m
[[34m2021-03-31 05:12:39,404[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8370[0m
[[34m2021-03-31 05:12:39,496[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8370, status='terminated', exitcode=0, started='05:02:36') (8370) terminated with exit code 0[0m
[[34m2021-03-31 05:12:39,499[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19756[0m
[[34m2021-03-31 05:12:39,508[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 05:12:39,519] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 05:12:39,671[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 05:00:00+00:00: scheduled__2021-03-31T05:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 05:13:45,051[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:18:45,086[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:20:01,951[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:20:01,952[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:20:01,952[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:20:01,953[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:20:01,954[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 05:20:01,954[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:20:01,957[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:20:02,960[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:20:03,009[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:20:03,010[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T05:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:20:03,328[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:20:03,474[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 05:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:20:03,474[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:20:03,475[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:20:03,475[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 05:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:20:03,476[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 05:20:03,476[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:20:03,479[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:20:04,065[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:20:04,113[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:20:04,113[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T05:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:20:40,652[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:20:40,817[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 05:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:20:40,818[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:20:40,818[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:20:40,818[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 05:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:20:40,819[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 05:20:40,819[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:20:40,823[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:20:42,087[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:20:42,167[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:20:42,167[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T05:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:22:40,753[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:22:40,769[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19756) last sent a heartbeat 119.98 seconds ago! Restarting it[0m
[[34m2021-03-31 05:22:40,772[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19756[0m
[[34m2021-03-31 05:22:40,864[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19756, status='terminated', exitcode=0, started='05:12:38') (19756) terminated with exit code 0[0m
[[34m2021-03-31 05:22:40,867[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31098[0m
[[34m2021-03-31 05:22:40,875[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 05:22:40,886] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 05:22:41,036[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 05:10:00+00:00: scheduled__2021-03-31T05:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 05:23:45,217[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:28:45,348[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:30:02,057[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:30:02,058[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:30:02,058[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:30:02,058[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:30:02,059[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 05:30:02,059[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:30:02,063[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:30:03,040[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:30:03,092[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:30:03,092[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T05:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:30:03,429[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:30:03,574[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 05:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:30:03,575[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:30:03,575[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:30:03,575[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 05:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:30:03,576[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 05:30:03,577[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:30:03,580[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:30:04,357[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:30:04,444[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:30:04,445[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T05:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:30:42,776[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:30:42,964[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 05:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:30:42,965[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:30:42,965[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:30:42,965[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 05:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:30:42,967[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 05:30:42,967[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:30:42,972[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:30:44,279[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:30:44,365[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:30:44,365[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T05:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:32:35,152[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:32:35,170[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31098) last sent a heartbeat 112.24 seconds ago! Restarting it[0m
[[34m2021-03-31 05:32:35,173[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31098[0m
[[34m2021-03-31 05:32:35,316[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31098, status='terminated', exitcode=0, started='05:22:40') (31098) terminated with exit code 0[0m
[[34m2021-03-31 05:32:35,318[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10153[0m
[[34m2021-03-31 05:32:35,329[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 05:32:35,349] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 05:32:35,528[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 05:20:00+00:00: scheduled__2021-03-31T05:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 05:33:45,415[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:38:45,547[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:40:01,334[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:40:01,335[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:40:01,335[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:40:01,335[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:40:01,336[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 05:40:01,336[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:40:01,339[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:40:02,316[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:40:02,365[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:40:02,365[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T05:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:40:02,681[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:40:02,827[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 05:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:40:02,828[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:40:02,828[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:40:02,828[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 05:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:40:02,829[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 05:40:02,829[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:40:02,832[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:40:03,463[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:40:03,532[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:40:03,532[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T05:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:40:38,219[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:40:38,384[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 05:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:40:38,385[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:40:38,386[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:40:38,386[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 05:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:40:38,387[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 05:40:38,387[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:40:38,390[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:40:39,607[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:40:39,717[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:40:39,717[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T05:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:42:33,869[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:42:33,885[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10153) last sent a heartbeat 115.52 seconds ago! Restarting it[0m
[[34m2021-03-31 05:42:33,888[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10153[0m
[[34m2021-03-31 05:42:33,980[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10153, status='terminated', exitcode=0, started='05:32:34') (10153) terminated with exit code 0[0m
[[34m2021-03-31 05:42:33,983[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21484[0m
[[34m2021-03-31 05:42:33,992[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 05:42:34,003] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 05:42:34,175[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 05:30:00+00:00: scheduled__2021-03-31T05:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 05:43:45,694[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:48:45,724[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:50:01,113[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:50:01,114[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:50:01,114[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:50:01,114[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:50:01,115[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 05:50:01,115[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:50:01,118[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:50:02,293[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:50:02,353[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:50:02,353[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T05:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:50:02,713[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:50:02,859[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 05:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:50:02,860[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:50:02,860[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:50:02,860[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 05:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:50:02,861[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 05:50:02,861[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:50:02,864[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:50:03,447[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:50:03,494[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:50:03,494[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T05:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:50:41,672[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:50:41,857[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 05:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:50:41,858[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 05:50:41,858[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 05:50:41,858[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 05:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 05:50:41,860[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 05:50:41,860[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:50:41,863[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 05:50:42,922[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 05:50:43,000[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 05:50:43,000[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T05:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 05:52:33,658[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 05:52:33,674[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21484) last sent a heartbeat 111.85 seconds ago! Restarting it[0m
[[34m2021-03-31 05:52:33,677[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21484[0m
[[34m2021-03-31 05:52:33,769[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21484, status='terminated', exitcode=0, started='05:42:33') (21484) terminated with exit code 0[0m
[[34m2021-03-31 05:52:33,771[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 388[0m
[[34m2021-03-31 05:52:33,781[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 05:52:33,792] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 05:52:33,942[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 05:40:00+00:00: scheduled__2021-03-31T05:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 05:53:45,758[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 05:58:45,787[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:00:01,201[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:00:01,202[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:00:01,202[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:00:01,202[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 05:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:00:01,203[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 06:00:01,203[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:00:01,206[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:00:02,314[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:00:02,370[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:00:02,370[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T05:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:00:02,843[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:00:02,989[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 05:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:00:02,990[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:00:02,990[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:00:02,990[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 05:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:00:02,991[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 06:00:02,991[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:00:02,994[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:00:03,584[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:00:03,632[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:00:03,632[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T05:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:00:41,131[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:00:41,318[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 05:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:00:41,319[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:00:41,320[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:00:41,320[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 05:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:00:41,321[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 06:00:41,321[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:00:41,324[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:00:42,823[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:00:42,939[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:00:42,939[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T05:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:03:24,425[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:03:24,440[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=388) last sent a heartbeat 163.16 seconds ago! Restarting it[0m
[[34m2021-03-31 06:03:24,443[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 388[0m
[[34m2021-03-31 06:03:24,616[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=388, status='terminated', exitcode=0, started='05:52:32') (388) terminated with exit code 0[0m
[[34m2021-03-31 06:03:24,621[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12639[0m
[[34m2021-03-31 06:03:24,632[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 06:03:24,645] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 06:03:24,849[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 05:50:00+00:00: scheduled__2021-03-31T05:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 06:03:45,926[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:08:46,059[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:10:02,015[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:10:02,016[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:10:02,016[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:10:02,016[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:10:02,017[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 06:10:02,017[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:10:02,020[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:10:03,144[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:10:03,200[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:10:03,200[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T06:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:10:03,721[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:10:03,796[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 06:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:10:03,798[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:10:03,798[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:10:03,798[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 06:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:10:03,800[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 06:10:03,800[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:10:03,804[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:10:04,476[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:10:04,528[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:10:04,528[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T06:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:10:43,166[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:10:43,331[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 06:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:10:43,332[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:10:43,332[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:10:43,332[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 06:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:10:43,333[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 06:10:43,333[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:10:43,336[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:10:44,409[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:10:44,494[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:10:44,494[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T06:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:12:38,394[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:12:38,409[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12639) last sent a heartbeat 115.10 seconds ago! Restarting it[0m
[[34m2021-03-31 06:12:38,413[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12639[0m
[[34m2021-03-31 06:12:38,585[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12639, status='terminated', exitcode=0, started='06:03:23') (12639) terminated with exit code 0[0m
[[34m2021-03-31 06:12:38,588[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23328[0m
[[34m2021-03-31 06:12:38,597[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 06:12:38,609] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 06:12:38,760[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 06:00:00+00:00: scheduled__2021-03-31T06:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 06:13:46,190[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:18:46,216[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:20:01,974[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:20:01,976[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:20:01,976[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:20:01,976[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:20:01,978[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 06:20:01,978[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:20:01,982[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:20:03,586[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:20:03,665[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:20:03,666[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T06:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:20:04,259[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:20:04,430[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 06:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:20:04,431[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:20:04,431[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:20:04,432[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 06:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:20:04,433[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 06:20:04,433[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:20:04,437[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:20:05,450[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:20:05,555[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:20:05,555[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T06:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:20:45,574[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:20:45,740[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 06:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:20:45,741[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:20:45,741[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:20:45,741[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 06:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:20:45,742[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 06:20:45,742[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:20:45,745[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:20:46,929[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:20:47,010[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:20:47,010[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T06:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:22:38,324[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:22:38,342[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23328) last sent a heartbeat 112.63 seconds ago! Restarting it[0m
[[34m2021-03-31 06:22:38,346[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23328[0m
[[34m2021-03-31 06:22:38,480[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23328, status='terminated', exitcode=0, started='06:12:37') (23328) terminated with exit code 0[0m
[[34m2021-03-31 06:22:38,483[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2353[0m
[[34m2021-03-31 06:22:38,506[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 06:22:38,525] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 06:22:38,696[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 06:10:00+00:00: scheduled__2021-03-31T06:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 06:23:46,347[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:28:46,391[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:30:01,809[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:30:01,810[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:30:01,810[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:30:01,810[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:30:01,811[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 06:30:01,811[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:30:01,814[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:30:02,569[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:30:02,615[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:30:02,615[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T06:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:30:02,940[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:30:03,086[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 06:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:30:03,087[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:30:03,087[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:30:03,087[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 06:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:30:03,088[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 06:30:03,089[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:30:03,091[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:30:03,676[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:30:03,722[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:30:03,722[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T06:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:30:40,032[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:30:40,224[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 06:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:30:40,225[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:30:40,225[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:30:40,225[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 06:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:30:40,227[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 06:30:40,227[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:30:40,230[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:30:41,551[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:30:41,631[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:30:41,632[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T06:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:32:28,695[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:32:28,711[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2353) last sent a heartbeat 108.52 seconds ago! Restarting it[0m
[[34m2021-03-31 06:32:28,714[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2353[0m
[[34m2021-03-31 06:32:28,887[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2353, status='terminated', exitcode=0, started='06:22:37') (2353) terminated with exit code 0[0m
[[34m2021-03-31 06:32:28,889[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 13750[0m
[[34m2021-03-31 06:32:28,899[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 06:32:28,911] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 06:32:29,090[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 06:20:00+00:00: scheduled__2021-03-31T06:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 06:33:46,523[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:38:46,658[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:40:01,068[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:40:01,069[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:40:01,069[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:40:01,069[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:40:01,070[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 06:40:01,070[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:40:01,073[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:40:02,306[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:40:02,391[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:40:02,391[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T06:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:40:03,044[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:40:03,207[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 06:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:40:03,208[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:40:03,208[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:40:03,209[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 06:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:40:03,210[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 06:40:03,210[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:40:03,214[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:40:03,912[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:40:03,958[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:40:03,959[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T06:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:40:43,568[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:40:43,734[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 06:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:40:43,735[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:40:43,735[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:40:43,735[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 06:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:40:43,736[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 06:40:43,736[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:40:43,740[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:40:44,705[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:40:44,786[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:40:44,786[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T06:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:42:35,484[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:42:35,500[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=13750) last sent a heartbeat 111.79 seconds ago! Restarting it[0m
[[34m2021-03-31 06:42:35,504[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 13750[0m
[[34m2021-03-31 06:42:35,596[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=13750, status='terminated', exitcode=0, started='06:32:28') (13750) terminated with exit code 0[0m
[[34m2021-03-31 06:42:35,598[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25920[0m
[[34m2021-03-31 06:42:35,608[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 06:42:35,618] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 06:42:35,790[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 06:30:00+00:00: scheduled__2021-03-31T06:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 06:43:46,790[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:48:46,921[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:50:01,519[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:50:01,520[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:50:01,520[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:50:01,521[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:50:01,522[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 06:50:01,522[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:50:01,524[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:50:02,494[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:50:02,548[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:50:02,548[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T06:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:50:03,041[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:50:03,187[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 06:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:50:03,188[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:50:03,188[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:50:03,188[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 06:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:50:03,189[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 06:50:03,190[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:50:03,193[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:50:03,794[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:50:03,843[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:50:03,843[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T06:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:50:39,456[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:50:39,646[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 06:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:50:39,648[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 06:50:39,648[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 06:50:39,648[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 06:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 06:50:39,650[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 06:50:39,650[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:50:39,653[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 06:50:41,199[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 06:50:41,321[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 06:50:41,321[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T06:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 06:52:36,338[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 06:52:36,354[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25920) last sent a heartbeat 116.74 seconds ago! Restarting it[0m
[[34m2021-03-31 06:52:36,357[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25920[0m
[[34m2021-03-31 06:52:36,449[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25920, status='terminated', exitcode=0, started='06:42:34') (25920) terminated with exit code 0[0m
[[34m2021-03-31 06:52:36,452[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5306[0m
[[34m2021-03-31 06:52:36,461[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 06:52:36,472] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 06:52:36,621[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 06:40:00+00:00: scheduled__2021-03-31T06:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 06:53:47,057[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 06:58:47,120[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:00:01,575[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:00:01,576[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:00:01,577[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:00:01,577[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:00:01,578[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 07:00:01,578[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:00:01,581[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:00:03,156[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:00:03,217[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:00:03,217[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:00:03,687[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:00:03,841[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:00:03,842[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:00:03,842[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:00:03,842[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:00:03,843[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 07:00:03,844[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:00:03,847[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:00:04,629[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:00:04,682[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:00:04,682[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:00:46,462[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:00:46,965[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:00:46,966[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:00:46,966[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:00:46,966[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 06:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:00:46,967[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 07:00:46,967[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:00:46,970[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:00:48,263[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:00:48,344[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:00:48,344[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:02:50,333[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:02:50,352[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5306) last sent a heartbeat 123.74 seconds ago! Restarting it[0m
[[34m2021-03-31 07:02:50,357[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5306[0m
[[34m2021-03-31 07:02:50,571[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5306, status='terminated', exitcode=0, started='06:52:35') (5306) terminated with exit code 0[0m
[[34m2021-03-31 07:02:50,574[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18932[0m
[[34m2021-03-31 07:02:50,586[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 07:02:50,599] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 07:02:51,111[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 06:50:00+00:00: scheduled__2021-03-31T06:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 07:03:47,251[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:08:47,382[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:10:01,812[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:10:01,813[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:10:01,814[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:10:01,814[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:10:01,815[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 07:10:01,815[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:10:01,818[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:10:03,112[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:10:03,172[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:10:03,172[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:10:03,950[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:10:04,096[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:10:04,097[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:10:04,097[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:10:04,097[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:10:04,098[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 07:10:04,098[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:10:04,101[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:10:04,856[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:10:04,964[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:10:04,964[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:10:47,174[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:10:47,654[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:10:47,655[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:10:47,655[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:10:47,655[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 07:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:10:47,657[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 07:10:47,657[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:10:47,660[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:10:48,896[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:10:48,976[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:10:48,977[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:12:52,466[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:12:52,483[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18932) last sent a heartbeat 125.16 seconds ago! Restarting it[0m
[[34m2021-03-31 07:12:52,486[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18932[0m
[[34m2021-03-31 07:12:52,659[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18932, status='terminated', exitcode=0, started='07:02:49') (18932) terminated with exit code 0[0m
[[34m2021-03-31 07:12:52,661[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 32419[0m
[[34m2021-03-31 07:12:52,671[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 07:12:52,684] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 07:12:53,161[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 07:00:00+00:00: scheduled__2021-03-31T07:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 07:13:47,512[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:18:47,547[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:20:02,146[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:20:02,147[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:20:02,147[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:20:02,148[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:20:02,149[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 07:20:02,149[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:20:02,152[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:20:03,532[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:20:03,583[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:20:03,583[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:20:04,340[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:20:04,490[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:20:04,491[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:20:04,491[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:20:04,491[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:20:04,492[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 07:20:04,492[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:20:04,495[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:20:05,080[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:20:05,125[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:20:05,125[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:20:44,984[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:20:45,474[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:20:45,475[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:20:45,476[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:20:45,476[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 07:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:20:45,477[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 07:20:45,477[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:20:45,480[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:20:46,714[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:20:46,829[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:20:46,829[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:22:47,399[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:22:47,416[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=32419) last sent a heartbeat 122.29 seconds ago! Restarting it[0m
[[34m2021-03-31 07:22:47,419[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 32419[0m
[[34m2021-03-31 07:22:47,592[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=32419, status='terminated', exitcode=0, started='07:12:51') (32419) terminated with exit code 0[0m
[[34m2021-03-31 07:22:47,595[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12548[0m
[[34m2021-03-31 07:22:47,604[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 07:22:47,617] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 07:22:48,123[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 07:10:00+00:00: scheduled__2021-03-31T07:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 07:23:47,575[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:28:47,603[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:30:01,138[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:30:01,140[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:30:01,140[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:30:01,140[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:30:01,142[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 07:30:01,142[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:30:01,145[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:30:02,790[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:30:02,932[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:30:02,933[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:30:03,823[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:30:03,995[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:30:03,997[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:30:03,997[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:30:03,997[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:30:03,999[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 07:30:03,999[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:30:04,002[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:30:04,980[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:30:05,101[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:30:05,101[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:30:47,132[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:30:47,605[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:30:47,606[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:30:47,606[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:30:47,606[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 07:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:30:47,607[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 07:30:47,608[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:30:47,611[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:30:48,626[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:30:48,707[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:30:48,707[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:32:49,167[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:32:49,184[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12548) last sent a heartbeat 121.91 seconds ago! Restarting it[0m
[[34m2021-03-31 07:32:49,187[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12548[0m
[[34m2021-03-31 07:32:49,359[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12548, status='terminated', exitcode=0, started='07:22:46') (12548) terminated with exit code 0[0m
[[34m2021-03-31 07:32:49,363[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24792[0m
[[34m2021-03-31 07:32:49,372[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 07:32:49,385] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 07:32:49,883[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 07:20:00+00:00: scheduled__2021-03-31T07:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 07:33:47,735[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:38:47,768[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:40:01,762[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:40:01,764[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:40:01,764[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:40:01,764[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:40:01,765[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 07:40:01,765[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:40:01,768[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:40:03,134[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:40:03,195[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:40:03,195[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:40:03,842[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:40:03,997[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:40:03,998[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:40:03,998[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:40:03,998[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:40:03,999[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 07:40:04,000[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:40:04,002[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:40:04,602[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:40:04,654[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:40:04,655[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:40:46,433[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:40:46,895[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:40:46,896[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:40:46,897[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:40:46,897[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 07:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:40:46,898[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 07:40:46,898[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:40:46,904[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:40:48,121[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:40:48,235[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:40:48,235[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:42:56,059[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:42:56,078[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24792) last sent a heartbeat 129.47 seconds ago! Restarting it[0m
[[34m2021-03-31 07:42:56,084[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24792[0m
[[34m2021-03-31 07:42:56,297[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24792, status='terminated', exitcode=0, started='07:32:48') (24792) terminated with exit code 0[0m
[[34m2021-03-31 07:42:56,301[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6798[0m
[[34m2021-03-31 07:42:56,318[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 07:42:56,345] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 07:42:56,933[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 07:30:00+00:00: scheduled__2021-03-31T07:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 07:43:47,900[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:48:48,030[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:50:01,120[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:50:01,122[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:50:01,122[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:50:01,122[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:50:01,126[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 7, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 07:50:01,127[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:50:01,131[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:50:02,945[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:50:03,030[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:50:03,031[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T07:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:50:03,964[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 07:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:50:04,128[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:50:04,130[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:50:04,131[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:50:04,131[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:50:04,132[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 7, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 07:50:04,133[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:50:04,136[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:50:05,566[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:50:05,637[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:50:05,638[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T07:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:50:49,163[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 07:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:50:49,636[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:50:49,637[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 07:50:49,637[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 07:50:49,637[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 07:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 07:50:49,638[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 7, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 07:50:49,639[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:50:49,642[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 07:50:50,969[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 07:50:51,054[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 07:50:51,055[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T07:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 07:52:53,036[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 07:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 07:52:53,053[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6798) last sent a heartbeat 123.75 seconds ago! Restarting it[0m
[[34m2021-03-31 07:52:53,057[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6798[0m
[[34m2021-03-31 07:52:53,230[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6798, status='terminated', exitcode=0, started='07:42:55') (6798) terminated with exit code 0[0m
[[34m2021-03-31 07:52:53,233[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18895[0m
[[34m2021-03-31 07:52:53,242[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 07:52:53,255] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 07:52:53,748[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 07:40:00+00:00: scheduled__2021-03-31T07:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 07:53:48,161[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 07:58:48,299[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:00:01,632[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:00:01,634[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:00:01,634[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:00:01,634[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:00:01,636[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 08:00:01,636[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:00:01,641[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:00:03,145[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:00:03,199[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:00:03,199[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:00:03,867[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 07:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:00:04,021[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:00:04,022[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:00:04,022[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:00:04,022[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:00:04,023[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 08:00:04,023[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:00:04,027[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:00:04,624[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:00:04,676[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:00:04,676[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:00:43,999[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 07:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:00:44,546[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:00:44,547[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:00:44,547[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:00:44,547[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 07:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:00:44,548[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 08:00:44,548[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:00:44,551[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:00:45,544[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:00:45,623[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:00:45,623[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:03:21,678[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 07:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:03:21,697[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18895) last sent a heartbeat 157.54 seconds ago! Restarting it[0m
[[34m2021-03-31 08:03:21,703[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18895[0m
[[34m2021-03-31 08:03:21,916[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18895, status='terminated', exitcode=0, started='07:52:52') (18895) terminated with exit code 0[0m
[[34m2021-03-31 08:03:21,920[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31571[0m
[[34m2021-03-31 08:03:21,954[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 08:03:21,976] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 08:03:22,581[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 07:50:00+00:00: scheduled__2021-03-31T07:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 08:03:48,434[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:08:48,570[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:10:01,253[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:10:01,254[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:10:01,254[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:10:01,254[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:10:01,256[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 8, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 08:10:01,256[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:10:01,259[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:10:03,054[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:10:03,196[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:10:03,196[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:10:04,016[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 08:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:10:04,161[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:10:04,162[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:10:04,162[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:10:04,163[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:10:04,164[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 8, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 08:10:04,164[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:10:04,167[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:10:04,997[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:10:05,076[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:10:05,076[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:10:47,076[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 08:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:10:47,573[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:10:47,574[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:10:47,574[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:10:47,574[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 08:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:10:47,575[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 8, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 08:10:47,575[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:10:47,579[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:10:48,610[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:10:48,680[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:10:48,680[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:12:50,190[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 08:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:12:50,207[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31571) last sent a heartbeat 122.99 seconds ago! Restarting it[0m
[[34m2021-03-31 08:12:50,211[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31571[0m
[[34m2021-03-31 08:12:50,383[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31571, status='terminated', exitcode=0, started='08:03:21') (31571) terminated with exit code 0[0m
[[34m2021-03-31 08:12:50,386[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 11746[0m
[[34m2021-03-31 08:12:50,399[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 08:12:50,412] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 08:12:50,909[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 08:00:00+00:00: scheduled__2021-03-31T08:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 08:13:48,701[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:18:48,727[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:20:02,131[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:20:02,132[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:20:02,133[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:20:02,133[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:20:02,134[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 8, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 08:20:02,134[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:20:02,137[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:20:03,358[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:20:03,453[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:20:03,453[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T08:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:20:04,163[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 08:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:20:04,321[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:20:04,322[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:20:04,323[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:20:04,323[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:20:04,324[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 8, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 08:20:04,324[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:20:04,327[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:20:04,916[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:20:04,963[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:20:04,963[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T08:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:20:44,102[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 08:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:20:44,643[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:20:44,645[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:20:44,645[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:20:44,645[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 08:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:20:44,646[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 8, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 08:20:44,646[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:20:44,649[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:20:45,920[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:20:46,023[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:20:46,024[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T08:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:22:46,379[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 08:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:22:46,399[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=11746) last sent a heartbeat 122.14 seconds ago! Restarting it[0m
[[34m2021-03-31 08:22:46,404[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 11746[0m
[[34m2021-03-31 08:22:46,618[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=11746, status='terminated', exitcode=0, started='08:12:49') (11746) terminated with exit code 0[0m
[[34m2021-03-31 08:22:46,621[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23885[0m
[[34m2021-03-31 08:22:46,646[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 08:22:46,666] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 08:22:47,233[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 08:10:00+00:00: scheduled__2021-03-31T08:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 08:23:48,877[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:28:49,016[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:30:01,509[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:30:01,510[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:30:01,510[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:30:01,510[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:30:01,511[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 8, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 08:30:01,512[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:30:01,515[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:30:02,823[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:30:02,876[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:30:02,876[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T08:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:30:03,601[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 08:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:30:03,747[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:30:03,748[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:30:03,748[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:30:03,748[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:30:03,749[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 8, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 08:30:03,749[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:30:03,752[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:30:04,389[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:30:04,444[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:30:04,444[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T08:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:30:46,379[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 08:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:30:46,903[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:30:46,905[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:30:46,905[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:30:46,905[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 08:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:30:46,906[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 8, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 08:30:46,907[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:30:46,910[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:30:48,456[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:30:48,543[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:30:48,543[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T08:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:32:54,573[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 08:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:32:54,590[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23885) last sent a heartbeat 128.07 seconds ago! Restarting it[0m
[[34m2021-03-31 08:32:54,594[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23885[0m
[[34m2021-03-31 08:32:54,766[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23885, status='terminated', exitcode=0, started='08:22:45') (23885) terminated with exit code 0[0m
[[34m2021-03-31 08:32:54,770[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 4175[0m
[[34m2021-03-31 08:32:54,779[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 08:32:54,793] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 08:32:55,288[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 08:20:00+00:00: scheduled__2021-03-31T08:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 08:33:49,147[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:38:49,251[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:40:01,606[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:40:01,608[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:40:01,608[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:40:01,608[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:40:01,611[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 8, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 08:40:01,612[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:40:01,615[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:40:02,868[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:40:02,931[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:40:02,931[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T08:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:40:03,621[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 08:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:40:03,672[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:40:03,673[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:40:03,673[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:40:03,673[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:40:03,674[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 8, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 08:40:03,674[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:40:03,677[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:40:04,272[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:40:04,324[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:40:04,324[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T08:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:40:44,216[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 08:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:40:44,762[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:40:44,764[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:40:44,764[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:40:44,764[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 08:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:40:44,765[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 8, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 08:40:44,766[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:40:44,769[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:40:46,352[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:40:46,471[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:40:46,471[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T08:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:42:48,337[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 08:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:42:48,354[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=4175) last sent a heartbeat 123.98 seconds ago! Restarting it[0m
[[34m2021-03-31 08:42:48,357[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 4175[0m
[[34m2021-03-31 08:42:48,529[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=4175, status='terminated', exitcode=0, started='08:32:53') (4175) terminated with exit code 0[0m
[[34m2021-03-31 08:42:48,533[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16315[0m
[[34m2021-03-31 08:42:48,542[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 08:42:48,556] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 08:42:49,051[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 08:30:00+00:00: scheduled__2021-03-31T08:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 08:43:49,387[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:48:49,423[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:50:01,335[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:50:01,337[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:50:01,337[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:50:01,337[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:50:01,338[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 8, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 08:50:01,339[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:50:01,343[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:50:02,626[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:50:02,677[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:50:02,678[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T08:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:50:03,355[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 08:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:50:03,514[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:50:03,515[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:50:03,515[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:50:03,515[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:50:03,516[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 8, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 08:50:03,516[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:50:03,519[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:50:04,148[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:50:04,195[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:50:04,195[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T08:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:50:45,260[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 08:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:50:45,745[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:50:45,746[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 08:50:45,747[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 08:50:45,747[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 08:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 08:50:45,748[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 8, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 08:50:45,748[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:50:45,751[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 08:50:47,033[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 08:50:47,113[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 08:50:47,113[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T08:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 08:52:51,353[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 08:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 08:52:51,370[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16315) last sent a heartbeat 125.97 seconds ago! Restarting it[0m
[[34m2021-03-31 08:52:51,373[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16315[0m
[[34m2021-03-31 08:52:51,546[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16315, status='terminated', exitcode=0, started='08:42:47') (16315) terminated with exit code 0[0m
[[34m2021-03-31 08:52:51,550[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28592[0m
[[34m2021-03-31 08:52:51,559[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 08:52:51,572] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 08:52:52,054[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 08:40:00+00:00: scheduled__2021-03-31T08:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 08:53:49,556[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 08:58:49,687[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:00:01,167[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:00:01,169[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:00:01,169[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:00:01,169[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:00:01,172[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 8, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 09:00:01,172[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:00:01,176[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:00:02,630[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:00:02,759[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:00:02,759[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:00:03,729[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 08:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:00:03,894[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:00:03,895[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:00:03,896[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:00:03,896[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:00:03,897[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 8, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 09:00:03,897[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:00:03,901[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:00:05,003[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:00:05,087[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:00:05,087[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:00:47,842[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 08:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:00:48,332[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:00:48,333[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:00:48,333[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:00:48,333[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 08:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:00:48,334[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 8, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 09:00:48,335[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:00:48,337[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T08:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:00:49,567[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:00:49,652[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:00:49,652[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T08:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:02:52,897[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 08:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:02:52,914[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28592) last sent a heartbeat 124.93 seconds ago! Restarting it[0m
[[34m2021-03-31 09:02:52,917[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28592[0m
[[34m2021-03-31 09:02:53,089[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28592, status='terminated', exitcode=0, started='08:52:50') (28592) terminated with exit code 0[0m
[[34m2021-03-31 09:02:53,093[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8811[0m
[[34m2021-03-31 09:02:53,102[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 09:02:53,115] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 09:02:53,606[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 08:50:00+00:00: scheduled__2021-03-31T08:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 09:03:49,817[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:08:49,853[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:10:01,515[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:10:01,516[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:10:01,516[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:10:01,516[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:10:01,517[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 9, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 09:10:01,517[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:10:01,520[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:10:02,892[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:10:02,968[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:10:02,968[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T09:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:10:03,623[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 09:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:10:03,777[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:10:03,778[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:10:03,778[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:10:03,778[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:10:03,779[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 9, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 09:10:03,779[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:10:03,782[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:10:04,367[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:10:04,415[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:10:04,415[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T09:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:10:48,165[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 09:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:10:48,698[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:10:48,699[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:10:48,699[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:10:48,699[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 09:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:10:48,701[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 9, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 09:10:48,701[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:10:48,704[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:10:50,254[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:10:50,373[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:10:50,373[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T09:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:12:55,448[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 09:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:12:55,466[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8811) last sent a heartbeat 127.14 seconds ago! Restarting it[0m
[[34m2021-03-31 09:12:55,469[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8811[0m
[[34m2021-03-31 09:12:55,642[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8811, status='terminated', exitcode=0, started='09:02:52') (8811) terminated with exit code 0[0m
[[34m2021-03-31 09:12:55,645[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21054[0m
[[34m2021-03-31 09:12:55,654[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 09:12:55,669] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 09:12:56,191[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 09:00:00+00:00: scheduled__2021-03-31T09:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 09:13:49,992[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:18:50,101[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:20:01,508[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:20:01,509[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:20:01,512[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:20:01,513[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:20:01,514[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 9, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 09:20:01,514[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:20:01,518[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:20:03,186[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:20:03,237[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:20:03,237[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T09:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:20:03,849[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 09:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:20:03,997[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:20:03,998[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:20:03,998[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:20:03,998[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:20:03,999[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 9, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 09:20:03,999[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:20:04,002[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:20:04,886[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:20:04,961[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:20:04,962[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T09:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:20:46,613[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 09:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:20:47,099[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:20:47,100[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:20:47,100[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:20:47,101[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 09:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:20:47,102[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 9, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 09:20:47,102[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:20:47,105[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:20:48,095[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:20:48,178[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:20:48,178[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T09:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:22:50,527[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 09:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:22:50,544[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21054) last sent a heartbeat 123.77 seconds ago! Restarting it[0m
[[34m2021-03-31 09:22:50,548[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21054[0m
[[34m2021-03-31 09:22:50,720[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21054, status='terminated', exitcode=0, started='09:12:54') (21054) terminated with exit code 0[0m
[[34m2021-03-31 09:22:50,724[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 796[0m
[[34m2021-03-31 09:22:50,733[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 09:22:50,745] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 09:22:51,238[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 09:10:00+00:00: scheduled__2021-03-31T09:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 09:23:50,129[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:28:50,202[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:30:01,593[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:30:01,594[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:30:01,594[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:30:01,594[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:30:01,595[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 9, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 09:30:01,595[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:30:01,598[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:30:02,939[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:30:03,015[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:30:03,015[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T09:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:30:03,704[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 09:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:30:03,855[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:30:03,856[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:30:03,856[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:30:03,856[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:30:03,857[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 9, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 09:30:03,857[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:30:03,860[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:30:04,452[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:30:04,510[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:30:04,510[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T09:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:30:43,421[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 09:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:30:43,935[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:30:43,936[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:30:43,936[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:30:43,936[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 09:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:30:43,938[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 9, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 09:30:43,938[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:30:43,941[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:30:45,305[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:30:45,407[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:30:45,407[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T09:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:33:38,686[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 09:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:33:38,703[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=796) last sent a heartbeat 175.13 seconds ago! Restarting it[0m
[[34m2021-03-31 09:33:38,708[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 796[0m
[[34m2021-03-31 09:33:38,921[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=796, status='terminated', exitcode=0, started='09:22:49') (796) terminated with exit code 0[0m
[[34m2021-03-31 09:33:38,925[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14060[0m
[[34m2021-03-31 09:33:38,934[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 09:33:38,947] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 09:33:39,453[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 09:20:00+00:00: scheduled__2021-03-31T09:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 09:33:50,313[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:38:50,321[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:40:01,160[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:40:01,162[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:40:01,162[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:40:01,162[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:40:01,163[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 9, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 09:40:01,163[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:40:01,165[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:40:02,828[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:40:02,882[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:40:02,882[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T09:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:40:03,494[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 09:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:40:03,647[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:40:03,648[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:40:03,648[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:40:03,648[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:40:03,650[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 9, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 09:40:03,650[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:40:03,653[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:40:04,242[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:40:04,295[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:40:04,295[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T09:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:40:46,014[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 09:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:40:46,485[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:40:46,486[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:40:46,486[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:40:46,486[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 09:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:40:46,487[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 9, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 09:40:46,487[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:40:46,491[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:40:47,628[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:40:47,706[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:40:47,706[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T09:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:43:01,669[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 09:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:43:01,690[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14060) last sent a heartbeat 135.53 seconds ago! Restarting it[0m
[[34m2021-03-31 09:43:01,695[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14060[0m
[[34m2021-03-31 09:43:01,909[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14060, status='terminated', exitcode=0, started='09:33:38') (14060) terminated with exit code 0[0m
[[34m2021-03-31 09:43:01,913[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25745[0m
[[34m2021-03-31 09:43:01,928[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 09:43:01,952] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 09:43:02,552[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 09:30:00+00:00: scheduled__2021-03-31T09:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 09:43:50,358[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:48:50,489[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:50:01,570[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:50:01,571[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:50:01,572[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:50:01,572[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:50:01,573[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 9, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 09:50:01,573[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:50:01,576[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:50:03,066[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:50:03,126[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:50:03,126[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T09:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:50:03,909[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 09:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:50:04,076[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:50:04,077[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:50:04,078[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:50:04,078[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:50:04,079[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 9, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 09:50:04,080[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:50:04,083[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:50:04,747[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:50:04,802[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:50:04,802[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T09:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:50:44,808[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 09:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:50:45,376[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:50:45,378[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 09:50:45,378[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 09:50:45,378[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 09:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 09:50:45,379[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 9, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 09:50:45,380[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:50:45,387[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 09:50:46,853[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 09:50:46,963[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 09:50:46,963[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T09:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 09:52:55,788[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 09:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 09:52:55,804[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25745) last sent a heartbeat 130.84 seconds ago! Restarting it[0m
[[34m2021-03-31 09:52:55,808[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25745[0m
[[34m2021-03-31 09:52:55,980[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25745, status='terminated', exitcode=0, started='09:43:01') (25745) terminated with exit code 0[0m
[[34m2021-03-31 09:52:55,984[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5841[0m
[[34m2021-03-31 09:52:55,993[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 09:52:56,007] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 09:52:56,509[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 09:40:00+00:00: scheduled__2021-03-31T09:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 09:53:50,621[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 09:58:50,654[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:00:01,666[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:00:01,667[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:00:01,667[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:00:01,667[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:00:01,668[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 9, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 10:00:01,668[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:00:01,671[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:00:02,865[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:00:02,922[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:00:02,922[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T09:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:00:03,652[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 09:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:00:03,811[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:00:03,812[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:00:03,812[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:00:03,812[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:00:03,814[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 9, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 10:00:03,814[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:00:03,817[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:00:04,475[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:00:04,522[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:00:04,522[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T09:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:00:46,098[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 09:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:00:46,571[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:00:46,572[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:00:46,572[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:00:46,572[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 09:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:00:46,573[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 9, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 10:00:46,573[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:00:46,577[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T09:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:00:47,833[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:00:47,918[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:00:47,918[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T09:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:02:51,477[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 09:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:02:51,494[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5841) last sent a heartbeat 125.25 seconds ago! Restarting it[0m
[[34m2021-03-31 10:02:51,498[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5841[0m
[[34m2021-03-31 10:02:51,670[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5841, status='terminated', exitcode=0, started='09:52:55') (5841) terminated with exit code 0[0m
[[34m2021-03-31 10:02:51,674[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18141[0m
[[34m2021-03-31 10:02:51,683[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 10:02:51,696] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 10:02:52,188[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 09:50:00+00:00: scheduled__2021-03-31T09:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 10:03:50,683[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:08:50,719[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:10:01,536[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:10:01,538[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:10:01,538[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:10:01,538[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:10:01,540[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 10, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 10:10:01,540[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:10:01,544[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:10:02,967[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:10:03,060[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:10:03,061[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T10:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:10:04,014[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 10:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:10:04,184[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:10:04,186[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:10:04,186[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:10:04,186[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:10:04,187[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 10, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 10:10:04,188[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:10:04,191[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:10:05,181[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:10:05,256[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:10:05,256[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T10:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:10:48,265[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 10:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:10:48,778[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:10:48,779[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:10:48,779[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:10:48,779[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 10:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:10:48,781[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 10, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 10:10:48,781[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:10:48,785[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:10:49,892[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:10:49,976[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:10:49,977[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T10:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:13:41,180[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 10:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:13:41,199[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18141) last sent a heartbeat 172.79 seconds ago! Restarting it[0m
[[34m2021-03-31 10:13:41,205[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18141[0m
[[34m2021-03-31 10:13:41,418[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18141, status='terminated', exitcode=0, started='10:02:50') (18141) terminated with exit code 0[0m
[[34m2021-03-31 10:13:41,423[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31188[0m
[[34m2021-03-31 10:13:41,450[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 10:13:41,464] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 10:13:42,045[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 10:00:00+00:00: scheduled__2021-03-31T10:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 10:13:50,852[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:18:50,990[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:20:01,469[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:20:01,470[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:20:01,470[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:20:01,470[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:20:01,471[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 10, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 10:20:01,471[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:20:01,474[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:20:02,852[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:20:02,909[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:20:02,909[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T10:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:20:03,536[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 10:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:20:03,681[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:20:03,682[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:20:03,682[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:20:03,682[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:20:03,683[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 10, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 10:20:03,683[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:20:03,686[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:20:04,273[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:20:04,320[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:20:04,320[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T10:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:20:43,551[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 10:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:20:44,035[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:20:44,036[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:20:44,036[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:20:44,036[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 10:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:20:44,037[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 10, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 10:20:44,037[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:20:44,041[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:20:45,086[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:20:45,170[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:20:45,170[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T10:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:22:54,151[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 10:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:22:54,172[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31188) last sent a heartbeat 130.47 seconds ago! Restarting it[0m
[[34m2021-03-31 10:22:54,177[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31188[0m
[[34m2021-03-31 10:22:54,394[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31188, status='terminated', exitcode=0, started='10:13:40') (31188) terminated with exit code 0[0m
[[34m2021-03-31 10:22:54,398[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10681[0m
[[34m2021-03-31 10:22:54,411[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 10:22:54,426] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 10:22:55,017[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 10:10:00+00:00: scheduled__2021-03-31T10:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 10:23:51,137[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:28:51,270[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:30:01,657[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:30:01,658[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:30:01,659[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:30:01,659[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:30:01,660[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 10, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 10:30:01,660[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:30:01,667[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:30:03,464[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:30:03,590[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:30:03,590[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T10:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:30:04,377[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 10:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:30:04,539[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:30:04,540[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:30:04,541[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:30:04,541[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:30:04,542[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 10, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 10:30:04,542[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:30:04,545[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:30:05,496[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:30:05,579[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:30:05,579[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T10:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:30:48,368[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 10:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:30:48,862[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:30:48,863[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:30:48,863[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:30:48,863[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 10:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:30:48,864[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 10, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 10:30:48,864[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:30:48,867[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:30:50,180[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:30:50,317[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:30:50,317[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T10:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:33:30,061[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 10:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:33:30,079[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10681) last sent a heartbeat 161.57 seconds ago! Restarting it[0m
[[34m2021-03-31 10:33:30,083[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10681[0m
[[34m2021-03-31 10:33:30,255[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10681, status='terminated', exitcode=0, started='10:22:53') (10681) terminated with exit code 0[0m
[[34m2021-03-31 10:33:30,259[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23439[0m
[[34m2021-03-31 10:33:30,268[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 10:33:30,284] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 10:33:30,835[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 10:20:00+00:00: scheduled__2021-03-31T10:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 10:33:51,403[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:38:51,534[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:40:01,681[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:40:01,685[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:40:01,685[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:40:01,685[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:40:01,687[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 10, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 10:40:01,687[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:40:01,691[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:40:02,959[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:40:03,010[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:40:03,010[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T10:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:40:03,687[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 10:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:40:03,876[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:40:03,877[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:40:03,878[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:40:03,878[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:40:03,879[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 10, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 10:40:03,879[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:40:03,888[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:40:04,562[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:40:04,627[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:40:04,627[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T10:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:40:45,224[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 10:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:40:45,740[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:40:45,741[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:40:45,741[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:40:45,742[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 10:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:40:45,743[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 10, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 10:40:45,743[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:40:45,746[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:40:46,915[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:40:47,009[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:40:47,009[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T10:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:42:52,021[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 10:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:42:52,038[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23439) last sent a heartbeat 126.66 seconds ago! Restarting it[0m
[[34m2021-03-31 10:42:52,041[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23439[0m
[[34m2021-03-31 10:42:52,253[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23439, status='terminated', exitcode=0, started='10:33:29') (23439) terminated with exit code 0[0m
[[34m2021-03-31 10:42:52,258[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2966[0m
[[34m2021-03-31 10:42:52,281[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 10:42:52,302] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 10:42:52,806[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 10:30:00+00:00: scheduled__2021-03-31T10:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 10:43:51,568[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:48:51,702[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:50:02,098[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:50:02,099[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:50:02,099[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:50:02,099[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:50:02,100[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 10, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 10:50:02,100[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:50:02,103[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:50:03,302[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:50:03,352[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:50:03,352[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T10:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:50:04,191[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 10:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:50:04,350[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:50:04,351[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:50:04,351[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:50:04,351[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:50:04,352[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 10, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 10:50:04,352[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:50:04,355[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:50:04,943[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:50:04,990[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:50:04,991[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T10:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:50:46,456[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 10:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:50:46,997[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:50:46,998[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 10:50:46,998[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 10:50:46,998[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 10:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 10:50:46,999[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 10, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 10:50:46,999[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:50:47,002[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 10:50:48,364[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 10:50:48,444[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 10:50:48,444[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T10:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 10:52:55,141[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 10:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 10:52:55,158[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2966) last sent a heartbeat 128.54 seconds ago! Restarting it[0m
[[34m2021-03-31 10:52:55,162[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2966[0m
[[34m2021-03-31 10:52:55,334[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2966, status='terminated', exitcode=0, started='10:42:51') (2966) terminated with exit code 0[0m
[[34m2021-03-31 10:52:55,338[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15470[0m
[[34m2021-03-31 10:52:55,347[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 10:52:55,360] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 10:52:55,847[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 10:40:00+00:00: scheduled__2021-03-31T10:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 10:53:51,831[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 10:58:51,961[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:00:01,162[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:00:01,163[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:00:01,163[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:00:01,163[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:00:01,164[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 10, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 11:00:01,164[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:00:01,167[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:00:02,462[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:00:02,515[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:00:02,516[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T10:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:00:03,159[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 10:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:00:03,305[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:00:03,306[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:00:03,306[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:00:03,306[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:00:03,307[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 10, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 11:00:03,308[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:00:03,310[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:00:03,894[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:00:03,940[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:00:03,940[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T10:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:00:45,541[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 10:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:00:46,073[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:00:46,074[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:00:46,074[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:00:46,074[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 10:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:00:46,076[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 10, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 11:00:46,076[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:00:46,079[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T10:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:00:47,643[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:00:47,749[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:00:47,749[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T10:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:02:51,500[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 10:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:02:51,519[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15470) last sent a heartbeat 125.83 seconds ago! Restarting it[0m
[[34m2021-03-31 11:02:51,524[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15470[0m
[[34m2021-03-31 11:02:51,698[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15470, status='terminated', exitcode=0, started='10:52:54') (15470) terminated with exit code 0[0m
[[34m2021-03-31 11:02:51,701[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 27803[0m
[[34m2021-03-31 11:02:51,711[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 11:02:51,723] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 11:02:52,216[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 10:50:00+00:00: scheduled__2021-03-31T10:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 11:03:51,989[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:08:52,136[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:10:01,486[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:10:01,487[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:10:01,487[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:10:01,487[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:10:01,489[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 11, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 11:10:01,489[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:10:01,492[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:10:02,809[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:10:02,889[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:10:02,889[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T11:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:10:03,541[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 11:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:10:03,691[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:10:03,692[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:10:03,692[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:10:03,692[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:10:03,694[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 11, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 11:10:03,694[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:10:03,697[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:10:04,421[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:10:04,469[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:10:04,469[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T11:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:10:44,426[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 11:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:10:44,592[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:10:44,593[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:10:44,593[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:10:44,593[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 11:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:10:44,594[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 11, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 11:10:44,594[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:10:44,597[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:10:45,607[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:10:45,688[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:10:45,688[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T11:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:12:46,074[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 11:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:12:46,090[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=27803) last sent a heartbeat 121.52 seconds ago! Restarting it[0m
[[34m2021-03-31 11:12:46,093[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 27803[0m
[[34m2021-03-31 11:12:46,266[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=27803, status='terminated', exitcode=0, started='11:02:50') (27803) terminated with exit code 0[0m
[[34m2021-03-31 11:12:46,269[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 7661[0m
[[34m2021-03-31 11:12:46,278[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 11:12:46,292] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 11:12:46,516[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 11:00:00+00:00: scheduled__2021-03-31T11:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 11:13:52,270[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:18:52,299[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:20:02,128[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:20:02,129[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:20:02,129[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:20:02,129[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:20:02,130[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 11, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 11:20:02,130[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:20:02,134[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:20:03,235[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:20:03,290[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:20:03,290[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T11:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:20:03,912[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 11:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:20:04,057[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:20:04,058[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:20:04,058[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:20:04,058[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:20:04,060[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 11, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 11:20:04,060[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:20:04,062[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:20:04,652[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:20:04,700[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:20:04,700[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T11:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:20:42,123[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 11:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:20:42,327[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:20:42,329[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:20:42,329[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:20:42,329[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 11:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:20:42,331[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 11, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 11:20:42,331[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:20:42,336[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:20:43,855[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:20:43,966[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:20:43,966[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T11:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:22:46,063[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 11:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:22:46,081[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=7661) last sent a heartbeat 123.79 seconds ago! Restarting it[0m
[[34m2021-03-31 11:22:46,085[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 7661[0m
[[34m2021-03-31 11:22:46,258[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=7661, status='terminated', exitcode=0, started='11:12:45') (7661) terminated with exit code 0[0m
[[34m2021-03-31 11:22:46,261[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19124[0m
[[34m2021-03-31 11:22:46,271[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 11:22:46,288] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 11:22:46,466[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 11:10:00+00:00: scheduled__2021-03-31T11:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 11:23:52,403[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:28:52,431[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:30:02,039[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:30:02,040[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:30:02,040[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:30:02,040[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:30:02,042[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 11, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 11:30:02,043[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:30:02,050[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:30:03,348[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:30:03,401[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:30:03,401[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T11:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:30:03,857[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 11:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:30:04,003[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:30:04,004[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:30:04,004[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:30:04,004[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:30:04,005[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 11, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 11:30:04,005[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:30:04,009[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:30:04,592[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:30:04,639[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:30:04,639[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T11:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:30:42,864[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 11:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:30:43,030[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:30:43,031[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:30:43,031[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:30:43,031[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 11:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:30:43,032[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 11, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 11:30:43,032[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:30:43,035[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:30:43,998[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:30:44,075[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:30:44,075[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T11:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:32:44,033[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 11:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:32:44,049[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19124) last sent a heartbeat 121.04 seconds ago! Restarting it[0m
[[34m2021-03-31 11:32:44,053[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19124[0m
[[34m2021-03-31 11:32:44,225[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19124, status='terminated', exitcode=0, started='11:22:45') (19124) terminated with exit code 0[0m
[[34m2021-03-31 11:32:44,227[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30667[0m
[[34m2021-03-31 11:32:44,237[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 11:32:44,251] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 11:32:44,424[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 11:20:00+00:00: scheduled__2021-03-31T11:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 11:33:52,459[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:38:52,486[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:40:01,639[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:40:01,640[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:40:01,641[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:40:01,641[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:40:01,649[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 11, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 11:40:01,649[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:40:01,652[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:40:03,276[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:40:03,358[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:40:03,358[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T11:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:40:03,977[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 11:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:40:04,142[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:40:04,143[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:40:04,143[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:40:04,143[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:40:04,145[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 11, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 11:40:04,145[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:40:04,148[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:40:05,157[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:40:05,234[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:40:05,234[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T11:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:40:46,267[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 11:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:40:46,741[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:40:46,742[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:40:46,743[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:40:46,743[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 11:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:40:46,744[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 11, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 11:40:46,744[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:40:46,746[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:40:47,732[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:40:47,824[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:40:47,824[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T11:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:42:48,939[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 11:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:42:48,955[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30667) last sent a heartbeat 122.55 seconds ago! Restarting it[0m
[[34m2021-03-31 11:42:48,958[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30667[0m
[[34m2021-03-31 11:42:49,130[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30667, status='terminated', exitcode=0, started='11:32:43') (30667) terminated with exit code 0[0m
[[34m2021-03-31 11:42:49,133[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10004[0m
[[34m2021-03-31 11:42:49,143[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 11:42:49,155] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 11:42:49,310[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 11:30:00+00:00: scheduled__2021-03-31T11:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 11:43:52,617[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:48:52,643[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:50:01,197[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:50:01,199[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:50:01,199[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:50:01,199[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:50:01,201[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 11, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 11:50:01,201[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:50:01,206[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:50:02,565[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:50:02,620[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:50:02,620[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T11:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:50:03,112[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 11:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:50:03,257[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:50:03,258[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:50:03,258[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:50:03,258[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:50:03,259[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 11, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 11:50:03,259[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:50:03,262[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:50:03,854[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:50:03,903[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:50:03,903[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T11:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:50:41,271[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 11:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:50:41,438[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:50:41,439[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 11:50:41,439[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 11:50:41,439[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 11:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 11:50:41,440[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 11, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 11:50:41,440[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:50:41,443[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 11:50:42,395[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 11:50:42,473[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 11:50:42,473[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T11:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 11:52:44,431[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 11:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 11:52:44,446[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10004) last sent a heartbeat 123.03 seconds ago! Restarting it[0m
[[34m2021-03-31 11:52:44,451[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10004[0m
[[34m2021-03-31 11:52:44,583[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10004, status='terminated', exitcode=0, started='11:42:48') (10004) terminated with exit code 0[0m
[[34m2021-03-31 11:52:44,587[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21355[0m
[[34m2021-03-31 11:52:44,609[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 11:52:44,630] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 11:52:44,796[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 11:40:00+00:00: scheduled__2021-03-31T11:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 11:53:52,676[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 11:58:52,710[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:00:01,511[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:00:01,512[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:00:01,512[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:00:01,513[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:00:01,514[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 11, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 12:00:01,514[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:00:01,518[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:00:02,958[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:00:03,013[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:00:03,013[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T11:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:00:03,468[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 11:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:00:03,618[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:00:03,619[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:00:03,619[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:00:03,619[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:00:03,620[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 11, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 12:00:03,620[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:00:03,623[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:00:04,459[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:00:04,540[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:00:04,540[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T11:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:00:44,762[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 11:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:00:44,927[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:00:44,928[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:00:44,928[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:00:44,928[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 11:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:00:44,929[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 11, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 12:00:44,930[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:00:44,932[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T11:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:00:46,201[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:00:46,284[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:00:46,284[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T11:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:03:36,669[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 11:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:03:36,687[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21355) last sent a heartbeat 171.78 seconds ago! Restarting it[0m
[[34m2021-03-31 12:03:36,691[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21355[0m
[[34m2021-03-31 12:03:36,827[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21355, status='terminated', exitcode=0, started='11:52:43') (21355) terminated with exit code 0[0m
[[34m2021-03-31 12:03:36,831[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 1183[0m
[[34m2021-03-31 12:03:36,857[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 12:03:36,868] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 12:03:37,042[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 11:50:00+00:00: scheduled__2021-03-31T11:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 12:03:52,841[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:08:52,868[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:10:01,842[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:10:01,843[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:10:01,843[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:10:01,843[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:10:01,844[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 12, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 12:10:01,844[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:10:01,847[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:10:02,843[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:10:02,894[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:10:02,894[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T12:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:10:03,359[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:10:03,505[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:10:03,506[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:10:03,506[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:10:03,506[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:10:03,507[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 12, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 12:10:03,507[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:10:03,510[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:10:04,093[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:10:04,143[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:10:04,143[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T12:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:10:40,750[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:10:40,956[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:10:40,957[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:10:40,957[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:10:40,957[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 12:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:10:40,959[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 12, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 12:10:40,959[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:10:40,962[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:10:42,514[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:10:42,628[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:10:42,628[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T12:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:12:41,478[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 12:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:12:41,494[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=1183) last sent a heartbeat 120.58 seconds ago! Restarting it[0m
[[34m2021-03-31 12:12:41,497[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 1183[0m
[[34m2021-03-31 12:12:41,590[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=1183, status='terminated', exitcode=0, started='12:03:36') (1183) terminated with exit code 0[0m
[[34m2021-03-31 12:12:41,592[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12188[0m
[[34m2021-03-31 12:12:41,603[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 12:12:41,615] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 12:12:41,765[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 12:00:00+00:00: scheduled__2021-03-31T12:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 12:13:52,998[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:18:53,137[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:20:01,526[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:20:01,527[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:20:01,527[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:20:01,527[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:20:01,528[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 12, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 12:20:01,528[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:20:01,531[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:20:02,736[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:20:02,790[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:20:02,790[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T12:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:20:03,254[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 12:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:20:03,408[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:20:03,409[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:20:03,409[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:20:03,409[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:20:03,410[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 12, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 12:20:03,411[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:20:03,414[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:20:04,004[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:20:04,053[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:20:04,053[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T12:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:20:43,756[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 12:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:20:43,921[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:20:43,922[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:20:43,922[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:20:43,923[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 12:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:20:43,924[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 12, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 12:20:43,924[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:20:43,927[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:20:45,148[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:20:45,254[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:20:45,254[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T12:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:22:49,597[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 12:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:22:49,613[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12188) last sent a heartbeat 125.72 seconds ago! Restarting it[0m
[[34m2021-03-31 12:22:49,616[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12188[0m
[[34m2021-03-31 12:22:49,789[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12188, status='terminated', exitcode=0, started='12:12:40') (12188) terminated with exit code 0[0m
[[34m2021-03-31 12:22:49,791[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23739[0m
[[34m2021-03-31 12:22:49,801[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 12:22:49,814] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 12:22:49,988[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 12:10:00+00:00: scheduled__2021-03-31T12:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 12:23:53,270[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:28:53,339[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:30:01,812[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:30:01,813[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:30:01,813[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:30:01,813[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:30:01,814[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 12, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 12:30:01,814[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:30:01,817[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:30:02,993[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:30:03,043[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:30:03,043[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T12:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:30:03,561[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 12:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:30:03,707[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:30:03,708[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:30:03,708[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:30:03,708[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:30:03,709[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 12, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 12:30:03,709[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:30:03,712[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:30:04,656[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:30:04,734[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:30:04,734[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T12:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:30:44,252[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 12:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:30:44,423[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:30:44,424[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:30:44,424[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:30:44,424[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 12:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:30:44,425[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 12, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 12:30:44,425[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:30:44,428[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:30:45,399[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:30:45,478[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:30:45,478[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T12:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:32:48,013[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 12:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:32:48,029[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23739) last sent a heartbeat 123.63 seconds ago! Restarting it[0m
[[34m2021-03-31 12:32:48,032[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23739[0m
[[34m2021-03-31 12:32:48,124[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23739, status='terminated', exitcode=0, started='12:22:48') (23739) terminated with exit code 0[0m
[[34m2021-03-31 12:32:48,127[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2946[0m
[[34m2021-03-31 12:32:48,136[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 12:32:48,147] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 12:32:48,298[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 12:20:00+00:00: scheduled__2021-03-31T12:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 12:33:53,366[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:38:53,497[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:40:01,300[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:40:01,301[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:40:01,301[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:40:01,301[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:40:01,302[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 12, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 12:40:01,302[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:40:01,306[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:40:02,369[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:40:02,424[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:40:02,424[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T12:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:40:02,804[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 12:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:40:02,951[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:40:02,952[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:40:02,952[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:40:02,952[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:40:02,953[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 12, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 12:40:02,953[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:40:02,956[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:40:03,543[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:40:03,589[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:40:03,589[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T12:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:40:41,295[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 12:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:40:41,491[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:40:41,492[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:40:41,492[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:40:41,492[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 12:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:40:41,494[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 12, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 12:40:41,494[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:40:41,497[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:40:42,688[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:40:42,766[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:40:42,766[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T12:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:43:01,837[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 12:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:43:01,856[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2946) last sent a heartbeat 140.40 seconds ago! Restarting it[0m
[[34m2021-03-31 12:43:01,861[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2946[0m
[[34m2021-03-31 12:43:01,996[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2946, status='terminated', exitcode=0, started='12:32:47') (2946) terminated with exit code 0[0m
[[34m2021-03-31 12:43:01,999[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14715[0m
[[34m2021-03-31 12:43:02,009[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 12:43:02,021] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 12:43:02,171[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 12:30:00+00:00: scheduled__2021-03-31T12:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 12:43:53,634[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:48:53,662[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:50:01,314[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:50:01,316[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:50:01,316[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:50:01,316[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:50:01,318[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 12, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 12:50:01,318[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:50:01,321[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:50:02,896[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:50:02,982[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:50:02,983[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T12:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:50:03,639[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 12:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:50:03,818[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:50:03,820[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:50:03,820[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:50:03,820[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:50:03,822[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 12, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 12:50:03,822[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:50:03,825[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:50:04,508[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:50:04,555[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:50:04,556[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T12:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:50:45,375[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 12:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:50:45,540[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:50:45,542[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 12:50:45,542[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 12:50:45,542[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 12:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 12:50:45,543[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 12, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 12:50:45,543[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:50:45,547[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 12:50:46,682[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 12:50:46,762[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 12:50:46,762[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T12:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 12:52:52,196[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 12:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 12:52:52,211[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14715) last sent a heartbeat 126.70 seconds ago! Restarting it[0m
[[34m2021-03-31 12:52:52,215[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14715[0m
[[34m2021-03-31 12:52:52,387[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14715, status='terminated', exitcode=0, started='12:43:01') (14715) terminated with exit code 0[0m
[[34m2021-03-31 12:52:52,389[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26033[0m
[[34m2021-03-31 12:52:52,399[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 12:52:52,412] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 12:52:52,589[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 12:40:00+00:00: scheduled__2021-03-31T12:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 12:53:53,796[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 12:58:53,835[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:00:02,046[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:00:02,047[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:00:02,047[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:00:02,047[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:00:02,048[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 12, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 13:00:02,048[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:00:02,051[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:00:03,291[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:00:03,342[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:00:03,342[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T12:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:00:03,809[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 12:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:00:03,968[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:00:03,969[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:00:03,969[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:00:03,969[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:00:03,970[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 12, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 13:00:03,970[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:00:03,973[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:00:04,586[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:00:04,658[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:00:04,658[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T12:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:00:41,156[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 12:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:00:41,321[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:00:41,322[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:00:41,322[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:00:41,322[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 12:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:00:41,323[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 12, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 13:00:41,324[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:00:41,327[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T12:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:00:42,911[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:00:43,021[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:00:43,021[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T12:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:02:52,223[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 12:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:02:52,238[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26033) last sent a heartbeat 130.94 seconds ago! Restarting it[0m
[[34m2021-03-31 13:02:52,242[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26033[0m
[[34m2021-03-31 13:02:52,334[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26033, status='terminated', exitcode=0, started='12:52:51') (26033) terminated with exit code 0[0m
[[34m2021-03-31 13:02:52,336[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5445[0m
[[34m2021-03-31 13:02:52,347[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 13:02:52,359] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 13:02:52,508[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 12:50:00+00:00: scheduled__2021-03-31T12:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 13:03:53,857[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:08:53,999[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:10:01,436[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:10:01,437[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:10:01,437[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:10:01,437[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:10:01,438[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 13, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 13:10:01,438[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:10:01,443[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:10:02,534[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:10:02,590[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:10:02,590[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T13:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:10:03,053[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 13:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:10:03,211[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:10:03,212[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:10:03,212[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:10:03,212[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:10:03,213[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 13, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 13:10:03,213[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:10:03,216[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:10:03,931[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:10:03,977[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:10:03,977[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T13:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:10:49,349[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 13:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:10:49,514[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:10:49,515[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:10:49,515[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:10:49,515[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 13:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:10:49,516[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 13, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 13:10:49,516[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:10:49,520[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:10:50,698[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:10:50,779[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:10:50,779[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T13:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:12:58,383[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 13:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:12:58,401[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5445) last sent a heartbeat 128.91 seconds ago! Restarting it[0m
[[34m2021-03-31 13:12:58,406[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5445[0m
[[34m2021-03-31 13:12:58,541[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5445, status='terminated', exitcode=0, started='13:02:51') (5445) terminated with exit code 0[0m
[[34m2021-03-31 13:12:58,545[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17002[0m
[[34m2021-03-31 13:12:58,571[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 13:12:58,587] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 13:12:58,766[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 13:00:00+00:00: scheduled__2021-03-31T13:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 13:13:54,132[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:18:54,161[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:20:01,421[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:20:01,422[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:20:01,423[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:20:01,423[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:20:01,424[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 13, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 13:20:01,424[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:20:01,428[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:20:02,534[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:20:02,586[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:20:02,586[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T13:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:20:03,046[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 13:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:20:03,204[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:20:03,205[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:20:03,205[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:20:03,205[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:20:03,206[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 13, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 13:20:03,206[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:20:03,210[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:20:03,797[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:20:03,845[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:20:03,845[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T13:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:20:41,466[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 13:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:20:41,655[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:20:41,656[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:20:41,657[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:20:41,657[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 13:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:20:41,658[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 13, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 13:20:41,659[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:20:41,662[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:20:43,302[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:20:43,423[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:20:43,423[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T13:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:22:45,363[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 13:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:22:45,379[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17002) last sent a heartbeat 123.76 seconds ago! Restarting it[0m
[[34m2021-03-31 13:22:45,382[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17002[0m
[[34m2021-03-31 13:22:45,474[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17002, status='terminated', exitcode=0, started='13:12:57') (17002) terminated with exit code 0[0m
[[34m2021-03-31 13:22:45,477[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28345[0m
[[34m2021-03-31 13:22:45,488[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 13:22:45,500] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 13:22:45,649[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 13:10:00+00:00: scheduled__2021-03-31T13:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 13:23:54,291[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:28:54,422[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:30:01,301[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:30:01,303[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:30:01,303[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:30:01,303[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:30:01,304[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 13, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 13:30:01,304[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:30:01,308[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:30:02,719[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:30:02,773[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:30:02,773[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T13:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:30:03,242[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 13:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:30:03,387[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:30:03,388[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:30:03,389[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:30:03,389[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:30:03,390[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 13, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 13:30:03,390[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:30:03,393[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:30:03,975[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:30:04,022[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:30:04,022[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T13:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:30:42,788[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 13:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:30:42,954[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:30:42,955[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:30:42,955[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:30:42,955[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 13:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:30:42,956[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 13, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 13:30:42,956[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:30:42,959[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:30:44,459[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:30:44,541[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:30:44,541[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T13:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:32:53,802[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 13:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:32:53,819[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28345) last sent a heartbeat 130.89 seconds ago! Restarting it[0m
[[34m2021-03-31 13:32:53,824[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28345[0m
[[34m2021-03-31 13:32:54,038[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28345, status='terminated', exitcode=0, started='13:22:44') (28345) terminated with exit code 0[0m
[[34m2021-03-31 13:32:54,041[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 7739[0m
[[34m2021-03-31 13:32:54,071[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 13:32:54,088] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 13:32:54,293[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 13:20:00+00:00: scheduled__2021-03-31T13:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 13:33:54,444[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:38:54,473[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:40:01,948[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:40:01,950[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:40:01,950[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:40:01,950[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:40:01,951[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 13, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 13:40:01,952[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:40:01,956[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:40:03,569[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:40:03,674[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:40:03,674[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T13:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:40:04,403[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 13:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:40:04,586[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:40:04,588[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:40:04,588[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:40:04,588[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:40:04,590[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 13, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 13:40:04,590[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:40:04,593[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:40:05,606[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:40:05,685[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:40:05,685[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T13:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:40:46,201[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 13:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:40:46,368[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:40:46,369[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:40:46,369[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:40:46,369[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:40:46,370[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 13, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 13:40:46,370[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:40:46,373[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:40:47,604[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:40:47,688[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:40:47,688[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T13:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:41:38,632[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 13:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:41:38,648[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=7739) last sent a heartbeat 52.30 seconds ago! Restarting it[0m
[[34m2021-03-31 13:41:38,651[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 7739[0m
[[34m2021-03-31 13:41:38,743[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=7739, status='terminated', exitcode=0, started='13:32:53') (7739) terminated with exit code 0[0m
[[34m2021-03-31 13:41:38,746[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15896[0m
[[34m2021-03-31 13:41:38,754[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 13:41:38,765] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 13:43:54,616[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:46:39,036[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:46:39,037[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:46:39,037[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:46:39,037[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 13:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:46:39,039[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 13, 30, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-03-31 13:46:39,039[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:46:39,042[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:46:40,037[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:46:40,120[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:46:40,120[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T13:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:48:48,741[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 13:30:00+00:00 exited with status success for try_number 2[0m
[[34m2021-03-31 13:48:48,757[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15896) last sent a heartbeat 129.76 seconds ago! Restarting it[0m
[[34m2021-03-31 13:48:48,760[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15896[0m
[[34m2021-03-31 13:48:48,852[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15896, status='terminated', exitcode=0, started='13:41:37') (15896) terminated with exit code 0[0m
[[34m2021-03-31 13:48:48,855[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24208[0m
[[34m2021-03-31 13:48:48,863[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 13:48:48,873] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 13:48:49,024[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 13:30:00+00:00: scheduled__2021-03-31T13:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 13:48:54,748[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:50:01,777[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:50:01,778[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:50:01,778[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:50:01,779[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:50:01,779[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 13, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 13:50:01,780[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:50:01,783[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:50:02,811[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:50:02,880[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:50:02,880[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T13:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:50:03,237[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 13:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:50:03,382[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:50:03,383[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:50:03,383[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:50:03,383[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:50:03,384[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 13, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 13:50:03,385[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:50:03,387[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:50:03,971[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:50:04,016[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:50:04,016[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T13:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:50:39,356[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 13:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:50:39,522[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:50:39,522[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 13:50:39,523[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 13:50:39,523[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 13:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 13:50:39,524[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 13, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 13:50:39,524[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:50:39,527[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 13:50:40,497[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 13:50:40,583[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 13:50:40,583[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T13:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 13:52:37,736[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 13:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 13:52:37,752[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24208) last sent a heartbeat 118.25 seconds ago! Restarting it[0m
[[34m2021-03-31 13:52:37,755[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24208[0m
[[34m2021-03-31 13:52:37,847[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24208, status='terminated', exitcode=0, started='13:48:48') (24208) terminated with exit code 0[0m
[[34m2021-03-31 13:52:37,850[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30571[0m
[[34m2021-03-31 13:52:37,862[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 13:52:37,874] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 13:52:38,073[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 13:40:00+00:00: scheduled__2021-03-31T13:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 13:53:54,783[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 13:58:54,922[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:00:01,382[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:00:01,383[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:00:01,383[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:00:01,383[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:00:01,384[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 13, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 14:00:01,385[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:00:01,388[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:00:02,809[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:00:02,892[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:00:02,892[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T13:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:00:03,355[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 13:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:00:03,511[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:00:03,512[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:00:03,512[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:00:03,512[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:00:03,513[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 13, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 14:00:03,513[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:00:03,517[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:00:04,102[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:00:04,152[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:00:04,152[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T13:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:00:44,730[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 13:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:00:44,896[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:00:44,897[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:00:44,897[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:00:44,897[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 13:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:00:44,898[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 13, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 14:00:44,899[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:00:44,902[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T13:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:00:46,055[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:00:46,162[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:00:46,162[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T13:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:02:54,046[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 13:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:02:54,062[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30571) last sent a heartbeat 129.19 seconds ago! Restarting it[0m
[[34m2021-03-31 14:02:54,065[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30571[0m
[[34m2021-03-31 14:02:54,157[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30571, status='terminated', exitcode=0, started='13:52:37') (30571) terminated with exit code 0[0m
[[34m2021-03-31 14:02:54,160[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10051[0m
[[34m2021-03-31 14:02:54,169[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 14:02:54,179] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 14:02:54,368[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 13:50:00+00:00: scheduled__2021-03-31T13:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 14:03:54,951[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:08:55,069[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:10:01,596[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:10:01,598[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:10:01,599[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:10:01,599[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:10:01,600[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 14, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 14:10:01,600[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:10:01,604[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:10:02,717[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:10:02,774[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:10:02,774[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T14:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:10:03,194[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 14:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:10:03,243[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:10:03,245[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:10:03,245[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:10:03,245[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:10:03,247[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 14, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 14:10:03,247[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:10:03,250[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:10:03,973[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:10:04,021[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:10:04,021[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T14:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:10:40,293[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 14:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:10:40,484[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:10:40,486[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:10:40,486[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:10:40,486[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 14:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:10:40,488[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 14, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 14:10:40,488[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:10:40,491[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:10:42,125[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:10:42,237[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:10:42,237[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T14:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:12:47,293[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 14:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:12:47,311[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10051) last sent a heartbeat 126.86 seconds ago! Restarting it[0m
[[34m2021-03-31 14:12:47,315[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10051[0m
[[34m2021-03-31 14:12:47,489[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10051, status='terminated', exitcode=0, started='14:02:53') (10051) terminated with exit code 0[0m
[[34m2021-03-31 14:12:47,493[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21507[0m
[[34m2021-03-31 14:12:47,506[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 14:12:47,525] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 14:12:47,725[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 14:00:00+00:00: scheduled__2021-03-31T14:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 14:13:55,208[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:18:55,241[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:20:02,140[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:20:02,141[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:20:02,141[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:20:02,141[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:20:02,142[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 14, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 14:20:02,142[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:20:02,145[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:20:03,101[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:20:03,158[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:20:03,158[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T14:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:20:03,531[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 14:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:20:03,687[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:20:03,688[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:20:03,688[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:20:03,688[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:20:03,689[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 14, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 14:20:03,690[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:20:03,692[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:20:04,292[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:20:04,343[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:20:04,343[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T14:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:20:44,429[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 14:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:20:44,595[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:20:44,596[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:20:44,596[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:20:44,596[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 14:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:20:44,597[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 14, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 14:20:44,597[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:20:44,600[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:20:45,644[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:20:45,724[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:20:45,724[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T14:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:23:01,613[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 14:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:23:01,631[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21507) last sent a heartbeat 137.06 seconds ago! Restarting it[0m
[[34m2021-03-31 14:23:01,636[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21507[0m
[[34m2021-03-31 14:23:01,810[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21507, status='terminated', exitcode=0, started='14:12:46') (21507) terminated with exit code 0[0m
[[34m2021-03-31 14:23:01,813[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 697[0m
[[34m2021-03-31 14:23:01,823[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 14:23:01,838] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 14:23:02,012[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 14:10:00+00:00: scheduled__2021-03-31T14:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 14:23:55,373[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:28:55,503[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:30:01,377[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:30:01,378[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:30:01,378[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:30:01,378[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:30:01,379[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 14, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 14:30:01,379[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:30:01,382[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:30:02,811[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:30:02,868[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:30:02,868[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T14:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:30:03,244[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 14:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:30:03,399[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:30:03,400[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:30:03,401[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:30:03,401[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:30:03,402[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 14, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 14:30:03,402[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:30:03,405[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:30:04,124[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:30:04,174[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:30:04,174[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T14:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:30:42,289[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 14:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:30:42,482[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:30:42,483[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:30:42,484[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:30:42,484[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 14:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:30:42,485[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 14, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 14:30:42,485[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:30:42,489[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:30:44,059[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:30:44,170[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:30:44,170[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T14:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:32:44,646[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 14:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:32:44,661[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=697) last sent a heartbeat 122.22 seconds ago! Restarting it[0m
[[34m2021-03-31 14:32:44,665[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 697[0m
[[34m2021-03-31 14:32:44,757[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=697, status='terminated', exitcode=0, started='14:23:01') (697) terminated with exit code 0[0m
[[34m2021-03-31 14:32:44,759[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12193[0m
[[34m2021-03-31 14:32:44,768[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 14:32:44,778] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 14:32:44,929[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 14:20:00+00:00: scheduled__2021-03-31T14:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 14:33:55,634[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:38:55,670[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:40:02,007[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:40:02,008[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:40:02,008[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:40:02,008[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:40:02,009[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 14, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 14:40:02,009[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:40:02,012[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:40:03,020[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:40:03,071[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:40:03,071[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T14:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:40:03,430[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 14:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:40:03,575[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:40:03,576[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:40:03,576[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:40:03,576[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:40:03,577[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 14, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 14:40:03,577[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:40:03,580[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:40:04,165[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:40:04,212[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:40:04,212[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T14:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:40:42,891[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 14:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:40:43,082[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:40:43,083[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:40:43,084[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:40:43,084[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 14:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:40:43,085[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 14, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 14:40:43,085[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:40:43,089[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:40:44,234[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:40:44,315[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:40:44,315[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T14:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:42:48,665[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 14:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:42:48,681[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12193) last sent a heartbeat 125.63 seconds ago! Restarting it[0m
[[34m2021-03-31 14:42:48,684[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12193[0m
[[34m2021-03-31 14:42:48,776[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12193, status='terminated', exitcode=0, started='14:32:43') (12193) terminated with exit code 0[0m
[[34m2021-03-31 14:42:48,779[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23799[0m
[[34m2021-03-31 14:42:48,787[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 14:42:48,797] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 14:42:48,968[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 14:30:00+00:00: scheduled__2021-03-31T14:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 14:43:55,704[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:48:55,731[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:50:01,676[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:50:01,678[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:50:01,678[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:50:01,678[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:50:01,679[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 14, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 14:50:01,680[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:50:01,683[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:50:03,437[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:50:03,515[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:50:03,515[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T14:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:50:04,040[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 14:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:50:04,214[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:50:04,216[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:50:04,216[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:50:04,216[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:50:04,218[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 14, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 14:50:04,218[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:50:04,222[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:50:05,208[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:50:05,279[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:50:05,279[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T14:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:50:46,614[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 14:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:50:46,817[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:50:46,819[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 14:50:46,819[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 14:50:46,819[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 14:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 14:50:46,821[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 14, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 14:50:46,821[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:50:46,824[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 14:50:47,874[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 14:50:47,948[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 14:50:47,948[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T14:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 14:52:50,656[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 14:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 14:52:50,671[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23799) last sent a heartbeat 123.89 seconds ago! Restarting it[0m
[[34m2021-03-31 14:52:50,675[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23799[0m
[[34m2021-03-31 14:52:50,767[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23799, status='terminated', exitcode=0, started='14:42:47') (23799) terminated with exit code 0[0m
[[34m2021-03-31 14:52:50,770[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3105[0m
[[34m2021-03-31 14:52:50,778[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 14:52:50,788] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 14:52:50,968[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 14:40:00+00:00: scheduled__2021-03-31T14:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 14:53:55,863[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 14:58:55,891[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:00:01,882[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:00:01,884[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:00:01,884[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:00:01,884[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:00:01,886[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 14, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 15:00:01,886[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:00:01,889[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:00:03,128[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:00:03,205[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:00:03,205[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T14:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:00:03,683[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 14:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:00:03,829[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:00:03,830[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:00:03,830[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:00:03,830[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:00:03,831[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 14, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 15:00:03,831[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:00:03,835[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:00:04,421[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:00:04,467[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:00:04,467[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T14:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:00:40,729[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 14:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:00:40,917[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:00:40,919[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:00:40,919[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:00:40,919[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 14:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:00:40,920[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 14, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 15:00:40,921[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:00:40,924[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T14:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:00:41,781[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:00:41,859[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:00:41,859[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T14:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:02:51,160[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 14:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:02:51,178[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3105) last sent a heartbeat 130.30 seconds ago! Restarting it[0m
[[34m2021-03-31 15:02:51,182[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3105[0m
[[34m2021-03-31 15:02:51,285[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3105, status='terminated', exitcode=0, started='14:52:49') (3105) terminated with exit code 0[0m
[[34m2021-03-31 15:02:51,289[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14772[0m
[[34m2021-03-31 15:02:51,314[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 15:02:51,325] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 15:02:51,505[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 14:50:00+00:00: scheduled__2021-03-31T14:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 15:03:56,024[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:08:56,060[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:10:02,005[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:10:02,006[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:10:02,006[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:10:02,007[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:10:02,008[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 15, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 15:10:02,008[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:10:02,011[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:10:02,930[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:10:02,985[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:10:02,985[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T15:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:10:03,345[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 15:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:10:03,491[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:10:03,492[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:10:03,492[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:10:03,492[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:10:03,493[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 15, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 15:10:03,493[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:10:03,496[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:10:04,240[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:10:04,290[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:10:04,290[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T15:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:10:43,910[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 15:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:10:44,078[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:10:44,079[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:10:44,079[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:10:44,079[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 15:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:10:44,080[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 15, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 15:10:44,080[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:10:44,084[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:10:45,130[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:10:45,212[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:10:45,213[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T15:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:12:47,958[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 15:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:12:47,974[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14772) last sent a heartbeat 123.92 seconds ago! Restarting it[0m
[[34m2021-03-31 15:12:47,978[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14772[0m
[[34m2021-03-31 15:12:48,070[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14772, status='terminated', exitcode=0, started='15:02:50') (14772) terminated with exit code 0[0m
[[34m2021-03-31 15:12:48,072[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26258[0m
[[34m2021-03-31 15:12:48,081[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 15:12:48,091] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 15:12:48,262[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 15:00:00+00:00: scheduled__2021-03-31T15:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 15:13:56,089[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:18:56,128[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:20:01,042[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:20:01,043[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:20:01,043[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:20:01,043[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:20:01,044[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 15, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 15:20:01,044[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:20:01,048[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:20:02,182[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:20:02,239[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:20:02,239[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T15:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:20:02,604[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 15:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:20:02,750[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:20:02,751[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:20:02,751[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:20:02,751[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:20:02,752[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 15, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 15:20:02,753[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:20:02,756[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:20:03,349[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:20:03,398[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:20:03,398[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T15:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:20:42,573[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 15:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:20:42,771[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:20:42,772[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:20:42,773[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:20:42,773[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 15:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:20:42,774[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 15, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 15:20:42,774[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:20:42,778[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:20:44,257[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:20:44,364[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:20:44,364[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T15:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:22:45,790[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 15:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:22:45,809[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26258) last sent a heartbeat 123.08 seconds ago! Restarting it[0m
[[34m2021-03-31 15:22:45,814[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26258[0m
[[34m2021-03-31 15:22:45,958[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26258, status='terminated', exitcode=0, started='15:12:47') (26258) terminated with exit code 0[0m
[[34m2021-03-31 15:22:45,962[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5470[0m
[[34m2021-03-31 15:22:45,983[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 15:22:46,001] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 15:22:46,175[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 15:10:00+00:00: scheduled__2021-03-31T15:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 15:23:56,156[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:28:56,286[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:30:01,992[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:30:01,993[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:30:01,993[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:30:01,993[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:30:01,994[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 15, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 15:30:01,994[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:30:01,997[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:30:03,024[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:30:03,077[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:30:03,077[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T15:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:30:03,441[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 15:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:30:03,587[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:30:03,588[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:30:03,588[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:30:03,588[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:30:03,589[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 15, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 15:30:03,589[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:30:03,592[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:30:04,186[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:30:04,234[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:30:04,234[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T15:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:30:44,405[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 15:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:30:44,582[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:30:44,584[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:30:44,584[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:30:44,584[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 15:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:30:44,585[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 15, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 15:30:44,585[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:30:44,588[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:30:45,678[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:30:45,761[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:30:45,761[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T15:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:32:52,421[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 15:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:32:52,436[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5470) last sent a heartbeat 127.89 seconds ago! Restarting it[0m
[[34m2021-03-31 15:32:52,440[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5470[0m
[[34m2021-03-31 15:32:52,532[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5470, status='terminated', exitcode=0, started='15:22:45') (5470) terminated with exit code 0[0m
[[34m2021-03-31 15:32:52,535[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17040[0m
[[34m2021-03-31 15:32:52,543[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 15:32:52,553] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 15:32:52,705[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 15:20:00+00:00: scheduled__2021-03-31T15:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 15:33:56,312[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:38:56,339[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:40:01,624[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:40:01,625[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:40:01,625[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:40:01,626[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:40:01,627[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 15, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 15:40:01,627[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:40:01,630[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:40:02,676[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:40:02,728[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:40:02,728[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T15:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:40:03,079[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 15:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:40:03,238[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:40:03,239[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:40:03,239[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:40:03,240[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:40:03,241[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 15, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 15:40:03,241[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:40:03,244[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:40:03,971[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:40:04,017[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:40:04,018[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T15:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:40:43,906[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 15:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:40:44,109[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:40:44,114[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:40:44,114[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:40:44,114[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 15:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:40:44,116[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 15, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 15:40:44,116[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:40:44,119[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:40:45,105[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:40:45,185[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:40:45,185[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T15:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:42:46,118[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 15:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:42:46,133[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17040) last sent a heartbeat 122.07 seconds ago! Restarting it[0m
[[34m2021-03-31 15:42:46,137[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17040[0m
[[34m2021-03-31 15:42:46,229[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17040, status='terminated', exitcode=0, started='15:32:51') (17040) terminated with exit code 0[0m
[[34m2021-03-31 15:42:46,232[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28464[0m
[[34m2021-03-31 15:42:46,240[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 15:42:46,250] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 15:42:46,402[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 15:30:00+00:00: scheduled__2021-03-31T15:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 15:43:56,366[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:48:56,497[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:50:01,123[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:50:01,124[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:50:01,124[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:50:01,124[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:50:01,125[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 15, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 15:50:01,125[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:50:01,129[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:50:02,172[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:50:02,224[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:50:02,224[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T15:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:50:02,591[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 15:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:50:02,746[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:50:02,747[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:50:02,747[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:50:02,747[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:50:02,748[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 15, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 15:50:02,748[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:50:02,751[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:50:03,340[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:50:03,388[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:50:03,388[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T15:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:50:40,989[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 15:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:50:41,153[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:50:41,154[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 15:50:41,154[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 15:50:41,155[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 15:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 15:50:41,156[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 15, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 15:50:41,156[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:50:41,159[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 15:50:42,188[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 15:50:42,265[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 15:50:42,265[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T15:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 15:52:50,553[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 15:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 15:52:50,571[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28464) last sent a heartbeat 129.44 seconds ago! Restarting it[0m
[[34m2021-03-31 15:52:50,575[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28464[0m
[[34m2021-03-31 15:52:50,721[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28464, status='terminated', exitcode=0, started='15:42:45') (28464) terminated with exit code 0[0m
[[34m2021-03-31 15:52:50,725[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 7756[0m
[[34m2021-03-31 15:52:50,750[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 15:52:50,781] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 15:52:50,976[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 15:40:00+00:00: scheduled__2021-03-31T15:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 15:53:56,631[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 15:58:56,659[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:00:01,402[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:00:01,404[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:00:01,404[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:00:01,404[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:00:01,406[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 15, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 16:00:01,406[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:00:01,410[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:00:03,177[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:00:03,265[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:00:03,266[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T15:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:00:03,797[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 15:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:00:03,959[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:00:03,960[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:00:03,960[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:00:03,960[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:00:03,962[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 15, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 16:00:03,962[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:00:03,965[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:00:04,996[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:00:05,101[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:00:05,106[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T15:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:00:45,018[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 15:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:00:45,206[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:00:45,207[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:00:45,208[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:00:45,208[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 15:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:00:45,209[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 15, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 16:00:45,209[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:00:45,213[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T15:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:00:46,206[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:00:46,285[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:00:46,285[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T15:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:02:54,442[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 15:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:02:54,458[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=7756) last sent a heartbeat 129.29 seconds ago! Restarting it[0m
[[34m2021-03-31 16:02:54,461[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 7756[0m
[[34m2021-03-31 16:02:54,555[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=7756, status='terminated', exitcode=0, started='15:52:49') (7756) terminated with exit code 0[0m
[[34m2021-03-31 16:02:54,558[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19298[0m
[[34m2021-03-31 16:02:54,567[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 16:02:54,577] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 16:02:54,754[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 15:50:00+00:00: scheduled__2021-03-31T15:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 16:03:56,790[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:08:56,921[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:10:01,788[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:10:01,789[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:10:01,789[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:10:01,790[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:10:01,791[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 16, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 16:10:01,791[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:10:01,796[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:10:02,974[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:10:03,027[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:10:03,027[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T16:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:10:03,415[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 16:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:10:03,465[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:10:03,466[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:10:03,466[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:10:03,466[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:10:03,467[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 16, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 16:10:03,468[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:10:03,470[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:10:04,156[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:10:04,244[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:10:04,244[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T16:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:10:40,375[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 16:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:10:40,540[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:10:40,541[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:10:40,541[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:10:40,541[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 16:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:10:40,542[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 16, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 16:10:40,542[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:10:40,545[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:10:41,377[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:10:41,455[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:10:41,456[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T16:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:12:43,375[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 16:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:12:43,391[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19298) last sent a heartbeat 122.88 seconds ago! Restarting it[0m
[[34m2021-03-31 16:12:43,394[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19298[0m
[[34m2021-03-31 16:12:43,486[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19298, status='terminated', exitcode=0, started='16:02:53') (19298) terminated with exit code 0[0m
[[34m2021-03-31 16:12:43,489[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30633[0m
[[34m2021-03-31 16:12:43,497[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 16:12:43,507] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 16:12:43,660[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 16:00:00+00:00: scheduled__2021-03-31T16:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 16:13:57,052[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:18:57,091[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:20:01,796[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:20:01,797[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:20:01,797[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:20:01,797[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:20:01,798[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 16, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 16:20:01,798[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:20:01,801[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:20:02,902[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:20:02,985[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:20:02,985[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T16:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:20:03,481[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 16:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:20:03,644[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:20:03,645[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:20:03,645[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:20:03,645[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:20:03,646[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 16, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 16:20:03,647[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:20:03,650[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:20:04,245[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:20:04,297[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:20:04,297[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T16:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:20:46,325[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 16:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:20:46,888[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:20:46,890[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:20:46,890[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:20:46,890[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 16:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:20:46,892[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 16, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 16:20:46,892[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:20:46,895[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:20:48,077[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:20:48,165[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:20:48,165[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T16:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:23:01,071[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 16:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:23:01,090[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30633) last sent a heartbeat 134.58 seconds ago! Restarting it[0m
[[34m2021-03-31 16:23:01,095[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30633[0m
[[34m2021-03-31 16:23:01,310[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30633, status='terminated', exitcode=0, started='16:12:42') (30633) terminated with exit code 0[0m
[[34m2021-03-31 16:23:01,314[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 11797[0m
[[34m2021-03-31 16:23:01,341[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 16:23:01,362] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 16:23:01,966[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 16:10:00+00:00: scheduled__2021-03-31T16:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 16:23:57,177[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:28:57,206[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:30:02,048[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:30:02,050[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:30:02,050[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:30:02,050[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:30:02,052[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 16, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 16:30:02,052[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:30:02,057[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:30:03,184[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:30:03,234[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:30:03,234[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T16:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:30:03,913[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 16:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:30:04,059[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:30:04,060[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:30:04,060[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:30:04,060[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:30:04,061[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 16, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 16:30:04,061[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:30:04,064[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:30:04,808[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:30:04,878[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:30:04,878[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T16:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:30:44,189[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 16:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:30:44,736[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:30:44,737[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:30:44,738[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:30:44,738[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 16:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:30:44,739[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 16, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 16:30:44,739[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:30:44,742[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:30:45,863[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:30:45,988[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:30:45,988[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T16:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:32:50,816[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 16:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:32:50,835[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=11797) last sent a heartbeat 126.49 seconds ago! Restarting it[0m
[[34m2021-03-31 16:32:50,838[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 11797[0m
[[34m2021-03-31 16:32:51,011[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=11797, status='terminated', exitcode=0, started='16:23:00') (11797) terminated with exit code 0[0m
[[34m2021-03-31 16:32:51,014[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23917[0m
[[34m2021-03-31 16:32:51,023[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 16:32:51,036] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 16:32:51,624[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 16:20:00+00:00: scheduled__2021-03-31T16:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 16:33:57,243[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:38:57,280[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:40:01,827[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:40:01,829[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:40:01,829[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:40:01,829[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:40:01,831[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 16, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 16:40:01,831[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:40:01,834[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:40:03,062[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:40:03,115[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:40:03,115[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T16:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:40:03,827[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 16:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:40:03,972[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:40:03,973[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:40:03,973[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:40:03,973[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:40:03,974[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 16, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 16:40:03,975[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:40:03,978[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:40:04,603[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:40:04,649[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:40:04,649[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T16:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:40:47,164[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 16:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:40:47,707[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:40:47,708[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:40:47,708[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:40:47,708[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 16:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:40:47,709[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 16, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 16:40:47,709[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:40:47,712[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:40:48,973[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:40:49,084[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:40:49,084[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T16:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:42:57,532[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 16:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:42:57,550[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23917) last sent a heartbeat 130.23 seconds ago! Restarting it[0m
[[34m2021-03-31 16:42:57,556[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23917[0m
[[34m2021-03-31 16:42:57,770[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23917, status='terminated', exitcode=0, started='16:32:50') (23917) terminated with exit code 0[0m
[[34m2021-03-31 16:42:57,775[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 4256[0m
[[34m2021-03-31 16:42:57,800[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 16:42:57,820] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 16:42:58,398[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 16:30:00+00:00: scheduled__2021-03-31T16:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 16:43:57,308[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:48:57,446[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:50:01,685[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:50:01,686[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:50:01,686[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:50:01,686[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:50:01,687[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 16, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 16:50:01,687[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:50:01,690[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:50:02,877[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:50:02,933[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:50:02,933[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T16:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:50:03,556[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 16:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:50:03,714[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:50:03,715[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:50:03,715[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:50:03,715[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:50:03,716[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 16, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 16:50:03,716[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:50:03,719[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:50:04,309[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:50:04,384[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:50:04,384[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T16:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:50:43,077[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 16:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:50:43,643[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:50:43,646[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 16:50:43,646[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 16:50:43,647[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 16:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 16:50:43,648[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 16, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 16:50:43,648[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:50:43,657[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 16:50:45,028[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 16:50:45,151[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 16:50:45,151[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T16:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 16:52:50,633[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 16:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 16:52:50,650[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=4256) last sent a heartbeat 127.42 seconds ago! Restarting it[0m
[[34m2021-03-31 16:52:50,654[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 4256[0m
[[34m2021-03-31 16:52:50,826[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=4256, status='terminated', exitcode=0, started='16:42:56') (4256) terminated with exit code 0[0m
[[34m2021-03-31 16:52:50,829[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17246[0m
[[34m2021-03-31 16:52:50,839[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 16:52:50,851] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 16:52:51,344[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 16:40:00+00:00: scheduled__2021-03-31T16:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 16:53:57,474[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 16:58:57,502[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:00:01,607[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:00:01,609[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:00:01,609[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:00:01,609[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:00:01,611[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 16, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 17:00:01,611[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:00:01,614[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:00:02,923[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:00:02,979[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:00:02,980[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T16:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:00:03,635[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 16:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:00:03,792[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:00:03,793[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:00:03,794[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:00:03,794[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:00:03,795[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 16, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 17:00:03,795[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:00:03,798[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:00:04,393[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:00:04,445[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:00:04,445[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T16:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:00:45,929[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 16:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:00:46,414[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:00:46,415[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:00:46,415[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:00:46,415[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 16:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:00:46,416[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 16, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 17:00:46,416[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:00:46,419[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T16:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:00:47,703[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:00:47,793[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:00:47,793[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T16:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:03:29,643[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 16:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:03:29,660[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17246) last sent a heartbeat 163.59 seconds ago! Restarting it[0m
[[34m2021-03-31 17:03:29,664[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17246[0m
[[34m2021-03-31 17:03:29,836[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17246, status='terminated', exitcode=0, started='16:52:50') (17246) terminated with exit code 0[0m
[[34m2021-03-31 17:03:29,840[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30102[0m
[[34m2021-03-31 17:03:29,849[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 17:03:29,861] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 17:03:30,381[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 16:50:00+00:00: scheduled__2021-03-31T16:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 17:03:57,557[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:08:57,691[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:10:01,215[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:10:01,216[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:10:01,216[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:10:01,216[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:10:01,217[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 17, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 17:10:01,217[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:10:01,221[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:10:02,918[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:10:03,032[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:10:03,033[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T17:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:10:03,955[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 17:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:10:04,144[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:10:04,145[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:10:04,146[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:10:04,146[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:10:04,147[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 17, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 17:10:04,147[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:10:04,151[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:10:05,141[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:10:05,221[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:10:05,221[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T17:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:10:48,986[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 17:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:10:49,524[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:10:49,526[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:10:49,526[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:10:49,526[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 17:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:10:49,527[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 17, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 17:10:49,528[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:10:49,532[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:10:50,849[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:10:50,932[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:10:50,933[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T17:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:12:57,427[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 17:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:12:57,447[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30102) last sent a heartbeat 128.32 seconds ago! Restarting it[0m
[[34m2021-03-31 17:12:57,452[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30102[0m
[[34m2021-03-31 17:12:57,666[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30102, status='terminated', exitcode=0, started='17:03:29') (30102) terminated with exit code 0[0m
[[34m2021-03-31 17:12:57,671[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 9929[0m
[[34m2021-03-31 17:12:57,700[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 17:12:57,712] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 17:12:58,241[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 17:00:00+00:00: scheduled__2021-03-31T17:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 17:13:57,718[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:18:57,746[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:20:01,198[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:20:01,199[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:20:01,200[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:20:01,200[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:20:01,201[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 17, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 17:20:01,202[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:20:01,207[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:20:02,396[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:20:02,452[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:20:02,453[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T17:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:20:03,187[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 17:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:20:03,344[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:20:03,345[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:20:03,345[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:20:03,345[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:20:03,346[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 17, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 17:20:03,346[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:20:03,349[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:20:04,052[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:20:04,121[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:20:04,121[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T17:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:20:43,247[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 17:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:20:43,742[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:20:43,743[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:20:43,743[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:20:43,743[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 17:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:20:43,744[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 17, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 17:20:43,744[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:20:43,747[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:20:44,834[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:20:44,909[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:20:44,909[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T17:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:22:54,556[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 17:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:22:54,573[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=9929) last sent a heartbeat 131.18 seconds ago! Restarting it[0m
[[34m2021-03-31 17:22:54,576[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 9929[0m
[[34m2021-03-31 17:22:54,748[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=9929, status='terminated', exitcode=0, started='17:12:56') (9929) terminated with exit code 0[0m
[[34m2021-03-31 17:22:54,752[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 22262[0m
[[34m2021-03-31 17:22:54,761[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 17:22:54,774] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 17:22:55,277[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 17:10:00+00:00: scheduled__2021-03-31T17:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 17:23:57,773[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:28:57,812[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:30:01,537[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:30:01,539[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:30:01,539[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:30:01,539[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:30:01,541[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 17, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 17:30:01,541[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:30:01,545[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:30:03,258[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:30:03,309[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:30:03,309[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T17:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:30:03,975[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 17:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:30:04,129[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:30:04,130[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:30:04,130[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:30:04,130[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:30:04,131[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 17, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 17:30:04,131[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:30:04,134[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:30:05,052[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:30:05,130[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:30:05,131[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T17:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:30:46,443[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 17:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:30:46,930[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:30:46,931[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:30:46,931[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:30:46,931[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 17:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:30:46,932[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 17, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 17:30:46,932[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:30:46,935[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:30:48,187[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:30:48,268[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:30:48,268[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T17:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:32:59,595[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 17:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:32:59,613[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=22262) last sent a heartbeat 133.03 seconds ago! Restarting it[0m
[[34m2021-03-31 17:32:59,616[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 22262[0m
[[34m2021-03-31 17:32:59,789[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=22262, status='terminated', exitcode=0, started='17:22:53') (22262) terminated with exit code 0[0m
[[34m2021-03-31 17:32:59,792[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2318[0m
[[34m2021-03-31 17:32:59,802[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 17:32:59,815] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 17:33:00,310[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 17:20:00+00:00: scheduled__2021-03-31T17:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 17:33:57,953[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:38:58,090[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:40:01,609[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:40:01,611[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:40:01,611[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:40:01,611[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:40:01,612[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 17, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 17:40:01,612[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:40:01,615[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:40:02,930[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:40:02,983[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:40:02,983[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T17:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:40:03,690[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 17:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:40:03,849[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:40:03,850[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:40:03,850[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:40:03,850[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:40:03,851[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 17, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 17:40:03,851[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:40:03,855[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:40:04,453[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:40:04,505[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:40:04,505[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T17:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:40:46,583[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 17:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:40:47,185[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:40:47,187[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:40:47,187[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:40:47,187[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 17:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:40:47,189[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 17, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 17:40:47,189[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:40:47,196[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:40:48,636[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:40:48,779[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:40:48,780[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T17:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:43:09,742[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 17:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:43:09,759[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2318) last sent a heartbeat 143.02 seconds ago! Restarting it[0m
[[34m2021-03-31 17:43:09,763[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2318[0m
[[34m2021-03-31 17:43:09,935[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2318, status='terminated', exitcode=0, started='17:32:58') (2318) terminated with exit code 0[0m
[[34m2021-03-31 17:43:09,939[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15118[0m
[[34m2021-03-31 17:43:09,949[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 17:43:09,962] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 17:43:10,465[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 17:30:00+00:00: scheduled__2021-03-31T17:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 17:43:58,222[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:48:58,256[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:50:01,626[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:50:01,627[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:50:01,627[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:50:01,627[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:50:01,628[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 17, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 17:50:01,628[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:50:01,632[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:50:02,981[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:50:03,087[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:50:03,087[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T17:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:50:03,813[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 17:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:50:03,968[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:50:03,969[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:50:03,969[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:50:03,969[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:50:03,970[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 17, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 17:50:03,971[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:50:03,974[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:50:04,570[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:50:04,621[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:50:04,621[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T17:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:50:46,429[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 17:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:50:46,941[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:50:46,942[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 17:50:46,943[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 17:50:46,943[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 17:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 17:50:46,944[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 17, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 17:50:46,944[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:50:46,947[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 17:50:48,250[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 17:50:48,342[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 17:50:48,342[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T17:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 17:52:55,934[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 17:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 17:52:55,951[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15118) last sent a heartbeat 129.38 seconds ago! Restarting it[0m
[[34m2021-03-31 17:52:55,954[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15118[0m
[[34m2021-03-31 17:52:56,127[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15118, status='terminated', exitcode=0, started='17:43:09') (15118) terminated with exit code 0[0m
[[34m2021-03-31 17:52:56,131[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 27270[0m
[[34m2021-03-31 17:52:56,140[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 17:52:56,153] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 17:52:56,655[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 17:40:00+00:00: scheduled__2021-03-31T17:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 17:53:58,389[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 17:58:58,520[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:00:02,114[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:00:02,115[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:00:02,115[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:00:02,115[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:00:02,116[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 17, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 18:00:02,117[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:00:02,119[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:00:03,325[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:00:03,380[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:00:03,380[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T17:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:00:04,095[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 17:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:00:04,254[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:00:04,255[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:00:04,255[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:00:04,255[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:00:04,257[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 17, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 18:00:04,257[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:00:04,260[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:00:05,098[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:00:05,152[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:00:05,152[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T17:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:00:45,495[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 17:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:00:46,078[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:00:46,079[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:00:46,079[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:00:46,079[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 17:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:00:46,080[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 17, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 18:00:46,080[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:00:46,083[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T17:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:00:47,858[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:00:47,978[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:00:47,978[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T17:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:02:58,840[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 17:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:02:58,859[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=27270) last sent a heartbeat 133.19 seconds ago! Restarting it[0m
[[34m2021-03-31 18:02:58,864[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 27270[0m
[[34m2021-03-31 18:02:59,077[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=27270, status='terminated', exitcode=0, started='17:52:55') (27270) terminated with exit code 0[0m
[[34m2021-03-31 18:02:59,082[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8278[0m
[[34m2021-03-31 18:02:59,092[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 18:02:59,106] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 18:02:59,631[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 17:50:00+00:00: scheduled__2021-03-31T17:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 18:03:58,548[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:08:58,680[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:10:02,093[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:10:02,094[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:10:02,094[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:10:02,094[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:10:02,095[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 18, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 18:10:02,096[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:10:02,099[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:10:03,368[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:10:03,425[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:10:03,426[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T18:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:10:04,126[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:10:04,289[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:10:04,290[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:10:04,290[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:10:04,290[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:10:04,291[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 18, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 18:10:04,291[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:10:04,294[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:10:04,894[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:10:04,945[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:10:04,945[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T18:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:10:47,498[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:10:47,986[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:10:47,987[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:10:47,987[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:10:47,987[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 18:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:10:47,988[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 18, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 18:10:47,988[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:10:47,991[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:10:49,274[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:10:49,360[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:10:49,360[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T18:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:12:58,771[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 18:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:12:58,789[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8278) last sent a heartbeat 131.15 seconds ago! Restarting it[0m
[[34m2021-03-31 18:12:58,794[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8278[0m
[[34m2021-03-31 18:12:59,008[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8278, status='terminated', exitcode=0, started='18:02:58') (8278) terminated with exit code 0[0m
[[34m2021-03-31 18:12:59,013[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 20638[0m
[[34m2021-03-31 18:12:59,037[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 18:12:59,059] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 18:12:59,688[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 18:00:00+00:00: scheduled__2021-03-31T18:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 18:13:58,717[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:18:58,779[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:20:01,129[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:20:01,130[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:20:01,130[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:20:01,130[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:20:01,131[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 18, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 18:20:01,131[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:20:01,134[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:20:02,516[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:20:02,598[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:20:02,599[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T18:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:20:03,262[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 18:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:20:03,420[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:20:03,421[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:20:03,421[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:20:03,422[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:20:03,423[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 18, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 18:20:03,423[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:20:03,426[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:20:04,379[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:20:04,457[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:20:04,458[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T18:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:20:48,664[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 18:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:20:49,197[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:20:49,198[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:20:49,198[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:20:49,198[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 18:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:20:49,199[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 18, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 18:20:49,199[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:20:49,202[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:20:50,526[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:20:50,612[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:20:50,612[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T18:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:23:01,279[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 18:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:23:01,296[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=20638) last sent a heartbeat 132.49 seconds ago! Restarting it[0m
[[34m2021-03-31 18:23:01,300[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 20638[0m
[[34m2021-03-31 18:23:01,472[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=20638, status='terminated', exitcode=0, started='18:12:58') (20638) terminated with exit code 0[0m
[[34m2021-03-31 18:23:01,476[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 597[0m
[[34m2021-03-31 18:23:01,485[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 18:23:01,498] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 18:23:02,055[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 18:10:00+00:00: scheduled__2021-03-31T18:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 18:23:58,911[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:28:58,945[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:30:01,063[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:30:01,064[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:30:01,064[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:30:01,064[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:30:01,065[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 18, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 18:30:01,065[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:30:01,067[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:30:02,564[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:30:02,622[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:30:02,622[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T18:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:30:03,419[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 18:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:30:03,565[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:30:03,566[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:30:03,566[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:30:03,566[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:30:03,567[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 18, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 18:30:03,568[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:30:03,570[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:30:04,169[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:30:04,217[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:30:04,217[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T18:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:30:45,636[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 18:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:30:46,133[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:30:46,134[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:30:46,134[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:30:46,135[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 18:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:30:46,136[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 18, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 18:30:46,136[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:30:46,138[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:30:47,483[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:30:47,568[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:30:47,568[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T18:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:33:27,718[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 18:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:33:27,734[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=597) last sent a heartbeat 161.95 seconds ago! Restarting it[0m
[[34m2021-03-31 18:33:27,738[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 597[0m
[[34m2021-03-31 18:33:27,910[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=597, status='terminated', exitcode=0, started='18:23:00') (597) terminated with exit code 0[0m
[[34m2021-03-31 18:33:27,914[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 13667[0m
[[34m2021-03-31 18:33:27,923[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 18:33:27,947] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 18:33:28,487[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 18:20:00+00:00: scheduled__2021-03-31T18:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 18:33:58,973[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:38:59,104[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:40:02,221[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:40:02,229[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:40:02,232[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:40:02,232[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:40:02,234[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 18, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 18:40:02,234[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:40:02,241[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:40:03,982[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:40:04,067[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:40:04,067[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T18:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:40:04,985[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 18:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:40:05,149[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:40:05,151[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:40:05,151[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:40:05,151[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:40:05,153[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 18, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 18:40:05,153[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:40:05,156[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:40:06,426[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:40:06,522[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:40:06,522[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T18:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:40:49,712[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 18:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:40:50,202[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:40:50,203[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:40:50,204[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:40:50,204[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 18:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:40:50,205[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 18, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 18:40:50,205[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:40:50,208[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:40:51,257[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:40:51,346[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:40:51,346[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T18:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:43:01,346[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 18:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:43:01,365[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=13667) last sent a heartbeat 131.51 seconds ago! Restarting it[0m
[[34m2021-03-31 18:43:01,368[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 13667[0m
[[34m2021-03-31 18:43:01,582[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=13667, status='terminated', exitcode=0, started='18:33:27') (13667) terminated with exit code 0[0m
[[34m2021-03-31 18:43:01,586[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25671[0m
[[34m2021-03-31 18:43:01,616[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 18:43:01,634] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 18:43:02,149[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 18:30:00+00:00: scheduled__2021-03-31T18:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 18:43:59,232[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:48:59,268[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:50:01,606[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:50:01,607[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:50:01,608[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:50:01,608[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:50:01,609[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 18, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 18:50:01,609[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:50:01,612[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:50:03,250[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:50:03,334[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:50:03,334[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T18:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:50:03,992[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 18:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:50:04,139[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:50:04,140[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:50:04,140[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:50:04,140[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:50:04,141[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 18, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 18:50:04,141[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:50:04,144[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:50:04,752[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:50:04,806[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:50:04,807[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T18:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:50:45,115[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 18:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:50:45,697[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:50:45,703[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 18:50:45,703[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 18:50:45,703[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 18:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 18:50:45,705[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 18, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 18:50:45,705[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:50:45,713[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 18:50:47,323[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 18:50:47,435[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 18:50:47,435[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T18:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 18:53:06,731[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 18:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 18:53:06,748[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25671) last sent a heartbeat 141.47 seconds ago! Restarting it[0m
[[34m2021-03-31 18:53:06,752[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25671[0m
[[34m2021-03-31 18:53:06,924[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25671, status='terminated', exitcode=0, started='18:43:00') (25671) terminated with exit code 0[0m
[[34m2021-03-31 18:53:06,928[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5940[0m
[[34m2021-03-31 18:53:06,943[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 18:53:06,963] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 18:53:07,493[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 18:40:00+00:00: scheduled__2021-03-31T18:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 18:53:59,352[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 18:58:59,387[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:00:01,511[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:00:01,512[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:00:01,513[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:00:01,513[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:00:01,525[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 18, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 19:00:01,526[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:00:01,530[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:00:03,184[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:00:03,242[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:00:03,243[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T18:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:00:03,981[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 18:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:00:04,150[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:00:04,151[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:00:04,151[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:00:04,151[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:00:04,153[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 18, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 19:00:04,153[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:00:04,166[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:00:05,185[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:00:05,277[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:00:05,277[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T18:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:00:48,761[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 18:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:00:49,261[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:00:49,262[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:00:49,262[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:00:49,262[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 18:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:00:49,263[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 18, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 19:00:49,263[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:00:49,267[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T18:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:00:50,537[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:00:50,639[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:00:50,639[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T18:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:03:01,115[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 18:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:03:01,132[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5940) last sent a heartbeat 132.23 seconds ago! Restarting it[0m
[[34m2021-03-31 19:03:01,137[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5940[0m
[[34m2021-03-31 19:03:01,391[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5940, status='terminated', exitcode=0, started='18:53:06') (5940) terminated with exit code 0[0m
[[34m2021-03-31 19:03:01,396[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18243[0m
[[34m2021-03-31 19:03:01,422[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 19:03:01,441] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 19:03:02,060[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 18:50:00+00:00: scheduled__2021-03-31T18:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 19:03:59,520[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:08:59,548[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:10:01,248[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:10:01,249[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:10:01,249[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:10:01,249[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:10:01,250[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 19, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 19:10:01,250[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:10:01,253[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:10:02,685[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:10:02,744[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:10:02,744[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T19:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:10:03,443[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 19:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:10:03,595[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:10:03,596[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:10:03,596[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:10:03,596[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:10:03,597[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 19, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 19:10:03,597[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:10:03,600[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:10:04,237[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:10:04,295[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:10:04,295[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T19:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:10:45,616[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 19:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:10:46,193[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:10:46,195[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:10:46,195[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:10:46,195[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 19:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:10:46,197[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 19, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 19:10:46,197[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:10:46,201[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:10:47,996[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:10:48,120[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:10:48,120[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T19:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:12:58,747[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 19:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:12:58,764[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18243) last sent a heartbeat 133.00 seconds ago! Restarting it[0m
[[34m2021-03-31 19:12:58,768[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18243[0m
[[34m2021-03-31 19:12:58,940[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18243, status='terminated', exitcode=0, started='19:03:00') (18243) terminated with exit code 0[0m
[[34m2021-03-31 19:12:58,944[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30644[0m
[[34m2021-03-31 19:12:58,954[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 19:12:58,969] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 19:12:59,484[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 19:00:00+00:00: scheduled__2021-03-31T19:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 19:13:59,575[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:18:59,706[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:20:01,211[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:20:01,213[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:20:01,213[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:20:01,213[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:20:01,215[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 19, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 19:20:01,215[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:20:01,223[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:20:02,444[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:20:02,521[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:20:02,521[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T19:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:20:03,168[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 19:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:20:03,326[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:20:03,327[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:20:03,327[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:20:03,328[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:20:03,329[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 19, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 19:20:03,329[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:20:03,331[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:20:03,917[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:20:03,964[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:20:03,964[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T19:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:20:46,737[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 19:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:20:47,304[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:20:47,305[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:20:47,305[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:20:47,305[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 19:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:20:47,307[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 19, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 19:20:47,307[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:20:47,310[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:20:48,585[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:20:48,671[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:20:48,671[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T19:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:22:58,222[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 19:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:22:58,243[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30644) last sent a heartbeat 131.36 seconds ago! Restarting it[0m
[[34m2021-03-31 19:22:58,248[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30644[0m
[[34m2021-03-31 19:22:58,466[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30644, status='terminated', exitcode=0, started='19:12:58') (30644) terminated with exit code 0[0m
[[34m2021-03-31 19:22:58,470[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10955[0m
[[34m2021-03-31 19:22:58,499[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 19:22:58,520] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 19:22:59,148[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 19:10:00+00:00: scheduled__2021-03-31T19:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 19:23:59,861[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:28:59,995[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:30:01,314[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:30:01,315[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:30:01,315[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:30:01,315[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:30:01,316[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 19, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 19:30:01,316[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:30:01,319[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:30:02,683[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:30:02,762[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:30:02,762[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T19:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:30:03,430[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 19:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:30:03,588[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:30:03,589[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:30:03,589[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:30:03,590[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:30:03,591[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 19, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 19:30:03,591[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:30:03,594[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:30:04,184[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:30:04,231[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:30:04,231[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T19:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:30:50,096[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 19:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:30:50,629[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:30:50,630[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:30:50,630[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:30:50,630[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 19:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:30:50,631[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 19, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 19:30:50,631[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:30:50,634[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:30:52,064[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:30:52,150[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:30:52,150[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T19:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:33:01,914[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 19:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:33:01,931[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10955) last sent a heartbeat 131.69 seconds ago! Restarting it[0m
[[34m2021-03-31 19:33:01,934[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10955[0m
[[34m2021-03-31 19:33:02,106[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10955, status='terminated', exitcode=0, started='19:22:57') (10955) terminated with exit code 0[0m
[[34m2021-03-31 19:33:02,110[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23322[0m
[[34m2021-03-31 19:33:02,121[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 19:33:02,150] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 19:33:02,715[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 19:20:00+00:00: scheduled__2021-03-31T19:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 19:34:00,126[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:39:00,257[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:40:01,728[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:40:01,729[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:40:01,730[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:40:01,730[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:40:01,731[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 19, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 19:40:01,731[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:40:01,739[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:40:03,007[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:40:03,058[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:40:03,058[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T19:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:40:03,731[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 19:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:40:03,877[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:40:03,878[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:40:03,878[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:40:03,878[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:40:03,879[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 19, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 19:40:03,879[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:40:03,883[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:40:04,465[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:40:04,512[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:40:04,512[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T19:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:40:45,164[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 19:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:40:45,665[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:40:45,666[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:40:45,666[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:40:45,666[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 19:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:40:45,667[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 19, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 19:40:45,667[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:40:45,670[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:40:46,947[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:40:47,041[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:40:47,041[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T19:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:43:06,170[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 19:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:43:06,187[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23322) last sent a heartbeat 140.88 seconds ago! Restarting it[0m
[[34m2021-03-31 19:43:06,191[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23322[0m
[[34m2021-03-31 19:43:06,363[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23322, status='terminated', exitcode=0, started='19:33:01') (23322) terminated with exit code 0[0m
[[34m2021-03-31 19:43:06,366[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3568[0m
[[34m2021-03-31 19:43:06,376[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 19:43:06,390] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 19:43:06,912[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 19:30:00+00:00: scheduled__2021-03-31T19:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 19:44:00,388[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:49:00,529[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:50:02,047[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:50:02,049[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:50:02,049[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:50:02,049[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:50:02,051[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 19, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 19:50:02,051[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:50:02,054[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:50:03,773[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:50:03,854[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:50:03,854[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T19:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:50:04,800[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 19:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:50:04,969[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:50:04,971[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:50:04,971[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:50:04,971[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:50:04,972[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 19, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 19:50:04,973[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:50:04,976[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:50:06,158[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:50:06,212[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:50:06,212[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T19:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:50:49,002[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 19:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:50:49,567[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:50:49,568[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 19:50:49,568[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 19:50:49,568[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 19:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 19:50:49,570[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 19, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 19:50:49,570[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:50:49,573[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 19:50:50,874[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 19:50:50,963[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 19:50:50,963[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T19:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 19:53:01,965[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 19:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 19:53:01,982[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3568) last sent a heartbeat 132.83 seconds ago! Restarting it[0m
[[34m2021-03-31 19:53:01,986[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3568[0m
[[34m2021-03-31 19:53:02,158[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3568, status='terminated', exitcode=0, started='19:43:05') (3568) terminated with exit code 0[0m
[[34m2021-03-31 19:53:02,162[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16005[0m
[[34m2021-03-31 19:53:02,171[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 19:53:02,184] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 19:53:02,747[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 19:40:00+00:00: scheduled__2021-03-31T19:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 19:54:00,662[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 19:59:00,689[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:00:01,658[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:00:01,659[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:00:01,659[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:00:01,659[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:00:01,660[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 19, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 20:00:01,660[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:00:01,664[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:00:03,019[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:00:03,073[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:00:03,073[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T19:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:00:03,766[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 19:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:00:03,913[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:00:03,914[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:00:03,914[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:00:03,914[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:00:03,915[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 19, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 20:00:03,915[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:00:03,918[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:00:04,513[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:00:04,562[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:00:04,562[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T19:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:00:44,025[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 19:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:00:44,610[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:00:44,611[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:00:44,612[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:00:44,612[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 19:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:00:44,613[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 19, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 20:00:44,613[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:00:44,617[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T19:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:00:46,270[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:00:46,377[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:00:46,377[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T19:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:02:57,156[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 19:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:02:57,174[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16005) last sent a heartbeat 132.98 seconds ago! Restarting it[0m
[[34m2021-03-31 20:02:57,177[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16005[0m
[[34m2021-03-31 20:02:57,349[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16005, status='terminated', exitcode=0, started='19:53:01') (16005) terminated with exit code 0[0m
[[34m2021-03-31 20:02:57,353[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28286[0m
[[34m2021-03-31 20:02:57,362[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 20:02:57,382] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 20:02:57,906[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 19:50:00+00:00: scheduled__2021-03-31T19:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 20:04:00,829[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:09:00,865[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:10:02,157[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:10:02,159[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:10:02,159[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:10:02,159[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:10:02,161[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 20, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 20:10:02,161[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:10:02,165[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:10:03,529[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:10:03,582[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:10:03,582[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T20:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:10:04,278[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 20:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:10:04,427[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:10:04,428[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:10:04,428[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:10:04,428[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:10:04,429[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 20, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 20:10:04,429[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:10:04,433[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:10:05,199[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:10:05,245[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:10:05,245[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T20:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:10:48,421[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 20:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:10:48,948[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:10:48,950[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:10:48,950[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:10:48,950[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 20:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:10:48,951[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 20, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 20:10:48,951[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:10:48,954[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:10:50,252[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:10:50,337[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:10:50,337[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T20:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:13:00,725[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 20:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:13:00,744[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28286) last sent a heartbeat 132.18 seconds ago! Restarting it[0m
[[34m2021-03-31 20:13:00,749[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28286[0m
[[34m2021-03-31 20:13:00,962[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28286, status='terminated', exitcode=0, started='20:02:56') (28286) terminated with exit code 0[0m
[[34m2021-03-31 20:13:00,967[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8637[0m
[[34m2021-03-31 20:13:00,994[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 20:13:01,013] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 20:13:01,617[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 20:00:00+00:00: scheduled__2021-03-31T20:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 20:14:00,894[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:19:00,922[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:20:01,646[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:20:01,647[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:20:01,648[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:20:01,648[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:20:01,649[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 20, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 20:20:01,649[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:20:01,652[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:20:02,926[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:20:02,978[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:20:02,978[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T20:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:20:03,658[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 20:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:20:03,803[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:20:03,804[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:20:03,805[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:20:03,805[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:20:03,806[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 20, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 20:20:03,806[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:20:03,809[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:20:04,392[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:20:04,437[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:20:04,438[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T20:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:20:44,644[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 20:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:20:45,226[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:20:45,227[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:20:45,228[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:20:45,228[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 20:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:20:45,229[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 20, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 20:20:45,230[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:20:45,233[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:20:46,931[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:20:47,049[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:20:47,049[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T20:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:22:54,994[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 20:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:22:55,015[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8637) last sent a heartbeat 130.20 seconds ago! Restarting it[0m
[[34m2021-03-31 20:22:55,019[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8637[0m
[[34m2021-03-31 20:22:55,232[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8637, status='terminated', exitcode=0, started='20:13:00') (8637) terminated with exit code 0[0m
[[34m2021-03-31 20:22:55,236[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 20931[0m
[[34m2021-03-31 20:22:55,263[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 20:22:55,286] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 20:22:55,882[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 20:10:00+00:00: scheduled__2021-03-31T20:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 20:24:01,052[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:29:01,135[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:30:02,178[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:30:02,179[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:30:02,179[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:30:02,179[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:30:02,180[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 20, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 20:30:02,180[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:30:02,183[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:30:03,404[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:30:03,459[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:30:03,459[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T20:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:30:04,144[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 20:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:30:04,196[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:30:04,197[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:30:04,197[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:30:04,197[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:30:04,198[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 20, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 20:30:04,198[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:30:04,201[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:30:04,839[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:30:04,888[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:30:04,888[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T20:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:30:48,080[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 20:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:30:48,580[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:30:48,581[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:30:48,581[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:30:48,582[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 20:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:30:48,583[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 20, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 20:30:48,583[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:30:48,587[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:30:49,882[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:30:49,967[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:30:49,967[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T20:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:32:58,413[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 20:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:32:58,432[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=20931) last sent a heartbeat 130.21 seconds ago! Restarting it[0m
[[34m2021-03-31 20:32:58,437[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 20931[0m
[[34m2021-03-31 20:32:58,651[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=20931, status='terminated', exitcode=0, started='20:22:54') (20931) terminated with exit code 0[0m
[[34m2021-03-31 20:32:58,656[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 888[0m
[[34m2021-03-31 20:32:58,681[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 20:32:58,700] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 20:32:59,314[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 20:20:00+00:00: scheduled__2021-03-31T20:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 20:34:01,276[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:39:01,288[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:40:01,517[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:40:01,518[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:40:01,518[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:40:01,518[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:40:01,520[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 20, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 20:40:01,520[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:40:01,523[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:40:02,785[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:40:02,853[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:40:02,853[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T20:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:40:03,514[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 20:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:40:03,675[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:40:03,676[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:40:03,676[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:40:03,676[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:40:03,677[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 20, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 20:40:03,677[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:40:03,680[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:40:04,410[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:40:04,476[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:40:04,477[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T20:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:40:45,578[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 20:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:40:46,164[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:40:46,166[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:40:46,166[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:40:46,166[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 20:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:40:46,168[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 20, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 20:40:46,168[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:40:46,171[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:40:47,961[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:40:48,092[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:40:48,092[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T20:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:42:54,608[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 20:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:42:54,625[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=888) last sent a heartbeat 128.89 seconds ago! Restarting it[0m
[[34m2021-03-31 20:42:54,629[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 888[0m
[[34m2021-03-31 20:42:54,801[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=888, status='terminated', exitcode=0, started='20:32:57') (888) terminated with exit code 0[0m
[[34m2021-03-31 20:42:54,805[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 13540[0m
[[34m2021-03-31 20:42:54,814[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 20:42:54,828] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 20:42:55,342[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 20:30:00+00:00: scheduled__2021-03-31T20:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 20:44:01,323[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:49:01,351[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:50:01,550[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:50:01,551[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:50:01,551[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:50:01,551[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:50:01,552[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 20, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 20:50:01,552[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:50:01,556[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:50:02,802[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:50:02,853[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:50:02,853[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T20:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:50:03,546[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 20:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:50:03,597[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:50:03,598[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:50:03,598[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:50:03,598[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:50:03,599[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 20, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 20:50:03,599[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:50:03,602[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:50:04,185[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:50:04,230[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:50:04,230[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T20:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:50:45,901[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 20:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:50:46,413[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:50:46,414[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 20:50:46,414[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 20:50:46,415[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 20:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 20:50:46,416[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 20, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 20:50:46,416[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:50:46,418[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 20:50:47,647[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 20:50:47,739[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 20:50:47,739[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T20:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 20:52:54,841[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 20:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 20:52:54,860[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=13540) last sent a heartbeat 128.81 seconds ago! Restarting it[0m
[[34m2021-03-31 20:52:54,866[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 13540[0m
[[34m2021-03-31 20:52:55,080[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=13540, status='terminated', exitcode=0, started='20:42:54') (13540) terminated with exit code 0[0m
[[34m2021-03-31 20:52:55,084[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25817[0m
[[34m2021-03-31 20:52:55,109[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 20:52:55,129] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 20:52:55,714[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 20:40:00+00:00: scheduled__2021-03-31T20:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 20:54:01,378[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 20:59:01,405[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:00:02,045[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:00:02,046[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:00:02,046[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:00:02,046[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:00:02,047[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 20, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 21:00:02,047[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:00:02,050[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:00:03,193[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:00:03,246[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:00:03,246[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T20:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:00:03,951[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 20:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:00:04,010[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:00:04,011[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:00:04,011[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:00:04,011[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:00:04,013[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 20, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 21:00:04,013[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:00:04,016[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:00:04,984[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:00:05,067[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:00:05,067[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T20:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:00:48,355[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 20:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:00:48,898[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:00:48,899[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:00:48,899[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:00:48,899[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 20:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:00:48,900[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 20, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 21:00:48,901[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:00:48,904[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T20:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:00:50,313[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:00:50,399[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:00:50,400[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T20:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:02:59,677[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 20:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:02:59,694[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25817) last sent a heartbeat 131.18 seconds ago! Restarting it[0m
[[34m2021-03-31 21:02:59,697[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25817[0m
[[34m2021-03-31 21:02:59,910[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25817, status='terminated', exitcode=0, started='20:52:54') (25817) terminated with exit code 0[0m
[[34m2021-03-31 21:02:59,915[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6155[0m
[[34m2021-03-31 21:02:59,938[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 21:02:59,958] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 21:03:00,489[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 20:50:00+00:00: scheduled__2021-03-31T20:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 21:04:01,418[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:09:01,561[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:10:01,789[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:10:01,795[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:10:01,795[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:10:01,795[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:10:01,797[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 21, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 21:10:01,797[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:10:01,801[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:10:03,032[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:10:03,084[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:10:03,084[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T21:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:10:03,720[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 21:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:10:03,903[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:10:03,904[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:10:03,904[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:10:03,904[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:10:03,906[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 21, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 21:10:03,906[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:10:03,911[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:10:04,644[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:10:04,691[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:10:04,691[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T21:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:10:45,782[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 21:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:10:46,271[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:10:46,272[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:10:46,272[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:10:46,272[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 21:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:10:46,273[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 21, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 21:10:46,273[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:10:46,276[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:10:47,382[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:10:47,492[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:10:47,492[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T21:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:12:56,347[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 21:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:12:56,366[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6155) last sent a heartbeat 130.44 seconds ago! Restarting it[0m
[[34m2021-03-31 21:12:56,370[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6155[0m
[[34m2021-03-31 21:12:56,623[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6155, status='terminated', exitcode=0, started='21:02:59') (6155) terminated with exit code 0[0m
[[34m2021-03-31 21:12:56,627[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18447[0m
[[34m2021-03-31 21:12:56,645[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 21:12:56,665] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 21:12:57,245[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 21:00:00+00:00: scheduled__2021-03-31T21:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 21:14:01,610[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:19:01,661[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:20:01,483[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:20:01,484[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:20:01,484[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:20:01,485[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:20:01,486[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 21, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 21:20:01,486[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:20:01,490[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:20:03,276[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:20:03,360[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:20:03,360[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T21:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:20:04,285[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 21:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:20:04,454[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:20:04,457[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:20:04,457[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:20:04,457[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:20:04,463[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 21, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 21:20:04,463[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:20:04,466[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:20:05,694[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:20:05,775[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:20:05,775[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T21:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:20:48,676[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 21:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:20:49,192[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:20:49,193[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:20:49,193[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:20:49,193[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 21:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:20:49,194[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 21, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 21:20:49,194[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:20:49,197[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:20:50,560[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:20:50,641[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:20:50,641[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T21:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:22:55,658[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 21:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:22:55,674[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18447) last sent a heartbeat 126.85 seconds ago! Restarting it[0m
[[34m2021-03-31 21:22:55,677[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18447[0m
[[34m2021-03-31 21:22:55,850[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18447, status='terminated', exitcode=0, started='21:12:55') (18447) terminated with exit code 0[0m
[[34m2021-03-31 21:22:55,854[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30658[0m
[[34m2021-03-31 21:22:55,867[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 21:22:55,888] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 21:22:56,359[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 21:10:00+00:00: scheduled__2021-03-31T21:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 21:24:01,794[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:29:01,829[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:30:01,461[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:30:01,462[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:30:01,462[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:30:01,463[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:30:01,463[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 21, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 21:30:01,464[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:30:01,467[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:30:02,653[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:30:02,711[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:30:02,711[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T21:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:30:03,335[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 21:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:30:03,481[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:30:03,482[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:30:03,482[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:30:03,482[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:30:03,483[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 21, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 21:30:03,483[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:30:03,487[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:30:04,074[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:30:04,120[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:30:04,120[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T21:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:30:40,741[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 21:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:30:40,907[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:30:40,908[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:30:40,908[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:30:40,908[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 21:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:30:40,909[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 21, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 21:30:40,909[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:30:40,912[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:30:42,119[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:30:42,202[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:30:42,202[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T21:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:32:42,899[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 21:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:32:42,915[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30658) last sent a heartbeat 122.03 seconds ago! Restarting it[0m
[[34m2021-03-31 21:32:42,918[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30658[0m
[[34m2021-03-31 21:32:43,090[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30658, status='terminated', exitcode=0, started='21:22:55') (30658) terminated with exit code 0[0m
[[34m2021-03-31 21:32:43,093[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 9855[0m
[[34m2021-03-31 21:32:43,102[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 21:32:43,115] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 21:32:43,290[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 21:20:00+00:00: scheduled__2021-03-31T21:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 21:34:01,960[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:39:01,987[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:40:01,161[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:40:01,162[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:40:01,163[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:40:01,163[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:40:01,164[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 21, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 21:40:01,164[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:40:01,166[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:40:02,478[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:40:02,555[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:40:02,556[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T21:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:40:03,010[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 21:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:40:03,197[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:40:03,198[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:40:03,198[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:40:03,198[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:40:03,200[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 21, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 21:40:03,200[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:40:03,206[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:40:03,979[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:40:04,025[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:40:04,025[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T21:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:40:43,301[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 21:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:40:43,472[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:40:43,473[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:40:43,473[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:40:43,473[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 21:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:40:43,474[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 21, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 21:40:43,475[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:40:43,478[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:40:44,549[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:40:44,629[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:40:44,629[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T21:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:42:46,774[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 21:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:42:46,790[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=9855) last sent a heartbeat 123.35 seconds ago! Restarting it[0m
[[34m2021-03-31 21:42:46,793[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 9855[0m
[[34m2021-03-31 21:42:46,886[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=9855, status='terminated', exitcode=0, started='21:32:42') (9855) terminated with exit code 0[0m
[[34m2021-03-31 21:42:46,888[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21354[0m
[[34m2021-03-31 21:42:46,897[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 21:42:46,907] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 21:42:47,100[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 21:30:00+00:00: scheduled__2021-03-31T21:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 21:44:02,130[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:49:02,260[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:50:01,230[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:50:01,232[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:50:01,232[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:50:01,232[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:50:01,233[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 21, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 21:50:01,233[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:50:01,236[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:50:02,369[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:50:02,446[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:50:02,446[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T21:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:50:02,855[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 21:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:50:03,029[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:50:03,030[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:50:03,030[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:50:03,030[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:50:03,032[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 21, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 21:50:03,032[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:50:03,035[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:50:03,963[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:50:04,012[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:50:04,012[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T21:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:50:41,244[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 21:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:50:41,452[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:50:41,454[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 21:50:41,454[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 21:50:41,454[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 21:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 21:50:41,456[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 21, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 21:50:41,456[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:50:41,461[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 21:50:42,821[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 21:50:42,938[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 21:50:42,939[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T21:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 21:52:43,533[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 21:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 21:52:43,552[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21354) last sent a heartbeat 122.14 seconds ago! Restarting it[0m
[[34m2021-03-31 21:52:43,557[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21354[0m
[[34m2021-03-31 21:52:43,691[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21354, status='terminated', exitcode=0, started='21:42:46') (21354) terminated with exit code 0[0m
[[34m2021-03-31 21:52:43,695[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 368[0m
[[34m2021-03-31 21:52:43,726[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 21:52:43,737] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 21:52:43,917[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 21:40:00+00:00: scheduled__2021-03-31T21:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 21:54:02,393[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 21:59:02,536[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:00:01,098[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:00:01,099[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:00:01,099[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:00:01,099[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:00:01,100[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 21, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 22:00:01,100[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:00:01,108[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:00:02,360[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:00:02,415[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:00:02,415[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T21:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:00:02,832[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 21:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:00:03,015[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:00:03,016[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:00:03,017[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:00:03,017[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:00:03,021[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 21, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 22:00:03,021[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:00:03,026[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:00:03,761[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:00:03,810[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:00:03,811[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T21:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:00:43,914[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 21:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:00:44,082[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:00:44,083[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:00:44,083[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:00:44,083[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 21:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:00:44,084[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 21, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 22:00:44,084[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:00:44,087[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T21:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:00:45,173[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:00:45,278[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:00:45,278[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T21:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:02:46,761[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 21:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:02:46,777[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=368) last sent a heartbeat 122.72 seconds ago! Restarting it[0m
[[34m2021-03-31 22:02:46,781[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 368[0m
[[34m2021-03-31 22:02:46,873[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=368, status='terminated', exitcode=0, started='21:52:42') (368) terminated with exit code 0[0m
[[34m2021-03-31 22:02:46,875[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12144[0m
[[34m2021-03-31 22:02:46,884[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 22:02:46,894] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 22:02:47,073[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 21:50:00+00:00: scheduled__2021-03-31T21:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 22:04:02,668[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:09:02,819[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:10:01,333[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:10:01,337[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:10:01,337[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:10:01,337[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:10:01,342[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 22, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 22:10:01,342[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:10:01,346[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:10:02,599[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:10:02,651[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:10:02,651[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T22:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:10:03,009[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 22:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:10:03,163[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:10:03,164[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:10:03,165[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:10:03,165[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:10:03,166[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 22, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 22:10:03,166[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:10:03,169[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:10:04,068[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:10:04,148[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:10:04,148[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T22:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:10:44,865[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 22:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:10:45,052[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:10:45,053[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:10:45,053[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:10:45,053[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 22:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:10:45,055[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 22, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 22:10:45,055[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:10:45,058[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:10:46,242[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:10:46,323[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:10:46,323[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T22:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:12:47,396[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 22:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:12:47,412[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12144) last sent a heartbeat 122.39 seconds ago! Restarting it[0m
[[34m2021-03-31 22:12:47,415[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12144[0m
[[34m2021-03-31 22:12:47,507[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12144, status='terminated', exitcode=0, started='22:02:46') (12144) terminated with exit code 0[0m
[[34m2021-03-31 22:12:47,510[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23620[0m
[[34m2021-03-31 22:12:47,519[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 22:12:47,529] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 22:12:47,684[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 22:00:00+00:00: scheduled__2021-03-31T22:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 22:14:02,848[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:19:02,981[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:20:01,857[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:20:01,858[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:20:01,858[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:20:01,859[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:20:01,860[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 22, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 22:20:01,860[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:20:01,864[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:20:02,852[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:20:02,905[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:20:02,905[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T22:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:20:03,401[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 22:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:20:03,558[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:20:03,559[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:20:03,559[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:20:03,559[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:20:03,560[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 22, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 22:20:03,561[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:20:03,564[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:20:04,160[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:20:04,212[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:20:04,212[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T22:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:20:42,193[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 22:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:20:42,359[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:20:42,360[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:20:42,360[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:20:42,360[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 22:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:20:42,361[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 22, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 22:20:42,361[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:20:42,366[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:20:43,411[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:20:43,490[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:20:43,491[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T22:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:22:47,558[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 22:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:22:47,575[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23620) last sent a heartbeat 125.24 seconds ago! Restarting it[0m
[[34m2021-03-31 22:22:47,578[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23620[0m
[[34m2021-03-31 22:22:47,670[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23620, status='terminated', exitcode=0, started='22:12:46') (23620) terminated with exit code 0[0m
[[34m2021-03-31 22:22:47,673[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2795[0m
[[34m2021-03-31 22:22:47,681[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 22:22:47,691] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 22:22:47,864[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 22:10:00+00:00: scheduled__2021-03-31T22:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 22:24:02,998[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:29:03,129[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:30:01,108[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:30:01,110[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:30:01,110[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:30:01,110[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:30:01,112[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 22, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 22:30:01,112[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:30:01,115[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:30:02,755[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:30:02,835[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:30:02,835[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T22:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:30:03,411[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 22:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:30:03,582[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:30:03,583[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:30:03,584[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:30:03,584[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:30:03,585[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 22, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 22:30:03,585[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:30:03,589[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:30:04,644[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:30:04,723[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:30:04,724[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T22:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:30:43,819[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 22:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:30:44,341[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:30:44,342[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:30:44,342[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:30:44,342[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 22:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:30:44,343[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 22, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 22:30:44,343[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:30:44,346[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:30:45,429[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:30:45,507[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:30:45,507[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T22:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:32:48,565[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 22:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:32:48,581[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=2795) last sent a heartbeat 124.60 seconds ago! Restarting it[0m
[[34m2021-03-31 22:32:48,584[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 2795[0m
[[34m2021-03-31 22:32:48,676[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=2795, status='terminated', exitcode=0, started='22:22:46') (2795) terminated with exit code 0[0m
[[34m2021-03-31 22:32:48,680[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14438[0m
[[34m2021-03-31 22:32:48,689[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 22:32:48,699] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 22:32:48,879[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 22:20:00+00:00: scheduled__2021-03-31T22:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 22:34:03,156[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:39:03,294[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:40:02,063[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:40:02,065[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:40:02,065[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:40:02,066[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:40:02,067[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 22, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 22:40:02,067[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:40:02,071[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:40:03,181[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:40:03,234[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:40:03,234[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T22:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:40:03,645[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 22:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:40:03,820[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:40:03,821[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:40:03,822[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:40:03,822[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:40:03,823[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 22, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 22:40:03,823[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:40:03,826[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:40:04,452[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:40:04,500[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:40:04,500[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T22:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:40:40,287[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 22:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:40:40,454[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:40:40,455[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:40:40,455[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:40:40,455[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 22:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:40:40,456[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 22, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 22:40:40,456[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:40:40,459[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:40:41,453[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:40:41,549[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:40:41,549[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T22:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:42:41,614[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 22:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:42:41,630[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14438) last sent a heartbeat 121.20 seconds ago! Restarting it[0m
[[34m2021-03-31 22:42:41,633[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14438[0m
[[34m2021-03-31 22:42:41,726[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14438, status='terminated', exitcode=0, started='22:32:47') (14438) terminated with exit code 0[0m
[[34m2021-03-31 22:42:41,729[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25801[0m
[[34m2021-03-31 22:42:41,738[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 22:42:41,748] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 22:42:41,921[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 22:30:00+00:00: scheduled__2021-03-31T22:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 22:44:03,426[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:49:03,557[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:50:01,653[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:50:01,655[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:50:01,655[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:50:01,655[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:50:01,656[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 22, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 22:50:01,657[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:50:01,661[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:50:02,839[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:50:02,891[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:50:02,892[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T22:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:50:03,253[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 22:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:50:03,424[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:50:03,426[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:50:03,426[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:50:03,426[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:50:03,427[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 22, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 22:50:03,427[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:50:03,430[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:50:04,384[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:50:04,441[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:50:04,441[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T22:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:50:43,175[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 22:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:50:43,664[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:50:43,665[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 22:50:43,665[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 22:50:43,665[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 22:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 22:50:43,666[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 22, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 22:50:43,666[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:50:43,669[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 22:50:44,505[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 22:50:44,578[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 22:50:44,578[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T22:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 22:52:47,445[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 22:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 22:52:47,461[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=25801) last sent a heartbeat 124.14 seconds ago! Restarting it[0m
[[34m2021-03-31 22:52:47,464[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25801[0m
[[34m2021-03-31 22:52:47,556[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=25801, status='terminated', exitcode=0, started='22:42:40') (25801) terminated with exit code 0[0m
[[34m2021-03-31 22:52:47,559[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5201[0m
[[34m2021-03-31 22:52:47,568[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 22:52:47,579] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 22:52:48,028[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 22:40:00+00:00: scheduled__2021-03-31T22:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 22:54:03,700[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 22:59:03,835[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:00:01,455[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:00:01,457[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:00:01,457[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:00:01,457[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:00:01,458[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 22, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 23:00:01,459[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:00:01,462[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:00:02,688[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:00:02,750[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:00:02,750[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T22:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:00:03,285[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 22:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:00:03,440[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:00:03,441[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:00:03,441[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:00:03,441[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:00:03,442[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 22, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 23:00:03,442[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:00:03,446[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:00:04,043[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:00:04,089[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:00:04,089[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T22:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:00:40,023[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 22:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:00:40,527[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:00:40,529[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:00:40,529[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:00:40,529[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 22:50:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:00:40,531[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 22, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 23:00:40,531[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:00:40,534[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T22:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:00:41,823[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:00:41,930[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:00:41,930[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T22:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:02:41,085[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 22:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:02:41,101[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5201) last sent a heartbeat 120.93 seconds ago! Restarting it[0m
[[34m2021-03-31 23:02:41,104[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5201[0m
[[34m2021-03-31 23:02:41,237[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5201, status='terminated', exitcode=0, started='22:52:46') (5201) terminated with exit code 0[0m
[[34m2021-03-31 23:02:41,241[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 16619[0m
[[34m2021-03-31 23:02:41,265[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 23:02:41,285] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 23:02:41,508[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 22:50:00+00:00: scheduled__2021-03-31T22:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 23:04:03,966[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:09:04,097[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:10:01,342[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:10:01,343[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:10:01,343[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:10:01,343[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:10:01,344[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 23, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 23:10:01,344[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:10:01,347[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:10:02,526[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:10:02,606[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:10:02,606[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T23:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:10:03,071[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 23:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:10:03,226[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:10:03,227[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:10:03,227[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:10:03,227[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:10:03,228[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 23, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 23:10:03,228[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:10:03,231[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:10:03,825[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:10:03,872[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:10:03,872[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T23:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:10:43,163[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 23:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:10:43,682[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:10:43,685[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:10:43,685[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:10:43,685[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 23:00:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:10:43,687[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 23, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 23:10:43,687[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:10:43,690[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:10:44,576[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:10:44,651[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:10:44,651[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T23:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:12:46,013[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 23:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:12:46,029[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=16619) last sent a heartbeat 122.72 seconds ago! Restarting it[0m
[[34m2021-03-31 23:12:46,032[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 16619[0m
[[34m2021-03-31 23:12:46,124[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=16619, status='terminated', exitcode=0, started='23:02:40') (16619) terminated with exit code 0[0m
[[34m2021-03-31 23:12:46,127[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28190[0m
[[34m2021-03-31 23:12:46,136[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 23:12:46,146] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 23:12:46,606[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 23:00:00+00:00: scheduled__2021-03-31T23:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 23:14:04,228[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:19:04,382[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:20:01,552[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:20:01,553[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:20:01,553[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:20:01,553[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:20:01,555[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 23, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 23:20:01,555[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:20:01,558[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:20:02,643[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:20:02,695[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:20:02,695[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T23:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:20:03,237[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 23:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:20:03,385[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:20:03,386[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:20:03,386[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:20:03,386[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:20:03,387[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 23, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 23:20:03,388[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:20:03,390[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:20:04,000[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:20:04,067[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:20:04,067[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T23:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:20:44,232[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 23:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:20:44,421[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:20:44,423[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:20:44,423[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:20:44,423[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 23:10:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:20:44,424[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 23, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 23:20:44,424[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:20:44,427[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:20:45,918[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:20:45,996[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:20:45,996[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T23:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:22:43,531[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 23:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:22:43,548[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28190) last sent a heartbeat 119.16 seconds ago! Restarting it[0m
[[34m2021-03-31 23:22:43,551[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28190[0m
[[34m2021-03-31 23:22:43,684[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28190, status='terminated', exitcode=0, started='23:12:45') (28190) terminated with exit code 0[0m
[[34m2021-03-31 23:22:43,687[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 7458[0m
[[34m2021-03-31 23:22:43,712[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 23:22:43,731] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 23:22:43,908[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 23:10:00+00:00: scheduled__2021-03-31T23:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 23:24:04,411[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:29:04,544[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:30:01,327[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:30:01,328[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:30:01,328[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:30:01,328[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:30:01,329[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 23, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 23:30:01,329[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:30:01,333[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:30:02,518[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:30:02,572[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:30:02,572[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T23:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:30:02,947[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 23:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:30:03,098[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:30:03,099[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:30:03,100[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:30:03,100[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:30:03,101[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 23, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 23:30:03,101[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:30:03,104[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:30:03,838[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:30:03,888[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:30:03,888[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T23:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:30:42,579[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 23:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:30:42,744[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:30:42,745[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:30:42,745[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:30:42,745[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 23:20:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:30:42,746[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 23, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 23:30:42,747[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:30:42,749[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:30:43,666[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:30:43,746[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:30:43,746[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T23:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:32:45,170[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 23:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:32:45,186[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=7458) last sent a heartbeat 122.47 seconds ago! Restarting it[0m
[[34m2021-03-31 23:32:45,189[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 7458[0m
[[34m2021-03-31 23:32:45,282[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=7458, status='terminated', exitcode=0, started='23:22:42') (7458) terminated with exit code 0[0m
[[34m2021-03-31 23:32:45,284[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18963[0m
[[34m2021-03-31 23:32:45,293[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 23:32:45,303] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 23:32:45,475[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 23:20:00+00:00: scheduled__2021-03-31T23:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 23:34:04,571[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:39:04,710[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:40:01,152[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:40:01,153[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:40:01,153[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:40:01,153[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:40:01,154[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 23, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 23:40:01,154[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:40:01,157[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:40:02,427[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:40:02,527[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:40:02,527[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T23:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:40:03,100[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 23:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:40:03,270[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:40:03,272[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:40:03,272[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:40:03,272[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:40:03,273[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 23, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 23:40:03,275[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:40:03,279[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:40:04,373[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:40:04,466[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:40:04,466[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T23:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:40:45,171[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 23:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:40:45,338[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:40:45,339[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:40:45,339[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:40:45,339[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 23:30:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:40:45,341[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 23, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 23:40:45,341[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:40:45,344[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:40:46,214[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:40:46,292[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:40:46,292[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T23:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:42:57,358[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 23:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:42:57,374[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18963) last sent a heartbeat 132.06 seconds ago! Restarting it[0m
[[34m2021-03-31 23:42:57,379[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18963[0m
[[34m2021-03-31 23:42:57,471[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18963, status='terminated', exitcode=0, started='23:32:44') (18963) terminated with exit code 0[0m
[[34m2021-03-31 23:42:57,474[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30665[0m
[[34m2021-03-31 23:42:57,482[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 23:42:57,492] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 23:42:57,672[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 23:30:00+00:00: scheduled__2021-03-31T23:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 23:44:04,845[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:49:04,884[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:50:01,504[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:50:01,505[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:50:01,505[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:50:01,506[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:50:01,507[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 23, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-31 23:50:01,507[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:50:01,510[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:50:02,572[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:50:02,626[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:50:02,626[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T23:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:50:03,037[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 23:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:50:03,197[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:50:03,198[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:50:03,198[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:50:03,198[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:50:03,200[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 23, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-31 23:50:03,200[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:50:03,203[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:50:03,799[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:50:03,848[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:50:03,848[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T23:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:50:42,217[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 23:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:50:42,757[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:50:42,759[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-03-31 23:50:42,759[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-03-31 23:50:42,759[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 23:40:00+00:00 [scheduled]>[0m
[[34m2021-03-31 23:50:42,761[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 23, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-03-31 23:50:42,761[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:50:42,766[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-03-31 23:50:43,925[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-03-31 23:50:44,004[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-31 23:50:44,004[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T23:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-03-31 23:52:44,909[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 23:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-31 23:52:44,925[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30665) last sent a heartbeat 122.56 seconds ago! Restarting it[0m
[[34m2021-03-31 23:52:44,932[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30665[0m
[[34m2021-03-31 23:52:45,037[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30665, status='terminated', exitcode=0, started='23:42:56') (30665) terminated with exit code 0[0m
[[34m2021-03-31 23:52:45,042[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 9920[0m
[[34m2021-03-31 23:52:45,056[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-31 23:52:45,074] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-31 23:52:45,276[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 23:40:00+00:00: scheduled__2021-03-31T23:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-03-31 23:54:05,017[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-31 23:59:05,052[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:00:02,196[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:00:02,197[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:00:02,197[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:00:02,198[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-03-31 23:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:00:02,199[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 3, 31, 23, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 00:00:02,199[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:00:02,203[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-03-31T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:00:03,784[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:00:03,871[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:00:03,872[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-03-31T23:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:00:04,478[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-03-31 23:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:00:04,636[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-03-31 23:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:00:04,637[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:00:04,637[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:00:04,637[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-03-31 23:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:00:04,638[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 3, 31, 23, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 00:00:04,639[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:00:04,642[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-03-31T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:00:05,246[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:00:05,296[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:00:05,296[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-03-31T23:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:00:44,819[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-03-31 23:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:00:44,986[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-03-31 23:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:00:44,987[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:00:44,987[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:00:44,987[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-03-31 23:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:00:44,988[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 3, 31, 23, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 00:00:44,988[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:00:44,991[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-03-31T23:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:00:46,118[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:00:46,198[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:00:46,198[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-03-31T23:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:02:53,315[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-03-31 23:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:02:53,332[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=9920) last sent a heartbeat 128.37 seconds ago! Restarting it[0m
[[34m2021-04-01 00:02:53,335[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 9920[0m
[[34m2021-04-01 00:02:53,427[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=9920, status='terminated', exitcode=0, started='23:52:44') (9920) terminated with exit code 0[0m
[[34m2021-04-01 00:02:53,430[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21725[0m
[[34m2021-04-01 00:02:53,439[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 00:02:53,449] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 00:02:53,621[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-03-31 23:50:00+00:00: scheduled__2021-03-31T23:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 00:04:05,208[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:09:05,237[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:10:01,334[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:10:01,335[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:10:01,335[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:10:01,335[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:10:01,336[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 00:10:01,336[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:10:01,339[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:10:02,505[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:10:02,563[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:10:02,563[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T00:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:10:02,978[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:10:03,026[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:10:03,027[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:10:03,027[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:10:03,027[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:10:03,028[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 00:10:03,028[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:10:03,031[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:10:03,627[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:10:03,676[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:10:03,676[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T00:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:10:40,467[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:10:40,633[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:10:40,634[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:10:40,634[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:10:40,634[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:10:40,635[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 00:10:40,635[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:10:40,638[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:10:41,669[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:10:41,776[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:10:41,776[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T00:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:12:42,625[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:12:42,641[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21725) last sent a heartbeat 122.03 seconds ago! Restarting it[0m
[[34m2021-04-01 00:12:42,644[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21725[0m
[[34m2021-04-01 00:12:42,736[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21725, status='terminated', exitcode=0, started='00:02:52') (21725) terminated with exit code 0[0m
[[34m2021-04-01 00:12:42,739[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 702[0m
[[34m2021-04-01 00:12:42,748[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 00:12:42,758] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 00:12:42,909[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 00:00:00+00:00: scheduled__2021-04-01T00:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 00:14:05,369[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:19:05,500[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:20:01,153[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:20:01,157[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:20:01,157[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:20:01,157[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:20:01,163[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 0, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 00:20:01,163[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:20:01,166[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:20:02,300[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:20:02,396[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:20:02,396[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T00:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:20:02,993[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 00:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:20:03,159[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 00:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:20:03,160[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:20:03,160[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:20:03,160[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 00:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:20:03,161[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 0, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 00:20:03,161[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:20:03,164[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:20:03,763[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:20:03,811[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:20:03,811[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T00:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:20:43,037[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 00:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:20:43,531[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 00:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:20:43,532[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:20:43,533[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:20:43,533[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 00:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:20:43,534[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 0, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 00:20:43,534[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:20:43,537[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:20:44,605[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:20:44,716[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:20:44,716[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T00:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:22:46,288[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 00:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:22:46,304[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=702) last sent a heartbeat 123.12 seconds ago! Restarting it[0m
[[34m2021-04-01 00:22:46,307[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 702[0m
[[34m2021-04-01 00:22:46,399[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=702, status='terminated', exitcode=0, started='00:12:41') (702) terminated with exit code 0[0m
[[34m2021-04-01 00:22:46,403[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12567[0m
[[34m2021-04-01 00:22:46,412[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 00:22:46,423] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 00:22:46,856[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 00:10:00+00:00: scheduled__2021-04-01T00:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 00:24:05,608[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:29:05,638[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:30:01,559[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:30:01,560[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:30:01,561[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:30:01,561[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:30:01,563[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 0, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 00:30:01,563[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:30:01,568[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:30:02,688[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:30:02,746[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:30:02,746[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T00:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:30:03,314[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 00:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:30:03,468[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 00:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:30:03,469[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:30:03,469[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:30:03,469[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 00:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:30:03,470[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 0, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 00:30:03,471[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:30:03,473[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:30:04,223[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:30:04,269[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:30:04,269[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T00:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:30:39,869[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 00:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:30:40,062[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 00:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:30:40,064[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:30:40,064[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:30:40,064[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 00:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:30:40,065[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 0, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 00:30:40,066[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:30:40,069[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:30:41,506[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:30:41,618[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:30:41,618[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T00:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:32:48,379[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 00:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:32:48,396[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12567) last sent a heartbeat 128.37 seconds ago! Restarting it[0m
[[34m2021-04-01 00:32:48,400[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12567[0m
[[34m2021-04-01 00:32:48,492[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12567, status='terminated', exitcode=0, started='00:22:45') (12567) terminated with exit code 0[0m
[[34m2021-04-01 00:32:48,495[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24210[0m
[[34m2021-04-01 00:32:48,504[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 00:32:48,514] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 00:32:49,017[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 00:20:00+00:00: scheduled__2021-04-01T00:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 00:34:05,666[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:39:05,797[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:40:01,412[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:40:01,413[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:40:01,413[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:40:01,413[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:40:01,414[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 0, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 00:40:01,414[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:40:01,418[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:40:02,715[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:40:02,768[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:40:02,768[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T00:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:40:03,352[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 00:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:40:03,507[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 00:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:40:03,508[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:40:03,508[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:40:03,508[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 00:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:40:03,509[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 0, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 00:40:03,510[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:40:03,513[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:40:04,116[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:40:04,165[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:40:04,166[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T00:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:40:44,072[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 00:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:40:44,238[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 00:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:40:44,239[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:40:44,239[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:40:44,240[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 00:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:40:44,241[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 0, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 00:40:44,241[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:40:44,244[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:40:45,613[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:40:45,696[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:40:45,696[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T00:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:42:51,228[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 00:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:42:51,245[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24210) last sent a heartbeat 127.03 seconds ago! Restarting it[0m
[[34m2021-04-01 00:42:51,248[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24210[0m
[[34m2021-04-01 00:42:51,420[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24210, status='terminated', exitcode=0, started='00:32:47') (24210) terminated with exit code 0[0m
[[34m2021-04-01 00:42:51,423[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3436[0m
[[34m2021-04-01 00:42:51,433[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 00:42:51,447] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 00:42:51,624[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 00:30:00+00:00: scheduled__2021-04-01T00:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 00:44:05,825[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:49:05,862[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:50:01,726[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:50:01,728[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:50:01,728[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:50:01,728[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:50:01,730[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 0, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 00:50:01,730[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:50:01,734[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:50:02,989[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:50:03,044[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:50:03,044[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T00:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:50:03,640[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 00:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:50:03,795[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 00:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:50:03,796[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:50:03,796[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:50:03,796[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 00:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:50:03,797[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 0, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 00:50:03,797[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:50:03,800[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:50:04,406[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:50:04,456[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:50:04,456[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T00:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:50:44,508[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 00:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:50:44,695[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 00:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:50:44,697[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 00:50:44,697[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 00:50:44,697[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 00:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 00:50:44,699[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 0, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 00:50:44,699[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:50:44,703[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 00:50:45,957[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 00:50:46,043[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 00:50:46,043[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T00:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 00:52:52,827[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 00:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 00:52:52,843[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3436) last sent a heartbeat 128.19 seconds ago! Restarting it[0m
[[34m2021-04-01 00:52:52,847[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3436[0m
[[34m2021-04-01 00:52:53,019[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3436, status='terminated', exitcode=0, started='00:42:50') (3436) terminated with exit code 0[0m
[[34m2021-04-01 00:52:53,022[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15037[0m
[[34m2021-04-01 00:52:53,032[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 00:52:53,044] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 00:52:53,197[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 00:40:00+00:00: scheduled__2021-04-01T00:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 00:54:05,952[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 00:59:06,083[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:00:01,951[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:00:01,952[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:00:01,953[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:00:01,953[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 00:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:00:01,954[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 0, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 01:00:01,954[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:00:01,958[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:00:03,119[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:00:03,178[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:00:03,178[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T00:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:00:03,643[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 00:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:00:03,804[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 00:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:00:03,805[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:00:03,805[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:00:03,805[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 00:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:00:03,806[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 0, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 01:00:03,806[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:00:03,809[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:00:04,409[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:00:04,458[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:00:04,458[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T00:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:00:43,319[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 00:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:00:43,496[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 00:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:00:43,497[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:00:43,497[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:00:43,497[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 00:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:00:43,499[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 0, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 01:00:43,499[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:00:43,502[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T00:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:00:44,798[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:00:44,928[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:00:44,928[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T00:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:02:50,242[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 00:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:02:50,259[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15037) last sent a heartbeat 126.79 seconds ago! Restarting it[0m
[[34m2021-04-01 01:02:50,262[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15037[0m
[[34m2021-04-01 01:02:50,434[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15037, status='terminated', exitcode=0, started='00:52:52') (15037) terminated with exit code 0[0m
[[34m2021-04-01 01:02:50,437[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26479[0m
[[34m2021-04-01 01:02:50,447[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 01:02:50,459] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 01:02:50,634[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 00:50:00+00:00: scheduled__2021-04-01T00:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 01:04:06,110[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:09:06,242[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:10:01,386[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:10:01,388[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:10:01,388[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:10:01,388[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:10:01,390[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 1, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 01:10:01,390[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:10:01,394[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:10:03,100[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:10:03,186[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:10:03,186[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T01:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:10:03,937[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 01:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:10:04,098[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 01:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:10:04,099[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:10:04,099[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:10:04,099[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 01:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:10:04,101[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 1, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 01:10:04,101[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:10:04,104[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:10:05,109[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:10:05,184[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:10:05,184[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T01:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:10:46,632[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 01:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:10:46,798[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 01:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:10:46,800[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:10:46,800[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:10:46,800[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 01:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:10:46,801[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 1, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 01:10:46,801[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:10:46,804[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:10:48,136[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:10:48,220[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:10:48,220[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T01:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:12:55,607[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 01:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:12:55,623[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26479) last sent a heartbeat 128.85 seconds ago! Restarting it[0m
[[34m2021-04-01 01:12:55,627[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26479[0m
[[34m2021-04-01 01:12:55,799[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26479, status='terminated', exitcode=0, started='01:02:49') (26479) terminated with exit code 0[0m
[[34m2021-04-01 01:12:55,802[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 5793[0m
[[34m2021-04-01 01:12:55,812[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 01:12:55,825] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 01:12:56,005[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 01:00:00+00:00: scheduled__2021-04-01T01:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 01:14:06,270[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:19:06,401[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:20:01,831[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:20:01,833[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:20:01,833[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:20:01,833[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:20:01,835[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 1, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 01:20:01,835[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:20:01,839[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:20:03,056[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:20:03,110[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:20:03,110[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T01:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:20:03,582[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 01:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:20:03,728[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 01:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:20:03,729[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:20:03,729[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:20:03,729[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 01:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:20:03,730[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 1, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 01:20:03,730[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:20:03,733[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:20:04,327[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:20:04,374[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:20:04,374[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T01:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:20:41,783[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 01:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:20:41,950[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 01:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:20:41,951[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:20:41,951[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:20:41,951[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 01:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:20:41,952[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 1, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 01:20:41,952[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:20:41,955[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:20:42,957[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:20:43,036[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:20:43,036[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T01:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:22:50,885[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 01:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:22:50,903[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=5793) last sent a heartbeat 128.98 seconds ago! Restarting it[0m
[[34m2021-04-01 01:22:50,907[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 5793[0m
[[34m2021-04-01 01:22:51,161[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=5793, status='terminated', exitcode=0, started='01:12:55') (5793) terminated with exit code 0[0m
[[34m2021-04-01 01:22:51,164[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17269[0m
[[34m2021-04-01 01:22:51,180[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 01:22:51,205] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 01:22:51,417[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 01:10:00+00:00: scheduled__2021-04-01T01:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 01:24:06,443[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:29:06,470[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:30:01,151[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:30:01,152[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:30:01,152[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:30:01,152[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:30:01,154[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 1, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 01:30:01,154[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:30:01,157[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:30:02,985[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:30:03,085[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:30:03,086[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T01:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:30:03,716[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 01:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:30:03,871[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 01:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:30:03,872[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:30:03,872[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:30:03,872[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 01:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:30:03,873[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 1, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 01:30:03,873[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:30:03,876[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:30:04,608[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:30:04,658[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:30:04,658[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T01:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:30:44,819[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 01:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:30:44,987[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 01:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:30:44,988[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:30:44,988[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:30:44,988[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 01:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:30:44,990[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 1, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 01:30:44,990[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:30:44,993[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:30:46,478[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:30:46,565[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:30:46,565[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T01:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:32:52,610[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 01:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:32:52,626[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17269) last sent a heartbeat 127.66 seconds ago! Restarting it[0m
[[34m2021-04-01 01:32:52,629[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17269[0m
[[34m2021-04-01 01:32:52,802[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17269, status='terminated', exitcode=0, started='01:22:50') (17269) terminated with exit code 0[0m
[[34m2021-04-01 01:32:52,805[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28781[0m
[[34m2021-04-01 01:32:52,815[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 01:32:52,827] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 01:32:53,010[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 01:20:00+00:00: scheduled__2021-04-01T01:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 01:34:06,610[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:39:06,622[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:40:02,045[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:40:02,046[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:40:02,046[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:40:02,047[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:40:02,048[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 1, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 01:40:02,048[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:40:02,052[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:40:03,278[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:40:03,364[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:40:03,364[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T01:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:40:04,031[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 01:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:40:04,209[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 01:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:40:04,211[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:40:04,211[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:40:04,211[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 01:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:40:04,212[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 1, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 01:40:04,212[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:40:04,216[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:40:04,826[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:40:04,875[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:40:04,875[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T01:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:40:41,019[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 01:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:40:41,187[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 01:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:40:41,188[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:40:41,188[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:40:41,189[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 01:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:40:41,190[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 1, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 01:40:41,190[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:40:41,193[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:40:42,465[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:40:42,589[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:40:42,589[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T01:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:42:55,277[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 01:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:42:55,293[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=28781) last sent a heartbeat 134.13 seconds ago! Restarting it[0m
[[34m2021-04-01 01:42:55,297[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 28781[0m
[[34m2021-04-01 01:42:55,429[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=28781, status='terminated', exitcode=0, started='01:32:52') (28781) terminated with exit code 0[0m
[[34m2021-04-01 01:42:55,432[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8153[0m
[[34m2021-04-01 01:42:55,442[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 01:42:55,455] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 01:42:55,610[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 01:30:00+00:00: scheduled__2021-04-01T01:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 01:44:06,689[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:49:06,717[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:50:01,533[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:50:01,535[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:50:01,535[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:50:01,535[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:50:01,536[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 1, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 01:50:01,537[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:50:01,540[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:50:02,643[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:50:02,701[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:50:02,701[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T01:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:50:03,257[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 01:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:50:03,427[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 01:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:50:03,429[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:50:03,429[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:50:03,429[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 01:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:50:03,435[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 1, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 01:50:03,435[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:50:03,438[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:50:04,413[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:50:04,460[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:50:04,460[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T01:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:50:44,138[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 01:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:50:44,306[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 01:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:50:44,307[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 01:50:44,307[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 01:50:44,307[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 01:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 01:50:44,308[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 1, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 01:50:44,309[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:50:44,312[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 01:50:45,342[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 01:50:45,423[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 01:50:45,423[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T01:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 01:52:52,092[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 01:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 01:52:52,108[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8153) last sent a heartbeat 127.83 seconds ago! Restarting it[0m
[[34m2021-04-01 01:52:52,112[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8153[0m
[[34m2021-04-01 01:52:52,284[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8153, status='terminated', exitcode=0, started='01:42:54') (8153) terminated with exit code 0[0m
[[34m2021-04-01 01:52:52,287[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19523[0m
[[34m2021-04-01 01:52:52,297[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 01:52:52,309] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 01:52:52,485[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 01:40:00+00:00: scheduled__2021-04-01T01:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 01:54:06,745[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 01:59:06,876[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:00:01,460[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:00:01,461[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:00:01,461[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:00:01,461[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 01:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:00:01,462[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 1, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 02:00:01,462[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:00:01,465[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:00:02,710[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:00:02,768[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:00:02,768[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T01:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:00:03,237[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 01:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:00:03,397[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 01:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:00:03,398[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:00:03,398[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:00:03,398[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 01:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:00:03,399[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 1, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 02:00:03,400[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:00:03,402[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:00:04,004[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:00:04,052[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:00:04,052[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T01:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:00:40,962[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 01:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:00:41,157[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 01:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:00:41,159[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:00:41,159[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:00:41,159[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 01:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:00:41,161[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 1, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 02:00:41,161[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:00:41,164[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T01:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:00:42,577[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:00:42,694[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:00:42,694[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T01:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:02:46,881[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 01:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:02:46,899[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19523) last sent a heartbeat 125.78 seconds ago! Restarting it[0m
[[34m2021-04-01 02:02:46,906[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19523[0m
[[34m2021-04-01 02:02:47,078[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19523, status='terminated', exitcode=0, started='01:52:51') (19523) terminated with exit code 0[0m
[[34m2021-04-01 02:02:47,081[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 30888[0m
[[34m2021-04-01 02:02:47,092[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 02:02:47,107] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 02:02:47,258[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 01:50:00+00:00: scheduled__2021-04-01T01:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 02:04:07,007[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:09:07,035[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:10:01,497[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:10:01,498[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:10:01,498[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:10:01,498[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:10:01,499[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 2, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 02:10:01,499[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:10:01,502[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:10:02,788[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:10:02,846[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:10:02,846[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T02:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:10:03,307[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 02:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:10:03,474[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 02:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:10:03,475[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:10:03,476[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:10:03,476[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 02:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:10:03,477[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 2, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 02:10:03,478[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:10:03,482[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:10:04,193[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:10:04,242[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:10:04,243[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T02:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:10:43,582[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 02:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:10:43,748[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 02:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:10:43,749[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:10:43,750[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:10:43,750[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 02:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:10:43,751[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 2, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 02:10:43,751[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:10:43,754[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:10:44,794[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:10:44,877[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:10:44,877[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T02:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:12:49,777[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 02:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:12:49,794[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=30888) last sent a heartbeat 126.07 seconds ago! Restarting it[0m
[[34m2021-04-01 02:12:49,797[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 30888[0m
[[34m2021-04-01 02:12:49,969[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=30888, status='terminated', exitcode=0, started='02:02:46') (30888) terminated with exit code 0[0m
[[34m2021-04-01 02:12:49,972[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10267[0m
[[34m2021-04-01 02:12:49,983[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 02:12:49,997] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 02:12:50,174[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 02:00:00+00:00: scheduled__2021-04-01T02:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 02:14:07,062[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:19:07,179[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:20:01,184[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:20:01,186[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:20:01,186[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:20:01,186[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:20:01,187[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 2, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 02:20:01,188[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:20:01,194[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:20:02,507[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:20:02,562[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:20:02,562[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T02:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:20:03,094[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 02:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:20:03,241[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 02:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:20:03,242[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:20:03,242[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:20:03,242[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 02:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:20:03,243[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 2, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 02:20:03,243[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:20:03,246[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:20:03,842[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:20:03,891[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:20:03,891[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T02:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:20:43,742[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 02:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:20:43,929[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 02:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:20:43,931[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:20:43,931[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:20:43,931[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 02:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:20:43,933[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 2, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 02:20:43,933[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:20:43,936[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:20:45,138[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:20:45,218[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:20:45,218[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T02:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:22:53,735[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 02:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:22:53,751[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10267) last sent a heartbeat 129.86 seconds ago! Restarting it[0m
[[34m2021-04-01 02:22:53,754[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10267[0m
[[34m2021-04-01 02:22:53,846[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10267, status='terminated', exitcode=0, started='02:12:49') (10267) terminated with exit code 0[0m
[[34m2021-04-01 02:22:53,849[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21694[0m
[[34m2021-04-01 02:22:53,861[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 02:22:53,875] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 02:22:54,026[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 02:10:00+00:00: scheduled__2021-04-01T02:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 02:24:07,323[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:29:07,362[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:30:01,898[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:30:01,900[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:30:01,903[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:30:01,903[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:30:01,904[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 2, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 02:30:01,904[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:30:01,910[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:30:02,938[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:30:02,989[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:30:02,989[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T02:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:30:03,460[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 02:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:30:03,507[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 02:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:30:03,507[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:30:03,508[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:30:03,508[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 02:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:30:03,509[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 2, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 02:30:03,509[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:30:03,512[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:30:04,113[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:30:04,158[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:30:04,158[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T02:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:30:43,319[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 02:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:30:43,487[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 02:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:30:43,488[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:30:43,488[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:30:43,488[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 02:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:30:43,489[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 2, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 02:30:43,489[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:30:43,493[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:30:44,556[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:30:44,640[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:30:44,640[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T02:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:32:49,187[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 02:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:32:49,204[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21694) last sent a heartbeat 125.74 seconds ago! Restarting it[0m
[[34m2021-04-01 02:32:49,207[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21694[0m
[[34m2021-04-01 02:32:49,299[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21694, status='terminated', exitcode=0, started='02:22:53') (21694) terminated with exit code 0[0m
[[34m2021-04-01 02:32:49,302[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 689[0m
[[34m2021-04-01 02:32:49,312[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 02:32:49,325] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 02:32:49,477[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 02:20:00+00:00: scheduled__2021-04-01T02:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 02:34:07,389[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:39:07,518[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:40:01,327[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:40:01,329[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:40:01,329[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:40:01,329[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:40:01,330[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 2, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 02:40:01,331[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:40:01,334[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:40:03,030[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:40:03,116[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:40:03,116[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T02:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:40:03,762[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 02:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:40:03,927[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 02:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:40:03,928[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:40:03,928[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:40:03,929[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 02:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:40:03,930[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 2, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 02:40:03,930[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:40:03,933[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:40:04,953[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:40:05,035[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:40:05,035[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T02:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:40:45,192[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 02:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:40:45,359[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 02:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:40:45,360[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:40:45,361[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:40:45,361[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 02:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:40:45,362[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 2, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 02:40:45,362[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:40:45,365[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:40:46,653[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:40:46,742[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:40:46,742[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T02:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:42:51,791[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 02:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:42:51,808[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=689) last sent a heartbeat 126.47 seconds ago! Restarting it[0m
[[34m2021-04-01 02:42:51,811[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 689[0m
[[34m2021-04-01 02:42:51,903[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=689, status='terminated', exitcode=0, started='02:32:48') (689) terminated with exit code 0[0m
[[34m2021-04-01 02:42:51,906[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12483[0m
[[34m2021-04-01 02:42:51,916[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 02:42:51,928] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 02:42:52,080[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 02:30:00+00:00: scheduled__2021-04-01T02:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 02:44:07,646[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:49:07,749[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:50:01,747[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:50:01,748[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:50:01,748[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:50:01,748[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:50:01,749[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 2, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 02:50:01,749[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:50:01,752[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:50:02,845[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:50:02,896[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:50:02,896[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T02:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:50:03,358[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 02:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:50:03,512[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 02:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:50:03,513[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:50:03,513[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:50:03,513[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 02:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:50:03,514[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 2, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 02:50:03,515[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:50:03,517[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:50:04,118[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:50:04,164[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:50:04,165[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T02:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:50:40,285[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 02:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:50:40,456[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 02:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:50:40,457[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 02:50:40,457[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 02:50:40,457[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 02:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 02:50:40,458[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 2, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 02:50:40,458[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:50:40,461[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 02:50:41,523[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 02:50:41,610[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 02:50:41,610[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T02:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 02:52:44,600[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 02:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 02:52:44,616[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12483) last sent a heartbeat 124.19 seconds ago! Restarting it[0m
[[34m2021-04-01 02:52:44,619[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12483[0m
[[34m2021-04-01 02:52:44,791[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12483, status='terminated', exitcode=0, started='02:42:51') (12483) terminated with exit code 0[0m
[[34m2021-04-01 02:52:44,794[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 23835[0m
[[34m2021-04-01 02:52:44,805[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 02:52:44,817] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 02:52:44,992[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 02:40:00+00:00: scheduled__2021-04-01T02:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 02:54:07,881[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 02:59:08,012[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:00:02,094[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:00:02,095[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:00:02,095[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:00:02,095[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 02:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:00:02,096[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 2, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 03:00:02,096[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:00:02,099[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:00:03,250[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:00:03,309[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:00:03,309[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T02:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:00:03,791[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 02:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:00:03,943[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 02:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:00:03,944[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:00:03,944[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:00:03,944[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 02:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:00:03,945[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 2, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 03:00:03,945[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:00:03,948[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:00:04,714[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:00:04,765[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:00:04,765[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T02:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:00:44,650[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 02:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:00:44,816[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 02:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:00:44,817[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:00:44,817[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:00:44,818[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 02:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:00:44,819[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 2, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 03:00:44,819[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:00:44,822[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T02:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:00:46,122[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:00:46,206[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:00:46,206[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T02:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:02:51,392[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 02:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:02:51,409[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=23835) last sent a heartbeat 126.62 seconds ago! Restarting it[0m
[[34m2021-04-01 03:02:51,413[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 23835[0m
[[34m2021-04-01 03:02:51,625[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=23835, status='terminated', exitcode=0, started='02:52:43') (23835) terminated with exit code 0[0m
[[34m2021-04-01 03:02:51,628[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3288[0m
[[34m2021-04-01 03:02:51,660[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 03:02:51,675] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 03:02:51,864[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 02:50:00+00:00: scheduled__2021-04-01T02:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 03:04:08,145[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:09:08,287[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:10:01,455[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:10:01,456[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:10:01,456[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:10:01,456[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:10:01,458[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 3, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 03:10:01,458[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:10:01,461[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:10:02,801[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:10:02,880[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:10:02,880[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T03:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:10:03,400[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 03:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:10:03,555[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 03:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:10:03,556[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:10:03,556[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:10:03,556[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 03:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:10:03,557[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 3, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 03:10:03,557[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:10:03,561[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:10:04,171[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:10:04,223[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:10:04,223[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T03:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:10:40,941[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 03:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:10:41,152[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 03:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:10:41,154[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:10:41,154[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:10:41,154[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 03:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:10:41,155[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 3, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 03:10:41,156[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:10:41,159[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:10:42,676[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:10:42,830[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:10:42,830[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T03:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:12:45,949[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 03:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:12:45,967[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3288) last sent a heartbeat 124.85 seconds ago! Restarting it[0m
[[34m2021-04-01 03:12:45,972[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3288[0m
[[34m2021-04-01 03:12:46,186[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3288, status='terminated', exitcode=0, started='03:02:50') (3288) terminated with exit code 0[0m
[[34m2021-04-01 03:12:46,190[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14998[0m
[[34m2021-04-01 03:12:46,220[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 03:12:46,237] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 03:12:46,417[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 03:00:00+00:00: scheduled__2021-04-01T03:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 03:14:08,317[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:19:08,447[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:20:01,990[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:20:01,991[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:20:01,991[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:20:01,991[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:20:01,992[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 3, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 03:20:01,992[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:20:01,995[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:20:03,132[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:20:03,184[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:20:03,184[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T03:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:20:03,648[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 03:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:20:03,793[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 03:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:20:03,794[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:20:03,794[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:20:03,794[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 03:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:20:03,795[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 3, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 03:20:03,795[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:20:03,798[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:20:04,394[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:20:04,441[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:20:04,441[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T03:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:20:45,390[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 03:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:20:45,557[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 03:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:20:45,558[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:20:45,558[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:20:45,558[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 03:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:20:45,560[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 3, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 03:20:45,560[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:20:45,563[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:20:46,782[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:20:46,863[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:20:46,863[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T03:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:22:57,182[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 03:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:22:57,200[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14998) last sent a heartbeat 131.67 seconds ago! Restarting it[0m
[[34m2021-04-01 03:22:57,205[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14998[0m
[[34m2021-04-01 03:22:57,419[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14998, status='terminated', exitcode=0, started='03:12:45') (14998) terminated with exit code 0[0m
[[34m2021-04-01 03:22:57,423[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26761[0m
[[34m2021-04-01 03:22:57,448[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 03:22:57,469] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 03:22:57,677[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 03:10:00+00:00: scheduled__2021-04-01T03:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 03:24:08,578[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:29:08,612[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:30:01,562[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:30:01,564[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:30:01,564[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:30:01,564[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:30:01,566[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 3, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 03:30:01,566[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:30:01,570[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:30:02,851[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:30:02,925[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:30:02,925[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T03:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:30:03,397[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 03:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:30:03,558[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 03:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:30:03,559[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:30:03,559[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:30:03,559[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 03:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:30:03,560[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 3, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 03:30:03,560[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:30:03,563[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:30:04,164[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:30:04,211[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:30:04,212[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T03:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:30:42,370[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 03:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:30:42,967[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 03:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:30:42,969[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:30:42,969[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:30:42,969[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 03:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:30:42,975[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 3, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 03:30:42,976[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:30:42,979[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:30:44,512[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:30:44,631[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:30:44,631[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T03:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:32:46,552[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 03:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:32:46,568[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26761) last sent a heartbeat 124.03 seconds ago! Restarting it[0m
[[34m2021-04-01 03:32:46,571[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26761[0m
[[34m2021-04-01 03:32:46,663[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26761, status='terminated', exitcode=0, started='03:22:56') (26761) terminated with exit code 0[0m
[[34m2021-04-01 03:32:46,666[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6129[0m
[[34m2021-04-01 03:32:46,674[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 03:32:46,684] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 03:32:47,120[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 03:20:00+00:00: scheduled__2021-04-01T03:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 03:34:08,756[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:39:08,888[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:40:01,365[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:40:01,367[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:40:01,367[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:40:01,367[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:40:01,369[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 3, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 03:40:01,369[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:40:01,373[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:40:02,491[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:40:02,547[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:40:02,547[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T03:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:40:03,150[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 03:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:40:03,306[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 03:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:40:03,307[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:40:03,307[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:40:03,307[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 03:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:40:03,308[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 3, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 03:40:03,309[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:40:03,311[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:40:03,925[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:40:03,992[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:40:03,992[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T03:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:40:42,786[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 03:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:40:43,320[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 03:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:40:43,321[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:40:43,321[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:40:43,322[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 03:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:40:43,323[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 3, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 03:40:43,323[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:40:43,326[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:40:44,418[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:40:44,498[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:40:44,498[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T03:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:42:59,642[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 03:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:42:59,657[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6129) last sent a heartbeat 136.73 seconds ago! Restarting it[0m
[[34m2021-04-01 03:42:59,661[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6129[0m
[[34m2021-04-01 03:42:59,753[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6129, status='terminated', exitcode=0, started='03:32:45') (6129) terminated with exit code 0[0m
[[34m2021-04-01 03:42:59,756[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17822[0m
[[34m2021-04-01 03:42:59,765[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 03:42:59,775] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 03:43:00,019[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 03:30:00+00:00: scheduled__2021-04-01T03:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 03:44:08,915[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:49:09,047[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:50:01,696[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:50:01,698[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:50:01,698[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:50:01,698[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:50:01,700[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 3, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 03:50:01,700[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:50:01,703[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:50:03,370[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:50:03,490[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:50:03,490[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T03:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:50:04,064[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 03:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:50:04,236[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 03:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:50:04,238[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:50:04,238[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:50:04,238[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 03:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:50:04,240[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 3, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 03:50:04,240[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:50:04,243[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:50:05,228[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:50:05,305[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:50:05,305[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T03:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:50:45,051[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 03:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:50:45,299[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 03:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:50:45,301[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 03:50:45,301[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 03:50:45,301[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 03:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 03:50:45,303[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 3, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 03:50:45,303[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:50:45,306[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 03:50:46,188[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 03:50:46,285[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 03:50:46,285[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T03:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 03:52:48,790[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 03:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 03:52:48,806[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17822) last sent a heartbeat 123.61 seconds ago! Restarting it[0m
[[34m2021-04-01 03:52:48,809[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17822[0m
[[34m2021-04-01 03:52:48,901[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17822, status='terminated', exitcode=0, started='03:42:58') (17822) terminated with exit code 0[0m
[[34m2021-04-01 03:52:48,904[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 29097[0m
[[34m2021-04-01 03:52:48,913[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 03:52:48,924] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 03:52:49,112[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 03:40:00+00:00: scheduled__2021-04-01T03:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 03:54:09,189[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 03:59:09,218[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:00:02,106[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:00:02,107[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:00:02,108[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:00:02,108[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 03:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:00:02,109[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 3, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 04:00:02,109[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:00:02,112[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:00:03,246[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:00:03,301[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:00:03,301[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T03:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:00:03,707[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 03:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:00:03,864[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 03:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:00:03,865[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:00:03,865[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:00:03,866[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 03:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:00:03,867[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 3, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 04:00:03,867[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:00:03,870[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:00:04,481[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:00:04,532[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:00:04,532[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T03:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:00:41,685[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 03:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:00:41,853[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 03:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:00:41,854[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:00:41,854[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:00:41,855[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 03:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:00:41,856[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 3, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 04:00:41,856[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:00:41,859[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T03:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:00:43,086[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:00:43,199[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:00:43,199[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T03:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:02:51,505[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 03:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:02:51,523[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=29097) last sent a heartbeat 129.69 seconds ago! Restarting it[0m
[[34m2021-04-01 04:02:51,528[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 29097[0m
[[34m2021-04-01 04:02:51,667[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=29097, status='terminated', exitcode=0, started='03:52:48') (29097) terminated with exit code 0[0m
[[34m2021-04-01 04:02:51,670[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8485[0m
[[34m2021-04-01 04:02:51,686[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 04:02:51,706] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 04:02:51,916[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 03:50:00+00:00: scheduled__2021-04-01T03:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 04:04:09,350[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:09:09,385[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:10:01,637[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:10:01,639[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:10:01,639[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:10:01,639[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:10:01,641[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 4, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 04:10:01,641[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:10:01,645[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:10:03,486[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:10:03,593[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:10:03,593[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T04:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:10:04,327[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 04:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:10:04,490[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 04:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:10:04,491[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:10:04,491[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:10:04,492[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 04:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:10:04,493[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 4, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 04:10:04,493[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:10:04,496[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:10:05,116[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:10:05,170[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:10:05,171[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T04:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:10:46,349[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 04:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:10:46,544[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 04:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:10:46,546[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:10:46,547[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:10:46,547[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 04:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:10:46,548[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 4, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 04:10:46,548[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:10:46,552[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:10:47,699[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:10:47,777[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:10:47,777[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T04:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:12:58,863[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 04:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:12:58,879[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8485) last sent a heartbeat 132.37 seconds ago! Restarting it[0m
[[34m2021-04-01 04:12:58,882[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8485[0m
[[34m2021-04-01 04:12:58,975[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8485, status='terminated', exitcode=0, started='04:02:50') (8485) terminated with exit code 0[0m
[[34m2021-04-01 04:12:58,978[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 19963[0m
[[34m2021-04-01 04:12:58,987[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 04:12:58,998] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 04:12:59,177[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 04:00:00+00:00: scheduled__2021-04-01T04:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 04:14:09,423[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:19:09,557[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:20:01,924[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:20:01,926[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:20:01,926[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:20:01,926[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:20:01,928[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 4, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 04:20:01,928[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:20:01,932[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:20:02,970[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:20:03,023[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:20:03,023[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T04:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:20:03,585[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 04:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:20:03,763[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 04:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:20:03,765[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:20:03,765[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:20:03,765[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 04:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:20:03,767[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 4, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 04:20:03,767[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:20:03,770[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:20:04,459[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:20:04,508[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:20:04,508[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T04:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:20:42,898[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 04:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:20:43,095[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 04:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:20:43,096[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:20:43,096[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:20:43,097[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 04:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:20:43,098[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 4, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 04:20:43,098[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:20:43,101[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:20:44,642[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:20:44,793[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:20:44,793[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T04:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:22:53,763[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 04:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:22:53,779[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=19963) last sent a heartbeat 130.72 seconds ago! Restarting it[0m
[[34m2021-04-01 04:22:53,782[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 19963[0m
[[34m2021-04-01 04:22:53,875[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=19963, status='terminated', exitcode=0, started='04:12:58') (19963) terminated with exit code 0[0m
[[34m2021-04-01 04:22:53,877[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31384[0m
[[34m2021-04-01 04:22:53,887[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 04:22:53,898] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 04:22:54,049[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 04:10:00+00:00: scheduled__2021-04-01T04:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 04:24:09,585[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:29:09,620[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:30:01,886[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:30:01,887[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:30:01,887[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:30:01,888[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:30:01,889[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 4, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 04:30:01,889[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:30:01,892[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:30:02,818[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:30:02,873[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:30:02,873[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T04:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:30:03,234[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 04:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:30:03,277[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 04:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:30:03,278[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:30:03,278[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:30:03,279[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 04:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:30:03,280[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 4, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 04:30:03,280[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:30:03,283[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:30:03,886[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:30:03,937[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:30:03,937[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T04:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:30:43,814[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 04:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:30:44,000[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 04:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:30:44,002[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:30:44,002[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:30:44,002[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 04:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:30:44,003[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 4, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 04:30:44,003[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:30:44,006[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:30:45,081[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:30:45,160[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:30:45,161[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T04:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:33:02,235[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 04:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:33:02,255[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31384) last sent a heartbeat 138.28 seconds ago! Restarting it[0m
[[34m2021-04-01 04:33:02,258[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31384[0m
[[34m2021-04-01 04:33:02,392[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31384, status='terminated', exitcode=0, started='04:22:53') (31384) terminated with exit code 0[0m
[[34m2021-04-01 04:33:02,396[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10778[0m
[[34m2021-04-01 04:33:02,408[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 04:33:02,419] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 04:33:02,642[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 04:20:00+00:00: scheduled__2021-04-01T04:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 04:34:09,751[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:39:09,787[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:40:01,545[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:40:01,547[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:40:01,547[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:40:01,547[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:40:01,548[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 4, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 04:40:01,548[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:40:01,551[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:40:02,827[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:40:02,884[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:40:02,884[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T04:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:40:03,421[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 04:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:40:03,584[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 04:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:40:03,585[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:40:03,585[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:40:03,585[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 04:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:40:03,586[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 4, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 04:40:03,586[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:40:03,589[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:40:04,208[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:40:04,262[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:40:04,262[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T04:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:40:42,210[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 04:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:40:42,405[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 04:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:40:42,407[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:40:42,407[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:40:42,407[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 04:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:40:42,409[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 4, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 04:40:42,409[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:40:42,413[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:40:43,718[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:40:43,828[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:40:43,828[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T04:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:42:55,929[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 04:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:42:55,947[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10778) last sent a heartbeat 133.58 seconds ago! Restarting it[0m
[[34m2021-04-01 04:42:55,953[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10778[0m
[[34m2021-04-01 04:42:56,086[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10778, status='terminated', exitcode=0, started='04:33:01') (10778) terminated with exit code 0[0m
[[34m2021-04-01 04:42:56,091[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 22086[0m
[[34m2021-04-01 04:42:56,115[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 04:42:56,135] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 04:42:56,308[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 04:30:00+00:00: scheduled__2021-04-01T04:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 04:44:09,921[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:49:09,927[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:50:01,197[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:50:01,198[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:50:01,198[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:50:01,198[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:50:01,199[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 4, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 04:50:01,199[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:50:01,202[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:50:02,272[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:50:02,327[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:50:02,327[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T04:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:50:02,693[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 04:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:50:02,852[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 04:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:50:02,853[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:50:02,853[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:50:02,853[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 04:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:50:02,854[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 4, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 04:50:02,854[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:50:02,858[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:50:03,590[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:50:03,640[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:50:03,641[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T04:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:50:43,432[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 04:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:50:43,599[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 04:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:50:43,600[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 04:50:43,601[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 04:50:43,601[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 04:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 04:50:43,602[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 4, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 04:50:43,602[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:50:43,605[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 04:50:44,679[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 04:50:44,757[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 04:50:44,757[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T04:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 04:52:52,108[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 04:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 04:52:52,124[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=22086) last sent a heartbeat 128.55 seconds ago! Restarting it[0m
[[34m2021-04-01 04:52:52,127[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 22086[0m
[[34m2021-04-01 04:52:52,219[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=22086, status='terminated', exitcode=0, started='04:42:55') (22086) terminated with exit code 0[0m
[[34m2021-04-01 04:52:52,222[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 1070[0m
[[34m2021-04-01 04:52:52,233[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 04:52:52,244] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 04:52:52,452[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 04:40:00+00:00: scheduled__2021-04-01T04:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 04:54:10,058[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 04:59:10,089[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:00:01,761[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:00:01,762[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:00:01,762[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:00:01,762[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 04:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:00:01,763[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 4, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 05:00:01,764[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:00:01,767[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:00:03,227[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:00:03,320[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:00:03,320[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T04:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:00:03,969[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 04:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:00:04,163[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 04:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:00:04,165[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:00:04,165[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:00:04,165[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 04:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:00:04,167[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 4, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 05:00:04,167[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:00:04,172[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:00:05,258[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:00:05,345[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:00:05,345[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T04:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:00:46,935[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 04:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:00:47,105[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 04:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:00:47,106[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:00:47,106[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:00:47,106[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 04:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:00:47,107[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 4, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 05:00:47,107[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:00:47,110[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:00:48,262[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:00:48,349[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:00:48,349[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T04:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:02:57,675[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 04:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:02:57,691[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=1070) last sent a heartbeat 130.61 seconds ago! Restarting it[0m
[[34m2021-04-01 05:02:57,694[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 1070[0m
[[34m2021-04-01 05:02:57,786[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=1070, status='terminated', exitcode=0, started='04:52:51') (1070) terminated with exit code 0[0m
[[34m2021-04-01 05:02:57,789[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 12932[0m
[[34m2021-04-01 05:02:57,799[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 05:02:57,811] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 05:02:57,965[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 04:50:00+00:00: scheduled__2021-04-01T04:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 05:04:10,236[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:09:10,267[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:10:01,599[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:10:01,601[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:10:01,601[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:10:01,601[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:10:01,603[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 05:10:01,603[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:10:01,606[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:10:02,857[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:10:02,915[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:10:02,915[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T05:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:10:03,345[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:10:03,504[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:10:03,505[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:10:03,505[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:10:03,505[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:10:03,506[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 05:10:03,507[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:10:03,509[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:10:04,106[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:10:04,152[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:10:04,152[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T05:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:10:42,694[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:10:42,862[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:10:42,863[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:10:42,863[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:10:42,863[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:10:42,864[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 05:10:42,864[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:10:42,867[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:10:43,928[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:10:44,008[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:10:44,008[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T05:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:12:59,500[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:12:59,516[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=12932) last sent a heartbeat 136.68 seconds ago! Restarting it[0m
[[34m2021-04-01 05:12:59,520[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 12932[0m
[[34m2021-04-01 05:12:59,612[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=12932, status='terminated', exitcode=0, started='05:02:56') (12932) terminated with exit code 0[0m
[[34m2021-04-01 05:12:59,615[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 24367[0m
[[34m2021-04-01 05:12:59,626[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 05:12:59,638] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 05:12:59,788[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 05:00:00+00:00: scheduled__2021-04-01T05:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 05:14:10,398[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:19:10,423[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:20:01,712[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:20:01,714[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:20:01,714[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:20:01,714[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:20:01,715[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 05:20:01,716[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:20:01,719[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:20:03,338[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:20:03,419[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:20:03,419[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T05:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:20:04,052[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:20:04,226[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:20:04,228[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:20:04,228[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:20:04,228[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:20:04,230[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 05:20:04,230[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:20:04,233[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:20:05,231[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:20:05,318[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:20:05,318[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T05:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:20:47,057[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:20:47,225[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:20:47,226[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:20:47,226[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:20:47,226[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:20:47,227[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 05:20:47,228[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:20:47,231[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:20:48,295[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:20:48,376[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:20:48,376[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T05:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:22:56,254[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:22:56,271[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=24367) last sent a heartbeat 129.07 seconds ago! Restarting it[0m
[[34m2021-04-01 05:22:56,274[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 24367[0m
[[34m2021-04-01 05:22:56,446[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=24367, status='terminated', exitcode=0, started='05:12:58') (24367) terminated with exit code 0[0m
[[34m2021-04-01 05:22:56,449[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 3557[0m
[[34m2021-04-01 05:22:56,461[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 05:22:56,475] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 05:22:56,657[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 05:10:00+00:00: scheduled__2021-04-01T05:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 05:24:10,561[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:29:10,589[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:30:01,564[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:30:01,565[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:30:01,565[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:30:01,565[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:30:01,566[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 05:30:01,566[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:30:01,569[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:30:02,794[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:30:02,847[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:30:02,847[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T05:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:30:03,306[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:30:03,466[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:30:03,467[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:30:03,467[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:30:03,467[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:30:03,468[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 05:30:03,468[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:30:03,471[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:30:04,204[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:30:04,252[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:30:04,252[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T05:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:30:41,177[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:30:41,365[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:30:41,367[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:30:41,367[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:30:41,367[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:30:41,369[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 05:30:41,369[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:30:41,372[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:30:42,565[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:30:42,676[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:30:42,676[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T05:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:32:59,421[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:32:59,437[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=3557) last sent a heartbeat 138.11 seconds ago! Restarting it[0m
[[34m2021-04-01 05:32:59,440[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 3557[0m
[[34m2021-04-01 05:32:59,532[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=3557, status='terminated', exitcode=0, started='05:22:55') (3557) terminated with exit code 0[0m
[[34m2021-04-01 05:32:59,536[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15301[0m
[[34m2021-04-01 05:32:59,546[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 05:32:59,557] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 05:32:59,706[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 05:20:00+00:00: scheduled__2021-04-01T05:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 05:34:10,705[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:39:10,733[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:40:01,457[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:40:01,458[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:40:01,458[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:40:01,459[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:40:01,460[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 05:40:01,460[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:40:01,468[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:40:02,770[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:40:02,824[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:40:02,825[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T05:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:40:03,292[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:40:03,446[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:40:03,447[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:40:03,447[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:40:03,447[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:40:03,449[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 05:40:03,449[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:40:03,452[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:40:04,272[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:40:04,322[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:40:04,322[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T05:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:40:45,771[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:40:45,978[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:40:45,979[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:40:45,980[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:40:45,980[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:40:45,981[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 05:40:45,981[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:40:45,984[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:40:47,045[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:40:47,129[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:40:47,129[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T05:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:42:54,720[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:42:54,735[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15301) last sent a heartbeat 128.81 seconds ago! Restarting it[0m
[[34m2021-04-01 05:42:54,739[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15301[0m
[[34m2021-04-01 05:42:54,911[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15301, status='terminated', exitcode=0, started='05:32:58') (15301) terminated with exit code 0[0m
[[34m2021-04-01 05:42:54,914[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 26863[0m
[[34m2021-04-01 05:42:54,924[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 05:42:54,939] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 05:42:55,114[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 05:30:00+00:00: scheduled__2021-04-01T05:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 05:44:10,761[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:49:10,904[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:50:02,129[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:50:02,131[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:50:02,131[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:50:02,131[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:50:02,132[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 05:50:02,133[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:50:02,136[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:50:03,274[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:50:03,327[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:50:03,327[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T05:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:50:03,876[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:50:03,921[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:50:03,922[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:50:03,922[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:50:03,922[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:50:03,923[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 05:50:03,923[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:50:03,927[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:50:04,592[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:50:04,661[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:50:04,661[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T05:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:50:40,675[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:50:40,870[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:50:40,871[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 05:50:40,871[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 05:50:40,872[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 05:50:40,873[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 05:50:40,873[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:50:40,877[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 05:50:42,405[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 05:50:42,535[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 05:50:42,535[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T05:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 05:52:48,172[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 05:52:48,188[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=26863) last sent a heartbeat 127.36 seconds ago! Restarting it[0m
[[34m2021-04-01 05:52:48,192[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 26863[0m
[[34m2021-04-01 05:52:48,284[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=26863, status='terminated', exitcode=0, started='05:42:54') (26863) terminated with exit code 0[0m
[[34m2021-04-01 05:52:48,287[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 6258[0m
[[34m2021-04-01 05:52:48,298[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 05:52:48,310] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 05:52:48,460[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 05:40:00+00:00: scheduled__2021-04-01T05:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 05:54:11,037[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 05:59:11,168[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:00:01,700[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:00:01,702[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:00:01,702[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:00:01,702[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:00:01,704[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 06:00:01,704[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:00:01,714[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:00:02,799[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:00:02,853[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:00:02,853[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T05:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:00:03,332[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:00:03,486[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:00:03,487[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:00:03,487[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:00:03,487[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:00:03,488[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 06:00:03,488[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:00:03,491[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:00:04,088[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:00:04,135[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:00:04,135[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T05:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:00:44,978[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:00:45,145[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:00:45,146[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:00:45,146[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:00:45,146[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:00:45,147[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 06:00:45,147[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:00:45,150[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:00:46,204[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:00:46,285[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:00:46,285[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T05:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:03:00,435[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:03:00,454[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=6258) last sent a heartbeat 135.33 seconds ago! Restarting it[0m
[[34m2021-04-01 06:03:00,457[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 6258[0m
[[34m2021-04-01 06:03:00,630[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=6258, status='terminated', exitcode=0, started='05:52:47') (6258) terminated with exit code 0[0m
[[34m2021-04-01 06:03:00,633[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 17971[0m
[[34m2021-04-01 06:03:00,648[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 06:03:00,668] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 06:03:00,846[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 05:50:00+00:00: scheduled__2021-04-01T05:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 06:04:11,300[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:09:11,323[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:10:01,206[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:10:01,207[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:10:01,207[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:10:01,207[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:10:01,208[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 06:10:01,208[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:10:01,211[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:10:02,443[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:10:02,499[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:10:02,499[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T06:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:10:02,974[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:10:03,120[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:10:03,121[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:10:03,121[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:10:03,121[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:10:03,122[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 06:10:03,122[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:10:03,125[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:10:03,738[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:10:03,791[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:10:03,791[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T06:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:10:43,450[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:10:43,640[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:10:43,642[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:10:43,642[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:10:43,642[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:10:43,644[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 06:10:43,644[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:10:43,648[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:10:45,147[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:10:45,251[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:10:45,251[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T06:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:12:47,158[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:12:47,175[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=17971) last sent a heartbeat 123.57 seconds ago! Restarting it[0m
[[34m2021-04-01 06:12:47,179[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 17971[0m
[[34m2021-04-01 06:12:47,314[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=17971, status='terminated', exitcode=0, started='06:02:59') (17971) terminated with exit code 0[0m
[[34m2021-04-01 06:12:47,318[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 29257[0m
[[34m2021-04-01 06:12:47,347[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 06:12:47,365] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 06:12:47,517[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 06:00:00+00:00: scheduled__2021-04-01T06:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 06:14:11,462[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:19:11,492[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:20:01,877[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:20:01,878[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:20:01,878[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:20:01,879[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:20:01,880[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 06:20:01,880[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:20:01,884[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:20:03,107[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:20:03,160[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:20:03,160[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T06:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:20:03,635[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:20:03,790[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:20:03,791[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:20:03,792[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:20:03,792[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:20:03,793[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 06:20:03,793[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:20:03,796[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:20:04,405[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:20:04,456[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:20:04,456[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T06:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:20:44,036[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:20:44,219[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:20:44,220[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:20:44,220[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:20:44,221[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:20:44,222[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 06:20:44,222[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:20:44,225[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:20:45,310[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:20:45,399[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:20:45,399[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T06:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:22:54,041[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:22:54,060[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=29257) last sent a heartbeat 129.87 seconds ago! Restarting it[0m
[[34m2021-04-01 06:22:54,065[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 29257[0m
[[34m2021-04-01 06:22:54,198[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=29257, status='terminated', exitcode=0, started='06:12:46') (29257) terminated with exit code 0[0m
[[34m2021-04-01 06:22:54,202[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 8702[0m
[[34m2021-04-01 06:22:54,228[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 06:22:54,244] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 06:22:54,423[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 06:10:00+00:00: scheduled__2021-04-01T06:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 06:24:11,519[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:29:11,657[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:30:01,491[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:30:01,493[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:30:01,493[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:30:01,493[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:30:01,496[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 06:30:01,497[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:30:01,501[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:30:02,989[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:30:03,072[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:30:03,072[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T06:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:30:03,702[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:30:03,761[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:30:03,763[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:30:03,763[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:30:03,763[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:30:03,765[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 06:30:03,765[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:30:03,769[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:30:04,812[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:30:04,913[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:30:04,913[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T06:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:30:45,205[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:30:45,724[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:30:45,725[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:30:45,725[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:30:45,726[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:30:45,727[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 06:30:45,727[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:30:45,730[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:30:46,993[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:30:47,081[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:30:47,081[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T06:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:32:48,775[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:32:48,793[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=8702) last sent a heartbeat 123.44 seconds ago! Restarting it[0m
[[34m2021-04-01 06:32:48,798[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 8702[0m
[[34m2021-04-01 06:32:48,892[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=8702, status='terminated', exitcode=0, started='06:22:53') (8702) terminated with exit code 0[0m
[[34m2021-04-01 06:32:48,895[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 20082[0m
[[34m2021-04-01 06:32:48,906[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 06:32:48,918] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 06:32:49,076[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 06:20:00+00:00: scheduled__2021-04-01T06:20:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 06:34:11,798[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:39:11,931[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:40:01,261[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:40:01,262[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:40:01,262[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:40:01,262[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:40:01,263[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 06:40:01,263[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:40:01,266[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:40:02,470[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:40:02,551[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:40:02,551[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T06:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:40:03,059[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:40:03,205[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:40:03,206[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:40:03,206[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:40:03,206[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:40:03,207[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 06:40:03,208[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:40:03,211[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:40:03,814[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:40:03,862[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:40:03,862[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T06:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:40:42,195[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:40:42,363[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:40:42,364[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:40:42,364[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:40:42,364[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:40:42,365[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 06:40:42,365[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:40:42,368[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:40:43,463[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:40:43,581[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:40:43,581[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T06:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:42:41,595[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:42:41,611[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=20082) last sent a heartbeat 119.27 seconds ago! Restarting it[0m
[[34m2021-04-01 06:42:41,614[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 20082[0m
[[34m2021-04-01 06:42:41,706[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=20082, status='terminated', exitcode=0, started='06:32:48') (20082) terminated with exit code 0[0m
[[34m2021-04-01 06:42:41,709[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 31341[0m
[[34m2021-04-01 06:42:41,721[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 06:42:41,735] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 06:42:41,886[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 06:30:00+00:00: scheduled__2021-04-01T06:30:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 06:44:11,960[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:49:11,989[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:50:01,763[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:50:01,765[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:50:01,765[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:50:01,765[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:50:01,767[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 06:50:01,767[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:50:01,775[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:50:03,375[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:50:03,456[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:50:03,457[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T06:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:50:04,114[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:50:04,285[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:50:04,287[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:50:04,287[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:50:04,287[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:50:04,289[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 06:50:04,289[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:50:04,292[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:50:05,316[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:50:05,366[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:50:05,366[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T06:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:50:45,826[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:50:45,995[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:50:45,996[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 06:50:45,996[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 06:50:45,996[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-01 06:50:45,997[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 06:50:45,997[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:50:46,001[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 06:50:47,101[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 06:50:47,184[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 06:50:47,185[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T06:40:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 06:52:40,464[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 06:52:40,480[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=31341) last sent a heartbeat 114.51 seconds ago! Restarting it[0m
[[34m2021-04-01 06:52:40,484[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 31341[0m
[[34m2021-04-01 06:52:40,576[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=31341, status='terminated', exitcode=0, started='06:42:40') (31341) terminated with exit code 0[0m
[[34m2021-04-01 06:52:40,579[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 10469[0m
[[34m2021-04-01 06:52:40,589[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 06:52:40,601] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 06:52:40,752[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 06:40:00+00:00: scheduled__2021-04-01T06:40:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 06:54:12,024[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 06:59:12,054[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 07:00:01,793[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:00:01,794[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:00:01,794[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:00:01,794[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:00:01,795[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 07:00:01,795[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:00:01,798[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:00:02,796[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:00:02,865[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:00:02,866[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:00:03,361[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:00:03,508[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:00:03,509[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:00:03,509[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:00:03,509[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:00:03,510[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 07:00:03,510[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:00:03,513[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:00:04,112[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:00:04,163[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:00:04,164[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:00:41,707[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:00:41,887[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:00:41,888[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:00:41,888[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:00:41,888[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:00:41,889[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 07:00:41,889[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:00:41,892[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:00:43,064[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:00:43,151[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:00:43,152[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T06:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:02:43,817[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:02:43,833[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=10469) last sent a heartbeat 121.97 seconds ago! Restarting it[0m
[[34m2021-04-01 07:02:43,836[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 10469[0m
[[34m2021-04-01 07:02:43,969[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=10469, status='terminated', exitcode=0, started='06:52:39') (10469) terminated with exit code 0[0m
[[34m2021-04-01 07:02:43,972[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 21852[0m
[[34m2021-04-01 07:02:43,983[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 07:02:43,996] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 07:02:44,147[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 06:50:00+00:00: scheduled__2021-04-01T06:50:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 07:04:12,186[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 07:09:12,319[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 07:10:01,331[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:10:01,338[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:10:01,339[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:10:01,339[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:10:01,340[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 07:10:01,340[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:10:01,344[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:10:02,751[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:10:02,806[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:10:02,806[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:10:03,286[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:10:03,438[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:10:03,439[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:10:03,439[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:10:03,439[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:10:03,440[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 07:10:03,440[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:10:03,443[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:10:04,051[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:10:04,103[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:10:04,103[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:10:45,395[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:10:45,581[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:10:45,582[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:10:45,582[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:10:45,583[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:10:45,584[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 07:10:45,584[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:10:45,587[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:10:46,835[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:10:46,918[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:10:46,918[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T07:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:12:49,551[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:12:49,568[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=21852) last sent a heartbeat 124.02 seconds ago! Restarting it[0m
[[34m2021-04-01 07:12:49,573[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 21852[0m
[[34m2021-04-01 07:12:49,785[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=21852, status='terminated', exitcode=0, started='07:02:43') (21852) terminated with exit code 0[0m
[[34m2021-04-01 07:12:49,788[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 848[0m
[[34m2021-04-01 07:12:49,800[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 07:12:49,813] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 07:12:49,992[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 07:00:00+00:00: scheduled__2021-04-01T07:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 07:14:12,347[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 07:19:12,383[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 07:20:02,049[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:20:02,050[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:20:02,050[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:20:02,050[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:20:02,052[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 07:20:02,052[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:20:02,055[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:20:03,275[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:20:03,336[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:20:03,336[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:20:03,818[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:20:03,965[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:20:03,966[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:20:03,966[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:20:03,966[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:20:03,968[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 07:20:03,968[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:20:03,971[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:20:04,574[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:20:04,623[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:20:04,623[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:20:46,108[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:20:46,650[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:20:46,651[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:20:46,651[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:20:46,651[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.aurora 2021-04-01 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:20:46,653[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 07:20:46,653[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:20:46,657[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:20:48,161[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:20:48,293[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:20:48,293[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T07:10:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:23:00,834[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:23:00,852[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=848) last sent a heartbeat 134.59 seconds ago! Restarting it[0m
[[34m2021-04-01 07:23:00,855[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 848[0m
[[34m2021-04-01 07:23:01,028[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=848, status='terminated', exitcode=0, started='07:12:48') (848) terminated with exit code 0[0m
[[34m2021-04-01 07:23:01,031[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 14404[0m
[[34m2021-04-01 07:23:01,042[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 07:23:01,055] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 07:23:01,574[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun build_sites @ 2021-04-01 07:10:00+00:00: scheduled__2021-04-01T07:10:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-01 07:27:16,094[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 07:27:16,098[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2021-04-01 07:31:48,378[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:31:48,380[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:31:48,380[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:31:48,380[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:31:48,382[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 07:31:48,382[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:31:48,385[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:34:47,938[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:34:48,138[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:34:48,138[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 07:34:49,676[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 07:34:49,699[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=14404) last sent a heartbeat 182.03 seconds ago! Restarting it[0m
[[34m2021-04-01 07:34:49,706[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 14404[0m
[[34m2021-04-01 07:34:49,961[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=14404, status='terminated', exitcode=0, started='07:23:00') (14404) terminated with exit code 0[0m
[[34m2021-04-01 07:34:49,965[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 15443[0m
[[34m2021-04-01 07:34:49,990[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 07:34:50,005] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 07:34:50,020[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 07:34:50,023[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2021-04-01 07:34:50,038[0m] {[34mscheduler_job.py:[0m1901} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: build_sites.gite 2021-04-01 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:34:50,833[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:34:50,838[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-01 07:34:50,838[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 07:34:50,838[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.gite 2021-04-01 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 07:34:50,840[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 07:34:50,840[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:34:50,843[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 07:39:05,372[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 07:39:05,572[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 07:39:05,572[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 08:03:03,698[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 08:03:03,730[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=15443) last sent a heartbeat 1693.53 seconds ago! Restarting it[0m
[[34m2021-04-01 08:03:03,736[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 15443[0m
[[34m2021-04-01 08:03:03,994[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=15443, status='terminated', exitcode=0, started='07:34:49') (15443) terminated with exit code 0[0m
[[34m2021-04-01 08:03:03,998[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 18807[0m
[[34m2021-04-01 08:03:04,012[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-04-01 08:03:04,026] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 08:03:04,059[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 08:03:04,074[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2021-04-01 08:03:04,079[0m] {[34mscheduler_job.py:[0m1901} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: build_sites.aurora 2021-04-01 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 08:03:04,841[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 07:20:00+00:00 [scheduled]>
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 08:03:04,842[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-01 08:03:04,842[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 08:03:04,842[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 1/16 running and queued tasks[0m
[[34m2021-04-01 08:03:04,843[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:30:00+00:00 [scheduled]>
	<TaskInstance: build_sites.aurora 2021-04-01 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-01 08:03:04,845[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 08:03:04,845[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:03:04,845[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 08:03:04,845[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:03:04,856[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:03:06,980[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 08:03:07,177[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 08:03:07,178[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 08:03:08,357[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:03:10,334[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 08:03:10,488[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 08:03:10,488[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.aurora 2021-04-01T07:20:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 08:03:12,232[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 08:03:12,232[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.aurora execution_date=2021-04-01 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 08:03:12,765[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: build_sites.gite 2021-04-01 07:30:00+00:00 [scheduled]>
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:50:00+00:00 [scheduled]>[0m
[[34m2021-04-01 08:03:12,774[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-01 08:03:12,774[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 08:03:12,774[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 1/16 running and queued tasks[0m
[[34m2021-04-01 08:03:12,774[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 07:50:00+00:00 [scheduled]>
	<TaskInstance: build_sites.gite 2021-04-01 07:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 08:03:12,776[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 08:03:12,776[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:03:12,776[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 08:03:12,776[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:03:12,785[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:08:34,035[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 08:08:34,217[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 08:08:34,217[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 08:08:35,285[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 08:08:36,945[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 08:08:37,068[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 08:08:37,068[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T07:30:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 09:28:34,135[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.print_current_date execution_date=2021-04-01 07:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 09:28:34,135[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of build_sites.gite execution_date=2021-04-01 07:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-01 09:28:34,262[0m] {[34mdag_processing.py:[0m412} ERROR[0m - DagFileProcessorManager (PID=18807) last sent a heartbeat 5121.88 seconds ago! Restarting it[0m
[[34m2021-04-01 09:28:34,295[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 18807[0m
[[34m2021-04-01 09:28:42,325[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=18807, status='terminated', exitcode=0, started='08:03:03') (18807) terminated with exit code 0[0m
[[34m2021-04-01 09:28:42,345[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 28838[0m
[[34m2021-04-01 09:28:42,482[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-04-01 09:28:42,629[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-01 09:28:42,651[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2021-04-01 09:28:42,656[0m] {[34mscheduler_job.py:[0m1901} INFO[0m - Reset the following 2 orphaned TaskInstances:
	<TaskInstance: build_sites.aurora 2021-04-01 07:30:00+00:00 [scheduled]>
	<TaskInstance: build_sites.gite 2021-04-01 07:50:00+00:00 [scheduled]>[0m
[2021-04-01 09:28:42,726] {dag_processing.py:518} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-04-01 09:29:17,707[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 4 tasks up for execution:
	<TaskInstance: build_sites.aurora 2021-04-01 07:20:00+00:00 [scheduled]>
	<TaskInstance: build_sites.aurora 2021-04-01 07:30:00+00:00 [scheduled]>
	<TaskInstance: build_sites.gite 2021-04-01 07:50:00+00:00 [scheduled]>
	<TaskInstance: build_sites.print_current_date 2021-04-01 08:00:00+00:00 [scheduled]>[0m
[[34m2021-04-01 09:29:17,708[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 4 task instances ready to be queued[0m
[[34m2021-04-01 09:29:17,708[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 0/16 running and queued tasks[0m
[[34m2021-04-01 09:29:17,708[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 1/16 running and queued tasks[0m
[[34m2021-04-01 09:29:17,708[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 2/16 running and queued tasks[0m
[[34m2021-04-01 09:29:17,708[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG build_sites has 3/16 running and queued tasks[0m
[[34m2021-04-01 09:29:17,708[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: build_sites.print_current_date 2021-04-01 08:00:00+00:00 [scheduled]>
	<TaskInstance: build_sites.gite 2021-04-01 07:50:00+00:00 [scheduled]>
	<TaskInstance: build_sites.aurora 2021-04-01 07:20:00+00:00 [scheduled]>
	<TaskInstance: build_sites.aurora 2021-04-01 07:30:00+00:00 [scheduled]>[0m
[[34m2021-04-01 09:29:17,710[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='print_current_date', execution_date=datetime.datetime(2021, 4, 1, 8, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-04-01 09:29:17,710[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 09:29:17,710[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='gite', execution_date=datetime.datetime(2021, 4, 1, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-01 09:29:17,710[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 09:29:17,710[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 7, 20, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-01 09:29:17,711[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 09:29:17,711[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='build_sites', task_id='aurora', execution_date=datetime.datetime(2021, 4, 1, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-01 09:29:17,711[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 09:29:18,452[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'print_current_date', '2021-04-01T08:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 09:30:18,153[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 09:30:21,715[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 09:30:21,715[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.print_current_date 2021-04-01T08:00:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 09:30:48,267[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'gite', '2021-04-01T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 09:31:52,144[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 09:31:55,176[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 09:31:55,176[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: build_sites.gite 2021-04-01T07:50:00+00:00 [queued]> on host ip-172-31-40-5.us-east-2.compute.internal
[[34m2021-04-01 09:48:01,045[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'build_sites', 'aurora', '2021-04-01T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/ec2-user/airflow/dags/build_sites.py'][0m
[[34m2021-04-01 09:48:45,333[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/ec2-user/airflow/dags/build_sites.py[0m
[[34m2021-04-01 09:48:47,677[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-04-01 09:48:47,677[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
